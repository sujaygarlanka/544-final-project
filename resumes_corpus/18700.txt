Senior Informatica Developer Senior Informatica <span class="hl">Developer</span> Senior Informatica Developer - CHUBB Insurance US * 10 years of work experience in application and product development using full SDLC primarily using Hadoop, Informatica and Mainframe Technologies.  * Proven skills in establishing strategic direction yet technically strong in designing, implementing, and deploying. Collected/translated business requirements into distributed architecture & robust scalable designs.  * Expertise in performance tuning of Informatica mappings and sessions for better performance and meeting SLA.  * In depth understanding/knowledge of Hadoop Architecture and various components such as HDFS, Job Tracker, Task Tracker, Name Node, Data Node and Map Reduce concepts.  * Experience in using Hive, Sqoop, Spark and Cloudera Manager.  * Extensive experience with ETL and Query big data tools like Impala and Hive QL.  * Hands on experience in big data ingestion like Sqoop.  * Good Programming knowledge in Python and Hive QL.  * Experience in importing and exporting data using Sqoop from Relational Database Systems to HDFS  * Converted Hive/SQL queries into Spark transformations using Spark RDDs and Python.  * Using Spark SQL and Spark Streaming for analyzing the data.  * Accountable for all phases of the Software Development Life Cycle (SDLC) and successfully delivering solutions to end customers on numerous ETL processes using Informatica.  * Experience in integration of various data sources like MySQL, Oracle, Flat files into an operational data store.  * Onsite Exposure - Well versed in client interaction and client facing role.  * Team oriented individual with a result driven attitude, strong interpersonal and Communication skills in effective information exchange and interaction with clients at all levels of management.  * Involved in Development, Enhancement and good exposure to 24x7 Production support environment.  * Executed software projects for Insurance, Healthcare, Energy and Utilities and Retail industry. Work Experience Senior Informatica Developer CHUBB Insurance January 2018 to Present Project Description:  CVAC is an existing Claim Application belongs to Legacy CHUBB. After ACE and CHUBB became one, they decided to integrate and migrate common applications between Legacy ACE and CHUBB and make one application. Due to this vision they requested IBM to help them to integrate and migrate CVAC data into CDW (Claims Data warehouse).  In this integration and migration project, Business wants to migrate CDW application from mainframe to Informatica. CDW sources data from multiple mainframe systems such as ClaimConnect, ClaimPath etc, so we ingest the files from these source systems into Sql Server through Informatica. This is a multilayer process where first we ingest raw files from mainframe systems, cleanse the data by applying business transformations and store the data in datawarehouse through Informatica. Once the data is provisioned in Sql tables , business uses these tables to read and generate their reports.    Responsibilities:  • Worked as a Sr. Informatica Developer to Design and developing the ETL process using Informatica.  • Involved in getting requirement from end users and design ETL solution for those requirements.  • Performed transformations, cleaning and filtering on imported data using multiple transformations and load data into Sql Datawarehouse.  • Expertise in performance tuning of Informatica mappings and sessions for better performance and meeting SLA.  • Experience in integration of various data sources like Oracle, Flat files into an operational data store.  • Accountable for all phases of the Software Development Life Cycle (SDLC) and successfully delivering solutions to end customers on numerous ETL processes using Informatica.    Technology Used:  • Data warehousing: Informatica Power Center 9.x/10.x  • Databases: Microsoft Sql Server  • Scheduling: Autosys    IBM India Pvt Ltd (Client AT&T)  Location: NJ (USA)    Project title: SCA (Sales Compensation Application) ETL/ Python Developer AT&T December 2015 to Present Project Description:  The Call Center Performance Management program is intended to aggregate data from various systems with a comprehensive set of tools for reporting to improve business operations and individual performance. Collecting and consolidating various types of information into a comparable and consistent format across the enterprise (all regions) will provide the ability to analyze performance in a more robust and timely manner. Implementing this program will allow AT&T to analyze data, from the employee to enterprise level, across functional perspectives such as operations, sales, quality and customer care.    We get large amount of data from multiple sources in various formats. Once the data is ingested, transformations are applied as per business requirements and data is loaded into HIVE. Analytics are performed and reports are produced which helps in managing the call center performance.    Responsibilities:  • Worked as a ETL/ Python Developer for providing solutions for big data problem.  • Worked in complete Software Development Lifecycle Experience (SDLC) from Requirement gathering to Development, Testing, Deployment and Documentation.  • Design, Architect, and help Maintain scalable solutions on the big data analytics platform for enterprise module.  • Proficient in writing Sqoop job to ingest data from MySql DB.  • Imported data from Relation Databases into Spark RDD and performed transformations and actions on RDD's.  • Involved in loading and transforming large sets of structured data from relational databases into HDFS using Sqoop imports.  • Implemented Map-Reduce programs to incorporate the business transformations.  • Responsible for writing Hive Queries for analyzing data in Hive warehouse using Hive Query Language (HQL).  • Populate the data into dimensions and fact tables, efficiently involved in creating Informatica Mappings.  • Identify data sources, create source-to-target mapping, storage estimation, provide support for Hadoop cluster setup, data partitioning.  • Work with Business stakeholder and translate Business objectives, requirements into technical requirements and design.    Technology Used:  • Big Data Ecosystems: Hadoop, HDFS, Sqoop, Apache Spark, Python and Informatica PowerCenter  • Database: Oracle , MySql Mainframes/Big Data Developer IBM India Pvt Ltd (Client Express Scripts) November 2012 to December 2015 IBM India Pvt Ltd (Client Express Scripts)  Location: NJ (USA)    Project title: Eligibility Claims processing system and DCRS/FRS App    Duration: - Nov 2012 - Dec 2015  Role: - Mainframes/Big Data Developer.  Client: - ESI    Project Description:  Express Scripts, Inc is largest pharmacy benefit manager in USA holding approximately 29% market share in PBM sector. Express Scripts fills close to 1.4 billion prescriptions a year for employers and insurers.    IBM was engaged to support many mission critical application for healthcare giant, amongst them were Eligibility and Drug Coverage Rule Station (DCRS) application. Eligibility application determines the eligibility of patients needed for claim adjudication and DCRS provides the rules coverages for formulary and non-formulary drugs.    We get large amount of claims data from the point of sale systems, websites, clients and vendors. Some of these data could be structured, some semi structured and a few unstructured. The claims data is collected, aggregated and analyzed in Hadoop cluster to improve the home delivery, welcome kit generation process, marketing insights, to analyze the claim patterns and to produce various reports.    Responsibilities:    • As a Senior System Engineer and senior team member, responsible for analyzing the critical production issues and guiding team in their daily tasks from technical perspective.  • Responsible for Analysis, coding and unit testing of enhancements in Eligibility and DCRS application.  • Deliver new and optimized solutions to clients in response to varying business requirements.  • Responsible for designing the structural framework of the application as per the business requirements for enhancements.  • Extracted the data from mainframe files and DB2 tables and imported into HDFS using file transfer servers and sqoop.  • Created sqoop scripts to load data to hive external tables.  • Created hive managed and external tables, staged hive output to HDFS.  • Created hive queries for market analysis - that compares fresh data to history warehouse data.  • Monitoring and managing the Hadoop cluster using cloudera manager.  • Reviewing and troubleshooting Hadoop log files using Job history server.  • Experience installing, configuring and testing Hadoop ecosystem components.  • Assessing business rules, collaborate with stake holders and perform source to target data mapping, design and review.    Technology Used:    • Mainframe - COBOL, DB2, JCL, VSAM, CICS  • Big Data Ecosystems: Hadoop, HDFS, MapReduce - MRv1 and MRv2 (YARN), Hive, Sqoop, Cloudera manager, Hue, Apache Spark.  • Databases: MySQL, DB2    IBM India Pvt. Ltd (Client Oncor Electric Delivery) Senior Application Developer Oncor Electric - Pune, Maharashtra August 2009 to November 2012 Location: Pune (India)    Project title: Legacy Customer Information System (LCIS)    Duration: - Aug 2009 to Nov 2012  Role: - Senior Application Developer  Client: - Oncor Electric  Project Description:  Oncor is a regulated electric distribution and transmission business that uses superior asset management skills to provide reliable electricity delivery to consumers. Oncor operates the largest distribution and transmission system in Texas, providing power to 3 million electric delivery points over more than 102,000 miles of distribution and 14,000 miles of transmission lines. While Oncor is a subsidiary of EFH, Oncor is a separate entity that reports to a separate board that is comprised of a majority of independent directors.  Legacy Customer Information System (LCIS) is the heart and the most critical backend application for Oncor business. This application serves various customer requirements from the beginning. It handles business functions that primarily deal with Service orders, Meter reading, Billing, Cash, Collections, Revenue and Analysis Master.  LCIS interfaces with other applications to meet the business demand of Oncor.    Roles and Responsibility:  • Responsible for Analysis, coding, unit testing and guiding team in their day to day task in prospect of technical problems encountered in our CIS (Customer Information System) application of ONCOR Electric.  • Delivering high quality solutions to clients using in-depth functional and technical knowledge of IBM Mainframes in response to varying business needs  • Responsible for effective communication between the project team and the customer. Provide day to day direction to the project team and regular project status to the customer.  • Responsible for providing 24*7 PRODUCTION Support and Maintaining LCIS (Legacy Customer Information System) Application which is the heart and the most critical and challenging application of ONCOR business.  • Responsible for resolving emergency and normal defects which were tagged under stipulated releases and creating Positive and Negative test cases so as to check the validation of changes.  • Establish Quality Procedure for the team and continuously monitor and audit to ensure team meets quality goals.  • Responsible for resolving service requests raised by the users of ONCOR.  • Responsible for conducting KT (Knowledge Transfer) sessions to other team members.    Technology Used:  • IBM Mainframes, JCL, CICS  • DB2  • VSAM  • ESP Scheduler, Changeman Application Developer IBM India Pvt. Ltd (Client Danish Supermarked Gruppen (DSG) - Kolkata, West Bengal December 2008 to August 2009 Location: Kolkata (India)    Project title: Legacy to SAP Migration    Duration: - Dec 2008 to Aug 2009  Role: - Application Developer  Client: - DSG  Project Description:  Dansk Supermarked Gruppen (The Danish Supermarket Group) is a corporation owning several chains of stores. The store chains under Dansk Supermarked only operate in Denmark, except Netto which has expanded to several neighboring countries. The corporation was founded by Herman Sallingas Jysk Supermarked, but the name was changed to the Dansk Supermarked Gruppen when Herman Salling partnered with the A.P. Moller-Maersk Group. The GD part of DSG has the responsibilities like Defect & Support, AMS and migration of data from legacy to SAP.    Roles and Responsibility:  * Working as an Application developer doing mostly the development activities (mainly the Phase 4 of SDLC cycle i.e. Coding and Unit Testing) to migrate data from legacy to SAP.  * Responsible for resolving defects which were tagged under stipulated releases and creating Positive and Negative test cases in order to validate the changes.  * SPOC for EDBADM application that handles the payment system of DSG.  * Responsible for Production job abends and their resolution within SLA (Service Level Agreement).    Technology Used:  • IBM Mainframe  • DB2  • CICS Education Bachelor's in Electrical Engineering in Electrical Engineering Jai Narain Vyas University - Jodhpur, Rajasthan 2008 Skills HDFS, SQOOP, DB2, HADOOP, INFORMATICA, Powercenter, Autosys, ETL, Bigdata Additional Information Technical Skills:    Hadoop Technology: HDFS  Hadoop Ecosystems: Hive, Sqoop, Spark 2.11, Hadoop 2.0,  Databases: MySQL, DB2, Oracle  Data warehousing: Informatica Power Center 9.x/10.x  Languages: UNIX, SQL, COBOL, Job Control Language (JCL), CICS  Scheduling: Autosys, Control-M  Tools: Toad, WinSCP, Putty  Processes: AGILE, IBM's QMS (Quality Management System), QPACE, OPAL