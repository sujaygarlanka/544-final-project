Software Engineer (ETL Developer) <span class="hl">Software</span> Engineer (ETL <span class="hl">Developer</span>) Software Engineer (ETL Developer) - Zions Bancorporation Salt Lake City, UT Authorized to work in the US for any employer Work Experience Software Engineer (ETL Developer) Zions Bancorporation March 2019 to Present Location: SaltLakeCity,Utah    Project Description:  ZionsBancorporation, is one of the largest Financial services bank based out in Salt Lake City, Utah. It has six different subsidiaries which operates under the control of Zions Bank. Now Zionsbancorporation want to merge all its subsidaries banks into one centralized system. For this they are using BANCS application which migrates data from each subsidiary to BANCS. They want to decommission their application on TRISON and want to migrate all subsidaries data to BANCS applications.  As part of Migration, we are using ETL DataStage which extracts data from Mainframe application(EBCDIC) to ASCII format conversion. Also we are applying different Transformation logics to fields that are needed as part of Migration.  Here we are migrating data on each subject area which involves RCIF,TIME DEPOSITS, DDA,CRL Accounts Data.  Responsibilities:  • Designs, develops and tests ETL applications using Datastage 11.3 with application architecture guidelines.  • Access disparate data sources (databases, hardware devices, VSAM and flat files) and apply internal and external business rules, policies and data quality measures.  • Use 253 files and apply all Transformation logics accordingly to generate middle file as expected that helps in loading to BANCS application.For this worked on Sequentialfilestage,Transformer,Join,lookup,Copy,Funnel Aggregator stages.  • Worked on Master sequence for generating summary report,Snapshot report accordingly.  • Gathers mapping and reporting requirements and coordinate all data movement activities. Insures the data is safely and securely transferred to target destinations.  • Analyzes business requirements as they relate to the ETL process, research, evaluate and recommend alternative solutions.  • Transforms business requirements into technical requirement documents.  • Analyzes, develops, installs, tests, upgrades, maintains and supports complex data/process models, and processes in an ETL environment.  • Works with management and staff to establish best practice standards for ETL functions and development of new methodologies for supporting data transformations and processing in a corporate-wide solutions  Mercedes Benz BI Data Migration Senior Datastage Developer and Tableau Developer Mercedes Benz - Atlanta, GA September 2017 to October 2018 Project Description:  MBUSA,is a distribution company for cars in USA. The main objective of the project is to migrate different applications running on existing Mainframes to Data stage. This include Warranty as the major application. Along with this there involves other applications like ASBD,CRM.  (1)Warranty/CRM:  These are main application in which we need to extract the data from DB2 and load data to flat files and xml and also to target DB2.in this application we need to design parallel DS jobs based on mappings prepared from mainframe code in Prod environment. Once parallel job is designed and tested then a subsequence followed with master sequence is developed. This entire mechanism is running on mainframes. Now as part of Datastage we need to implement the migration of code running in mainframes and implement the same ETL mechanism in Datastage.  Responsibilities:  • involved in understanding the existing Mainframe COBOL code and prepare the mapping based on the code.  • involved in Developing Datastage Job based on mapping and Cobol code such that both functionalities remain same. For this we validate the data in DB2.  • worked on different transformation logics(null handling,typeconversions),join,Lookup,Aggregtor,Sort,XML,DB2.  • Developed complete sequencer with all logics handled for each subject area which includes Execution command,user variable activities, mail notification.  • Involved in reducing the complexity of code by writing the DB2 queries and also done portioning at Datastage job level.  • Tested the job in all environments till the code moves from SIT to UAT.  • After Loading data to warehouse tables prepare reports and dashboard using(Actions,filters,Calculated fields) and deploy to tableau server.  • Build, customize and publish interactive reports, visualizations and dashboards using Tableau  • Worked in Drill down and drill up hirachies,calculated fields and parametrization in Tableau.  • Worked on different charts like Heat Maps, Scatterd Charts,Do nut chart and also Created complex reports, utilizing the features like hierarchy, New Calculated Columns, Drill functionality, Tables, graphs, line Charts and Bar Charts  • Worked mainly on Interactive Dashbaords with best performance tuning mecahnisms.  • Deployed the code to production as per the Sprint timelines with all the CRQ approved and documents needed.  • Monitored the Jobs in production using Control M in doing support activities and complete the batch as per the SLA.  • Create basic calculations including string manipulation, basic arithmetic calculations, custom aggregations and ratios, date math, logic statements and quick table calculations.  • Developed Ad-hoc reports using Tableau Desktop, Excel.  • Developed visualization Dashboards using sets, Parameters, Calculated Fields, Dynamic sorting, Filtering, Parameter driven analysis.  • Use Measure name and Measure Value fields to create visualizations with multiple measures and dimensions.  • Combine the visualizations into Interactive Dashboards and publish them to the Server    • Created the SharePoint folder and maintained all the documents of the project and code.  • Invloved in Status reporting on a daily & weekly basis of the project to ensure that all code is running with zero defects.  • worked in Reusability of the code and Performance Tuning activities of Datastage    Tools used: Datastage 11.7, Putty, IBM Datastudio 4.1(DB2),Mainframes,Tableau desktop 2019.1 Datastage developer/Tableau Developer Michaels Ecommerce Business Intelligence - Irving, TX September 2015 to August 2017 Project Description:  The Michaels Companies, Inc. is North America's largest provider of arts, crafts, framing, floral, wall décor, and merchandise for makers and do-it-yourself home decorators. The company owns and operates more than 1,250 Michaels stores, Aaron Brothers Custom Framing store-within-a-store, Artistree, a manufacturer of high quality custom and specialty framing merchandise; and Darice, a wholesale distributor to the craft, gift and décor industry.  The company's flagship is Michaels Stores. As of August 2018, the company reported that in addition to its Michaels Stores brand, it produces over a dozen private brands including Recollections, Studio Decor.  As part of Ecommerce Micheals used to sell their products through alternate channel Ebay.As part of existing architecture they are going to decommission channel through e bay and want to set up their own online store such that every transactions willhapen with in Michaels store.  Responsibilities:  • involved in understanding the existing Architecture and prepare the mapping based on the code.  • The work involved preparing the LLD mappings based on the Business from HLD on each subject area like Item Master,Inventory,POS(Point of Sales).  • The work involved Design and develop the jobs as per mappings with zero defects and good performance.  • Involved gathering requirement from client, ensuring quality deliverables, and giving technical solutions to the team.  • Worked on different files like XML files,sequentials files, datasets as part of datastage jobs design  • Data validation was done in various systems like WMS,TIPS, and also Data stage Jobs. Once data is validated data is send to downstream applications which is (WMS 2)  • Proficient in design and development of various dashboards, reports utilizing Tableau Visualizations like Bar chart, scatter plot, Dual axis charts, Water fall charts, Donut charts, Bubble charts, Heat maps, Line charts, Cross tab, Geographic Visualization(Maps), and making use of actions for interactivity, and global filters according to the business/client requirement.  • Worked extensively with Advance analysis Actions, Calculations, LOD's, Parameters, Background images and Maps in Tableau  • Set parameters for viewing data as daily, weekly, monthly, quarterly and yearly format.  • Developed Trend Lines, Reference Lines and statistical techniques to describe the data.  • Involved job execution and checking the logs and rectify them Through Datastage Director and ensure code runs with zero warnings and errors.  • Involved in Production support activities by monitoring the jobs From Control M. if any jobs fails then check in Unix script followed with Datstage Job name and fix accordingly.  • Created folder structure and uploaded all the documents in share point to keep track.  • Status reporting on a daily & weekly basis, defect capture, retesting and closure of defects.  Tools used: Datastage 11.3, Putty, Oracle SQL developer,Control M,Sql Assistant(Teradata),tableau Desktop 2018.1    Datastage version code Migration( 8.7 to 9.1) Software developer/Tableau Developer DNB Bank - Bergen, NO September 2014 to July 2015 Norway)  Project Description:  DNB ASA (formerly DnB NOR ASA) is Norway's largest financial services group with total combined assets of more than NOK 1.9 trillion and a market capitalisation NOK164 billion as per 20 May 2016. DNB's head office is located in Oslo. The two largest owners of DNB are the Norwegian Ministry of Trade and Industry and Sparebankstiftelsen DnB NOR. The latter was created as a foundation with the sole purpose of owning part of the company.  The main objective is to migrate the existing code running on Datastage 8.7 to 9.1 version such that all other systems run on same Unix. Also once the code migrated we need to validate the data from databases and other file systems.  Responsibilities:  • Was involved in the project from requirements analysis phase, interacted with customer as needed to get better understanding of requirements and responsible for Planning of test execution based on priority of the requirements for business.  • Involved in taking the back up of .dsx files from production and created all the folders and file systems in development of 9.1 version  • created the project folder and environmental variables and user defined variables in development environment of 9.1 version. then imported the .dsx file.  • After import compiled all the jobs using Multi jobs compile and ran the jobs by pointing the server to new version 9.1.  • performed SIT,UAT to ensure data is fine in both environments 8.7 and 9.1 and capture all the test results and documented accordingly.  • Implemented the same export and import of data from development environment of 9.1 to test environment and tested again.  • Finally deployed the code to production environment 9.1 and monitored the jobs using ControlM such that all jobs running fine without any errors and warnings.  • Fix the code accordingly if there are any issues come up while executing the jobs by monitoring the logs in Datastage Director.  Tools used: Datastage 8.7/9.1, Putty, DB2,CONTROLM. Software developer StarBucks DataFiltering - Seattle, WA October 2013 to 2014 Project Description:  starbucks Corporation is an American coffee company and coffeehouse chain. Starbucks was founded in Seattle, Washington in 1971. As of 2018, the company operates 28,218 locations worldwide.Starbucks is considered the main representative of "second wave coffee", initially distinguishing itself from other coffee-serving venues in the US by taste, quality, and customer experience while popularizing darkly roasted coffee. Since the 2000s, third wave coffee makers have targeted quality-minded coffee drinkers with hand-made coffee based on lighter roasts, while Starbucks nowadays uses automated espresso machines for efficiency and safety reasons.  The main objective is to refine datasets that are having junk and older data which is loading to database. so as part of existing jobs we are adding new extra logic in sql code such that junk data is removed and proper data is loaded to datasets.  Responsibilities:.  • Work involved Performance Tuning Techniques at Query level and Data stage job Level to Improve the Execution Time.  • The work involves formulate testing strategy, test planning, identifying test scenarios, test data set up, ensuring quality deliverables, and Design.  • Remodified the existing queries by adding some extra logic to remove unwanted data by filtering at date level so that refined data is loaded to datasets.  • Status reporting on a daily & weekly basis, defect capture, retesting and closure of defects.  • Created folder structure and uploaded all the documents in share point to keep track.  • Used Data Stage Director to identify the logs and rectify them with Zero errors.  • Extensively used data stage to check for logs and Performed Partioning(hash, same) as per required and used Oracle to query database.  Tools used: Datastage 8.7,Putty, Teradata,Sql plus,CRONTAB. Software developer Target-Enterprise Datawarehouse(EDW) February 2011 to 2013 Location: Minneapolis  Project Description:  Target Corporation is the eighth-largest retailer in the United States, and is a component of the S&P 500 IndexFounded by George Dayton and headquartered in Minneapolis, the company was originally named Goodfellow Dry Goods in June 1902 before being renamed the Dayton's Dry Goods Company in 1903 and later the Dayton Company in 1910. The first Target store opened in Roseville, Minnesota in 1962 while the parent company was renamed the Dayton Corporation  . The main objective is to load data from multiple systems to enterprise warehouse. this involves design of parallel jobs and followd with sequnecrs and Unix scripts accordingly.  Responsibilities:.  • Understanding the business functionality & Analysis of business requirements.  • Our responsibility starts from the point where a file from our source system lands in our server and ends when it is loaded to Target tables.  • Monitoring jobs and viewing logs for the jobs  • Running and Monitoring the Jobs in Director and performing Clean-up resources if needed.  • Used Plug-in Meta Data to import the source and target table definitions. Used most of the Stages in Data stage, such as the Transformer Stage, SCD Stage, Change Capture, Aggregator Stage, filter stage, job parameters Sequential file Stage.  • Used Lookup Stage efficiently for storing the primary rejected data for reference.  • Involved on Deployments i.e deploying code from DEV to UAT and UAT to PROD.  • The work involves gathering requirement from client, Preparing Mapping Specifications, formulate testing strategy, test planning, identifying test scenarios, test data set up, ensuring quality deliverables, and Design.  • Effectively interacts with clients  • Status reporting on a daily & weekly basis, defect capture, retesting and closure of defects.  • Created folder structure and uploaded all the documents in share point to keep track.  • Used Data Stage Director to identify the logs and rectify them with Zero errors.  • Coordinating with on-site team and involved in Oncalls, Project progress reviews.    Tools used: Datastage 8.7,unix(Putty), Sql plus,Teradata,Control M. Datastage developer Aetna-Predictive April 2009 to 2011 Location: Hartford  Project Description:    Aetna Inc.is an American managed health care company that sells traditional and consumer directed health care insurance plans and related services, such as medical, pharmaceutical, dental, behavioral health, long-term care, and disability plans, primarily through employer-paid (fully or partly) insurance and benefit programs, and through Medicare. Since November 28, 2018, the company has been a subsidiary of CVS Health.The company's network includes million medical members, 12.7 million dental members, 13.1 million pharmacy benefit management services members, 1,200,000 health-care professionals, over 690,000 primary care doctors and specialists, and over 5,700 hospitals  The goal of the Claims Fraud Analytics project is to identify the fraudulent claims from the information available from internal and external sources. In order to identify fraud, Selective Insurance has developed models based on a certain set of variables to score the claims. The Fraud Analytics Variable Development application was developed to pull information from various internal and external sources and present the information to the User to do mining on the data. This process also includes the scoring of all the current open claims on a daily basis.  Responsibilities:.  • Understanding the business functionality & Analysis of business requirements  • Designed and developed DataStage ETL Parallel Jobs between Source and Target using sequential file stage,dataset,oracle connector,sort,remove duplicates,aggregator,change capture,transformations accordingly.  • Import and export the .DSX from one environment to test for testing purpose.  • created the jobs using change capture to handle SCD Type 2 dimensions accordingly.  • Involved in promoting the code from DEV to UAT and to support QA team for each and every query's.  • Resolving the defects assigned by the QA and Business team.  • Involved in deploying code to UAT and PROD.  • Involved in UAT & Production support for the earlier releases for Daily loads, pre- verified and Monthly.  • Coordinating with on-site team and involved in Oncalls, Project progress reviews.    Tools used: Datastage 8.5,putty,sql plus,DB2,Notepad++,GIT Hub,Control M. Education Bachelor of Technology in Electronics and Communication Engineering Jawaharlal Nehru technology University College of Engineering Skills Hdfs, Datastage, Db2, Replication, Teradata, Database, Ms access, Oracle, Sql, Tableau, Data replication, Unix, Unix shell, Git, Hive, Real time, Xml, Itil, Retail, Excel Additional Information Technical Skills:  Languages: SQL,UNIX Shell Script, XML  Database: Oracle (9i,10g,11g), DB2,WINSQL(Netezza),Teradata(SQL Assistant)  BI Tools: IBM Datastage 11.x,9.x,8.x,7.5,Tableau 2019.1  Big data: HDFS,Hive  Real Time App: IBM CDC/IBM data Replication  ITIL Tools: Service now, ITSM  Scheduling tools: Control M, Zeke.  Utilities: Git hub,Zira.  Operating Systems: Windows XP/2007/Vista  Software / Applications: MS Word, MS Excel, MS Access, Outlook, PowerPoint  Business areas: Automobile, Retail,Banking,Insurance.