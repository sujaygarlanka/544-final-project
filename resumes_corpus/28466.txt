Hadoop Administrator Hadoop Administrator Hadoop Administrator - Lead IT Corporation Franklin, TN • 10+ years of experience in Information Technology with a strong background in application  development and maintenance/support and full project life cycle using Microsoft .NET  Technologies including Unit Testing, Client Interaction and handling functional, technical  queries.  • 4+ years of experience in BIG DATA Hadoop Administration and Development.  • Hands-on experience in designing and implementing solutions using Hadoop, HDFS, Map  Reduce, HBase, Hive, Pig, Spark, Oozie, Tez, Yarn, Sqoop, Solr, Zookeeper  • Experience in Configuring Name-node High availability and Name-node Federation  • Experience in Disaster recovery and Backup activities  • Experience in Multi-node setup of Hadoop cluster  • Experience in Performance tuning and benchmarking of Hadoop Cluster  • Experience in Monitoring, maintenance and troubleshooting of Hadoop cluster.  • Experience in Security integration of Hadoop Cluster.  • Good knowledge on Kerberos Security.  • Setting up and integrating Hadoop eco system tools - HBase, Hive, Pig, Sqoop etc.  • Making Hadoop cluster ready for development team working on POCs.  • Experience in deploying Hadoop 2.0(YARN).  • Enabling and managing various components in Hadoop Ecosystem like HDFS, YARN, Map  Reduce, Hive, Pig, Sqoop, Oozie, Sentry, Spark and Zookeeper.  • Experienced in configuring, installing, upgrading and managing Cloudera, Apache &  Hortonworks Hadoop Distributions.  • Familiar with writing Oozie workflows and Job Controllers for job automation - Hive  automation.  • Hands on experience in analyzing log files for Hadoop and eco-system services and finding  root cause.  • Hands on experience in Installing, Configuring and managing the Hands on experience in  analyzing log files for Hadoop and eco-system services and finding root cause.  • Hands on experience in Installing, Configuring and managing the HCatalog.  • Experience in importing and exporting the data using Sqoop from HDFS to Relational Database  systems and vice-versa.  • Optimizing performance of HBase/Hive/Pig jobs.  • Experience with Spark Streaming, Sql, MLib, GraphX and integrating Spark with HDFS,  and HBase.  • Experience in tuning and debugging Spark application running.  • Experience integration of Kafka with Spark for real time data processing.  • Hands-on-experience on ZKFC in managing and configuring the Name Node failure scenarios.  • Experience on Commissioning, Decommissioning, Balancing, and Managing Nodes and tuning  server for optimal performance of the cluster.  • Experience in importing and exporting the data using Sqoop from HDFS to Relational Database  systems and vice-versa.  • Optimizing performance of HBase/Hive/Pig jobs.  • Experience in understanding the security requirements for Hadoop and integrating with  Kerberos authentication infrastructure- KDC server setup, creating realm /domain,  managing.  • Good Experience in using SQL Server 2008/2005 and its tools like SQL Server Reporting  Services (SSRS) & SQL Server Integration Services (SSIS).  • Well Proficient in writing Views, Stored Procedures, Functions MS SQL Server and Oracle.  Balaram Sekuboyina  ( bs.hadoopadmn@gmail.com 513-724-4666) Work Experience Hadoop Administrator Lead IT Corporation December 2017 to Present Nissan was the sixth largest automaker in the world. Nissan leveraged the Austin patents to further develop their own modern engine designs. Nissan management realized their Datsun small  car line would fill an unmet need in markets. Nissan tried to convert the Greek plant into one  manufacturing cars for all European countries. The Renault-Nissan Alliance has evolved over years, the alliance itself is incorporated as the Renault-Nissan B.V Group. For many years, Nissan used a  red wordmark for the company, and car "badges" for the "Nissan" and "Infiniti" brands.    Nissan maintains all the data related to Cars, sales, features of the cars, faults of the cars when occurred using the telematics data and warranty of each vehicle depends of the type of cars.  All the data will is collected will be stored using using Hadoop environment by using  Hortonworks.This connects which various sources like Informatica, Tableau to the get data  maintained in Hadoop Cluster. This uses a Hortonworks Hadoop Cluster.    Responsibilities:  • Upgraded Hortonworks HADOOP Cluster from 2.5 to 2.6.5 for development, staging, pre- prod and Prod with most of the available services in the Hadoop Ecosystem  • Installed HDF Cluster in the all development, staging, pre-prod and Prod with most of the available services in the Hadoop Ecosystem  • Installed System Security Services Daemon on all the environments to sync the Active  Directory directly to the cluster and remove the local users  • Administered Cluster maintenance, commissioning and decommissioning Data nodes, Cluster Monitoring, Troubleshooting.  • Performed Adding/removing new nodes to an existing Hadoop cluster  • Defining the cron jobs to run the automated jobs at desired intervals.  • Creating the policies in Ranger to make the cluster robust without any issues.  • Installed different softwares likes subversion,Veracrypt,Git for better performance of the cluster  • Proposing tools like Pycharm which helps the developers for better debugging of the code.  ( bs.hadoopadmn@gmail.com 513-724-4666)    • Making the cluster available for various data sources for easy data access and make sure all  the connections are reachable.  • Scheduling the batch process jobs in the cluster and distribute the cluster among all jobs  • Creating alerts on Ambari based on the criticality.  • Creating the coding standards for the developers, reviewing the codes and deploying the codes in different enviroments.  • Overseeing the smooth execution of the code into the cluster on the three environemnts  of minor and major enhancements    Environment: HDP 2.6.5, Ambari 2.6.2, HADOOP, HDFS, Zookeeper, Map Reduce, YARN, Scala, Spark, Python, HBASE, Hive, SQOOP, OOZIE, Linux- CENTOS, UBUNTU, Red Hat. Hadoop Administrator CrBard - Houston, TX December 2016 to August 2017 CRBard is health care manufacturing equipment's for patients and healthcare professionals  in wellness and prevention, early diagnosis, treatment, and post-care management. In fact, throughout our history, Bard has lead the industry in groundbreaking devices and therapies that  continuously seek to set the new standard for excellence and quality.  A leading multinational developer, manufacturer, and marketer of innovative, life-enhancing  medical technologies in the fields of vascular, urology, oncology, and surgical specialties.  Developing innovative medical devices, for more than 100 years that meet the needs of clinicians  and their patients, with more than 75 locations worldwide. Pioneered the development of single- patient-use medical products for hospital procedures today it is dedicated to pursuing technological  innovations that offer superior clinical benefits while helping to reduce overall costs    Responsibilities:  • Installed and configured Hortonworks HADOOP Cluster from scratch for development, staging, pre-prod and Prod with most of the available services in the Hadoop Ecosystem  • Administered Cluster maintenance, commissioning and decommissioning Data nodes, Cluster Monitoring, Troubleshooting.  • Performed Adding/removing new nodes to an existing Hadoop cluster.  • Implemented Backup configurations and Recoveries from a Name Node failure.  • Monitored systems and services, architecture design and implementation of Hadoop  deployment, configuration management, backup, and disaster recovery systems and procedures.  • Configured various property files like core-site.xml, hdfs-site.xml, mapred-site.xml based  upon the job requirement.  • Performed Importing and exporting data into HDFS using SQOOP.  • Installed and configured HDFS, Zookeeper, Map Reduce, Yarn, HBASE, Hive, SQOOP and OOZIE.  • Integrated Hive and HBASE to perform analysis on data.  ( balaramsekuboyina.k2@gmail.com 513-724-4666)    • Managed and reviewed Hadoop Log files as a part of administration for troubleshooting  purposes. Communicated and escalated issues appropriately.  • Applied standard Back up policies to make sure the high availability of cluster.  • Involved in analyzing system failures, identifying root causes, and recommended course of actions. Documented the systems processes and procedures for future references.  • Worked with systems engineering team to plan and deploy new Hadoop environments and expand existing Hadoop clusters.  • Support pre-production and production support teams in the analysis of critical services and assists with maintenance operations.  • Improve system performance by working with the development team to analyze, identify  and resolve issues quickly.  • Automate administration tasks with shell scripting and Job Scheduling using CRON.  • Performed many Proof of Concepts to the client and suggested different tools available in the Hadoop Ecosystem.    Environment: HDP 2.6, Ambari 2.5, HADOOP, HDFS, Zookeeper, Map Reduce, YARN, Scala, Spark, Python, HBASE, Hive, SQOOP, OOZIE, Linux- CENTOS, UBUNTU, Red Hat. Application Software Developer/Hadoop Administrator Accenture, Hyderabad - Houston, TX January 2011 to November 2016 Shell Oil Company is the United States-based subsidiary of Royal Dutch Shell, a multinational "oil major" of Anglo-Dutch origins, which is amongst the largest oil companies in the  world. Shell products include oils, fuels, and card services as well as exploration, production, and refining of petroleum products. Shell Oil operates as a subsidiary of Royal Dutch/Shell Group, the  second largest oil company in the world. In 1999, Shell Oil and its U.S.-based counterparts secured  22 percent of the Group's income.  Shell is a global group of energy and petrochemicals companies with around 90,000  employees in more than 80 countries and territories. Shell innovative approach ensures to help  tackle the challenge of new energy future. Shell energy business is spread across 3 areas - Upstream, Midstream and Downstream. Shell has requirements to automate many standard  manual processes to automate using workflow technology. This enables lot of work spread among the teams to get chance to work on multiple workflows and deliverables. Shell is recommending to  develop workflows in SharePoint, K2. SharePoint team handled the following workflows:    Responsibilities:  • Analyze customer orders, set delivery priorities and make schedule adjustments to meet  timely delivery goals  • Worked on Intranet site and prepared plan for migration from SharePoint 2010 to 2013 and SharePoint online/365  • Installed and configured Hadoop Map Reduce, HDFS, Developed multiple Map Reduce jobs  in java for data cleaning and preprocessing.  • Worked on Installing and configuring the HDP Hortonworks 2.x Clusters in Dev and Staging Environments  • Worked on installing and configuring HDP 2.3, HDP 2.4, HDP 2.5 and HDP 2.6  • Worked on Capacity planning for the Production Cluster  • Involved in loading data from UNIX file system to HDFS using Sqoop.  • Involved in creating Hive tables, loading the data and writing hive queries, which will run  internally in, map reduce way.  ( balaramsekuboyina.k2@gmail.com 513-724-4666)    • Worked on Configuring Oozie Jobs.  • Worked on Configuring High Availability for Name Node in HDP 2.x.  • Worked on Configuring Kerberos Authentication in the cluster.  • Worked on cluster upgradation in Hadoop from HDP 2.1 to HDP 2.3.  • Worked on Configuring queues in capacity scheduler.  • Created tables, loaded data, and wrote queries in Hive.  • Monitored cluster using Ambari and optimize system based on job performance and criteria.  • Managed cluster through performance tuning and enhancements.  • Formulated procedures for installation of Hadoop patches, updates and version  upgrades.  • Ensured data recoverability by implementing system and application level backups.  • Worked on installing and configuring Solr 5.2.1 in Hadoop cluster.  • Worked on taking Snapshot backups for HBase tables.  • Responsible for Cluster maintenance, Monitoring, commissioning and decommissioning Data  nodes, Troubleshooting, Manage and review data backups, Manage & review log files.  • Day to day responsibilities includes solving developer issues, deployments moving code  from one environment to other environment, providing access to new users and providing  instant solutions to reduce the impact and documenting the same and preventing future  issues.  • Adding/installation of new components and removal of them through Ambari.  • Collaborating with application teams to install operating system and Hadoop updates, patches, version upgrades.  • Monitored workload, job performance and capacity planning  • Involved in Analyzing system failures, identifying root causes, and recommended course of actions.  • Responsible for resolving the issues with the specified SLA.  • Involved in Migration of k2 from 4.5 to 4.6.5  • Reduced the time out errors in K2.  • Involved in the K2 Archiving Process  • Extensively worked on Disaster Recovery Plan in K2  • Creating the deployment packages in K2  • Handling different cloud machines to maintain K2 the applications.  • Created Stored Procedures to support Daily scheduled Feeds by providing necessary Data  for the creation of Data Files.    Environment: HDP 2.X, CDH, Apache Hadoop, Hive, Pig, Flume, Oozie, Sqoop, MS Visual Studio  2012, MS. Net Framework 4.0, C#, Angular.js, K2 workflows, Smart Objects, SQL Server Oracle web logic Developer Procter and Gamble - Cincinnati, OH November 2009 to December 2010 Procter & Gamble Co., also known as P&G, is an American multinational consumer  goods company headquartered in downtown Cincinnati, Ohio, United States. P&G announced it was  streamlining the company, dropping around 100 brands and concentrating on the remaining 80  brands, which produced 95 percent of the company's profits. The company began to build factories in other locations in the United States because the demand for products had outgrown the capacity  of the Cincinnati facilities. Procter & Gamble acquired a number of other companies that diversified  its product line and significantly increased profits.  ( balaramsekuboyina.k2@gmail.com 513-724-4666)    Procter and Gamble's Global Business Services, Decision Cockpits is responsible for integrating data from across P&G to provide analytic reporting to enable decision making. In order to do this in a quick, efficient manner, P&G has put in place a strategy to create "Decision Cockpit"  (DC). Decision Cockpit 3.0 & 3.5, is to increase adoption among end users and meet the business  reporting needs of end users. DC 3.5 will add additional value by providing Personalization (Look & Feel) to employees and services tools such as travel, calendar, learning, and performance  resources.    Responsibilities:  • Involved in Functional, Integration, Regression, Smoke and System testing and logging  defects.  • Developed and Deployed InfoPath forms which involve creating InfoPath forms that retrieve  data directly from the Monsanto (End Client) Test SQL Server database.  • Developed Office SharePoint Server 2007 InfoPath Services to enable the interaction of the end users through a Web browser. Configured the custom workflows to InfoPath forms.  • Design and Developed the functionality to Add, Edit and Delete the existing items in the SharePoint list through a InfoPath form.  • Created SharePoint Features for enabling InfoPath form And Used the SharePoint Solutions  for creating the packages and Deployment  • Analyzed the System, Functional Requirement document and written Test Scenarios for the functionality testing of the application.  • Ensured that all the test cases are updated.  • Developed Portlets based on the client's requirements and assigning the security to the portlet based on the availability of the regions and users.  • Involved in doing the Build Reviews and Peer Reviews once the build is completed.  • Worked on SharePoint for security, quota to the sites.  • Reviewed tests specifications, test cases and performed manual testing.  • Involved in doing Smoke test once the production is completed.  • Providing Test status updates and daily test progress reports.  • Used Team Foundation Server (TFS) 2005/2008 as version/change management control.  • Testing of UAT defects in test environment before, the new code gets moved to UAT  environment.  • Preparing the Migration Documents with no issues from the production and Involving with the client team during the production Migration  • Created LLD's, HLD's and test case documents.  • Created documents like code review check list, Migration check list.  • Involved in all the existing releases and helped the team in resolving critical issues.  • Worked with the QA team to test the code.    Environment: Oracle web Logic, SharePoint Vignette Developer Nokia - Chennai, Tamil Nadu June 2008 to October 2009 The company currently focuses on large-scale telecommunications infrastructures, and technology development and licensing. Nokia is also a major contributor to the mobile telephony  industry, having assisted in development of the GSM and LTE standards, and was for a period, the largest vendor of mobile phones in the world. Nokia's dominance also extended into the  smartphone industry through its Symbian platform. Nokia eventually entered into a pact with Microsoft in 2011 to exclusively use its Windows Phone platform on future smartphones.  ( balaramsekuboyina.k2@gmail.com 513-724-4666)    The purpose of this Project is to create Web Sites across 58 countries for every phone they  release and for every accessory which are compatible for that phone. Creating Nokia maps which has got the highest reputation during the recent times.    Responsibilities:  • Created the content items in the Content Management System (CMS).  • Arranged the contents in the Site Management(SM).  • Involved in doing the Peer Reviews once the migration is completed.  • Created documents which helped for quick reference for the new joiners  • Involved in resolving the critical UAT issues.  • Involved in all the existing releases and helped the team in resolving critical issues.  • Worked with the QA team to reduce defects before moving to UAT.    Environment: Vignette. Accenture, Hyderabad - Hyderabad, Telangana 2005 to 2008 Education Bachelor of Technology in Computer Science & Engineering in Computer Science & Engineering PVP Nagarjuna Institute of Technology - Vijayawada, Andhra Pradesh August 2007 Skills Oozie, Sqoop, Hbase, Hadoop, Map reduce Certifications/Licenses Driver's License Additional Information TECHNICAL SKILLS:  Languages: C#.NET, VB.NET, JavaScript, JQuery.  ASP.NET, Angular.js ,Silverlight, XML, HTML, CSS.  Web Technologies: SQL Server 2012/2008/2005, MS ACCESS.  Databases: Visual Studio 2010/2008/2005,Sharepoint Online, SharePoint  IDE: 2013/2010,MOSS 2007  3rd Party Toolkits: K2 Black Pearl, Smart Forms, Vignette, ALUI (Aqua Logic User  Interface.)  Reporting Tools: SQL Server Reporting Services,  Version Control: Team Foundation Server.  Distribution: Hadoop  Frameworks  Hadoop: Hortonworks HDP 2.X , Cloudera  Distributions Map Reduce, HBase 1.1, Hive1.2, Sqoop 1.4.6, Pig 0.16,  Hadoop: Oozie 4.2.0  Technologies