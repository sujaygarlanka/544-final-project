Hadoop/spark Developer Hadoop/spark <span class="hl">Developer</span> Hadoop/spark Developer - BCBSA Morrisville, NC Work Experience Hadoop/spark Developer BCBSA - Chicago, IL January 2019 to Present Project Description: This project is based on webservices and FHIR response to get the prior member coverage for members when provided with member id, plan id and product id. It utilizes the Hadoop cluster for data storage and uses.  • Responsibilities:  • Work closely with the business and analytics team in gathering the system requirements.  • Data ingestion into HDFS from various mainframe Db2 table using Sqoop.  • Skilled on migrating the data from different databases to Hadoop HDFS and Hive using Sqoop.  • Analyzed large structured datasets using Hive's data warehousing infrastructure.  • Extensive Knowledge of creating manages tables and external tables in Eco system.  • Implement Hive UDF's for evaluation, filtering, loading, and storing of data.  • Involved in writing the Hive scripts to reduce the job execution time.  • Used all major ETL Transformations to load the tables from mapping.  • Importing & exporting historical data from Db2 to HDFS, Hive.  • Design managed and External tables in Hive to optimize performance to improve performance.  Environment: HDFS, Hive, Sqoop Import/Export, Spark, Hue, Oozie, ETL, Datawarehouse, UNIX, Cloudera. spark Developer Hadoop - Chicago, IL December 2017 to November 2018 Project Description: all scripts are specialized in designing and developing the high-performance production software with state-of -art computer vision capabilities. The increase of data made the existing databases un accommodable. So, all scripts Leap wants to move it to Hadoop, where exactly we can handle massive amount of data by means of its cluster nodes and to satisfy the scaling needs of the business operation.  Responsibilities:  • Extracted the data from the flat files and other RDBMS databases into staging area and ingested to Hadoop.  • Installed and configured Hadoop Map-Reduce, HDFS and developed multiple Map-Reduce jobs in Java for data cleansing and preprocessing.  • Importing and exporting data into HDFS and Hive using Sqoop from MySQL.  • Responsible for Coding batch pipelines, Restful Service, Map Reduce program, Hive query's, testing, debugging, Peer code review, troubleshooting and maintain status report.  • Involved in migrating tables from RDBMS into Hive tables using SQOOP and later generate visualizations using Tableau.  • Involved in creating Hive tables, loading with data and writing hive queries.  • Installed Oozie workflow engine and scheduled it to run data/time dependent Hive and Pig jobs.  • Environment: HDFS, MapReduce, Cassandra, Hive, Pig, Sqoop, Tableau, NoSQL, Shell Scripting, Git, HDP Distribution, Eclipse, Linux. Developer Hadoop - Dearborn, MI December 2016 to November 2017 Project Description: This is a Web based project, developed for providing services to their clients at different places. This application gathers the information, which is maintained in various service stations. The information at a service center is technically processed based on the warranty. The processed information is sent to the hub for placing the parts order for replacement. This project is of two modules Customer Management and service provider's management. This project also monitors and maintains the manifesto regarding the details of the inventory.  • Responsibilities:  • Used all ETL transformations to load a table through Ab Initio mapping documents.  • Used Hive as ETL tool to do transformations, joins and some pre-aggregations before storing the data onto HDFS.  • Worked on importing and exporting data from Oracle data into HDFS using SQOOP for analysis, visualization and to generate reports.  • Creating Hive external tables and partitioned tables using Hive Index and used HQL to make ease of data analytics.  • Provided Oracle development solutions as SQL loader, merge logic with insert/update, views, Materialized views, and generated PLSQL blocks with procedures, functions and cursors.  • Used TOAD to create, execute and Optimized SQL queries to analyze the data and create various DDL and DML scripts on daily activities.  • Worked on Hive for exposing data for further analysis and for generating transforming files from different analytical formats to text files.  • Used forward engineering to create a Physical Data Model with DDL that best suits the requirements.  • Worked with Sqoop to export analyzed data from HDFS environment into RDBMS for report generation and visualization purpose.  • Maintaining and monitoring clusters. Loaded data into the cluster from dynamically generated files using Flume and from relational database management systems using Sqoop.  • Environment: Cloudera, Cloudera Manager, HDFS, Map Reduce, Hive, Impala, Pig, Python, SQL, Sqoop, Flume, Yarn, Linux, Centos, HBase. Java developer NebuLogic Technologies - Hyderabad, Telangana January 2014 to December 2015 Project Description: Nebulogic is a provider of business outsourcing solutions. They provide Services such as Human Capital Management, Tax & Compliance, Electronic Payment Solutions, Dealer Services and Medical Practice Services. This project is focused on Human Capital Management and particularly on Payroll Services, Human Resource Management, Talent Management, Benefits Administration, Time & Attendance, and International Solutions. This front-end application has interface with various Java applications, which are used to send and receive information to third party vendors.  Responsibilities:  • Involved in Full Life Cycle Development in Distributed Environment Using Java and J2EE framework.  • Responsible for developing and modifying the existing service layer based on the business requirements.  • Involved in designing & developing web-services using SOAP and WSDL.  • Involved in database design.  • Created tables, views, triggers, stored procedures in SQL for data manipulation and retrieval  • Developed Web Services for Payment Transaction and Payment Release.  • Involved in Requirement Analysis, Development and Documentation.  • Developed front-end using JSP, HTML, CSS and JavaScript.  • Coding for DAO Objects using JDBC (using DAO pattern).  • XML and XSDs are used to define data formats.  • Implemented J2EE design patterns such as singleton, DAO for the presentation tier, business tier and Integration Tier layers of the project.  • Involved in Bug fixing and functionality enhancements.  • Followed coding and documentation standards and best practices.  • Participated in project planning discussions and worked with team members to analyze the requirements and translate them into working software modules.  • Environment: Java, J2EE, JSP, SOAP, WSDL, SQL, PL/SQL, XML, JDBC, Eclipse, Windows XP, Oracle Education Master's Skills HDFS, MAPREDUCE, SQOOP, HBASE, HADOOP, C#, C/C++, C++, Git, Hadoop, HBase, Hive, JAVASCRIPT, MapReduce, Pig, Ruby, ZooKeeper, DATABASE, MICROSOFT SQL SERVER, MICROSOFT SQL SERVER 2005 Certifications/Licenses Driver's License Additional Information TECHNICAL SKILLS:  Hadoop: Hive, HBase, MFS, HDFS, MapReduce, YARN, Spark, Sqoop, Pig, Cloudera Manager, Zookeeper  Tool(s): Apache Tomcat 7.0, Maven, Git, Hibernate, Microsoft SQL Server Management Studio, SQL Developer, MySQL Workbench, Eclipse  Language(s): Java 7/8, Scala, C#, C/C++, JavaScript, ABAP/4, Ruby, HTML5, CSS3 Database(s): Oracle 10g/11g/12c, MySQL, Microsoft SQL Server 2005/2008