(spark) & python developer (spark) &amp; <span class="hl">python</span> <span class="hl">developer</span> (spark) & python developer - TATA CONSULTANCY SERVICES LIMITED, Hyderabad A result oriented professional with over 2 years of experience as HADOOP DEVELOPER. Presently working with TATA CONSULTANCY SERVICES LIMITED as Developer in Banking & Financial Domain.    • 2 years of extensive experience on Big Data and Big Data analytics.  • Having 2 years of hands on experience on Python.  • Having hands on experience in using Hadoop Technologies such as HDFS, HIVE, SQOOP, Impala, Flume, Solr.  • Having hands on experience in writing Map Reduce jobs in Hive, Pig.  • Having experience on importing and exporting data from different systems to Hadoop file system using SQOOP.  • Using Hadoop ecosystem components for storage and processing data, exported data into Tableau using Live connection.  • Having experience on creating databases, tables and views in HIVEQL, IMPALA and PIG LATIN.  • Having experience on using OOZIE to define and schedule the jobs.  • Having experience on Storage and Processing in Hue covering all Hadoop ecosystem components.  • Having good experience on using Tableau, Qlikview Reporting Tools.  • Having knowledge on Zookeeper.  • Experience in working with different data sources like Flat files, XML files and Databases.  • Having experience on Sentiment Analysis.  • Having experience on Managing HDFS file system.  • End to end system implementation and post production support.  • Having good skill for project development using methodologies agile, waterfall etc.  • Excellence in learning new technologies, team building, team management and smart working. To be part of an organization which progresses dynamically renders creative and challenging environment. Put my expertise to the best use towards the growth of the Organization. Work Experience (spark) & python developer TATA CONSULTANCY SERVICES LIMITED, Hyderabad January 2018 to Present Description:  USAA, a leading financial services group of companies have widely horizontal industries in Insurance, Banking and Finacial services. They have planned to migrate their customer and other related informations to migrate to Hadoop and do some data analysis and process the large data with the help of spark.  Data migration from different relational database like Netezza, DB2 . and through datastage.  With the help of spark sql we are processing the large amount of data and storing into a staging area then as per our service agreement we have migrated the data from staging area to the Final table using Hive.    Responsibilities:  • Data migration from DB2 & Netezza using Sqoop and keep data in Hive  • Data migration from datastage and parking it in Hive  • Creating spark dataframes according to SLA for processing the data and storing into staging area  • Writing spark code to apply transformations using Python.  • Using Python's different modules for integrate Hive with spark.  • Using python's class approach for wrapping the script and move that into productio Hadoop Developer TATA CONSULTANCY SERVICES LIMITED, Hyderabad September 2016 to Present Accomplishments:  • Single point of contact between Business team and Developers from solution planning, sizing, to fulfilment.  • Working on various Projects with High end experience in Hadoop and Python development.  • Working as a developer Hadoop Ecosystems (HDFS, Hive, Sqoop & Tableau) Spark developer TATA CONSULTANCY SERVICES LIMITED, Hyderabad December 2016 to December 2017 Client: Trans America, US    Description:  Transamerica, a leading provider of life insurance, retirement and investment solutions, to enable the transformation of administration of its U.S. insurance and annuity business lines.TCS BaNCS, which is a insurance digital platform is partner with Transamerica in its ongoing transformation.  TCS BanCs push the datas (Structure data) in the form of flat files to edge node. Edge node work as the gateway from other network to hadoop environment so that it will not face the other network issue (other network can't touch the data).  Then from edge node or also known as sftp server, all the flat files in being pushed to hadoop server.  All the files has been placed in database known as hbase.Phoenix on top of hbase is use because Phoenix is an open source SQL skin for HBase.We can use standard JDBC APIs instead of the regular HBase client APIs to create tables, insert data, and query our HBase data.Based on the business requirement, some logics and trasformations should apply on Phoenix tables and need to store in warehouse called as hive.Finally busniess extract need to created and given to customers.    Responsibilities:  • Acquire the data from source connections (File/DB) and Keep the data in Hive.  • Transform the data by applying pre-defined transformation rules as well as custom rules.  • Write Spark code to apply the transformation as well as frame SQL Query using java.  • Created custom rules by using Java.  • Execute queries using SQL/HQL and generate the resultant data    Project 2: Education B.Tech Gandhi Institute of Engineering and Technology 2012 to 2016 Additional Information TECHNICAL SKILLS  Databases: Oracle 11g, MYSQL, Netezza, DB2  Operating systems Windows, Linux  Servers Weblogic 12c  Integrations: Webservices  Hadoop Technologies HDFS, Hive, Impala, Sqoop, Pig, Flume, Solr, oozie, Zookeeper, HBase, SPARK, PYTHON  Methodology Agile Methodology  Reporting Tools Tableau, IBM Cognos