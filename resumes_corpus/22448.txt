Programmer Analyst Programmer Analyst Programmer Analyst - PepsiCo, Inc • Around 11+ yrs. of IT experience in Software Analysis, Development, and Implementation of Data Warehousing and Mainframe Applications for Insurance, Finance and Retail verticals.  • Proficient in Informatica Power Center 10.1/9.1/8.x/7.x / 6.2 / 5.1, Informatica Data Quality(IDQ) 9.5.1, Informatica Power Mart 5.1 / 4.7  • Having strong application development skills, and mainframe and DW technical experience in Informatica, COBOL, DB2, CICS, VSAM, JCL, Maestro, CA7  • Experience working in Onsite/Offshore model.  • Extensive experience in Dimensional Data modelling using Star and Snow Flake schema.  • Expertise in working with relational databases such as Oracle, Teradata, IBM UDB DB2  • Extensive experience in developing Stored Procedures, Functions, Views and Triggers, Complex SQL queries using Oracle PL/SQL.  • Experience in writing UNIX shell scripting and automation of the ETL processes using UNIX shell scripting.  • Good in understanding Business process/requirements and translating them into technical requirements.  • Performed all dimensions of development including Extraction, Transformation, and Loading (ETL) data from various sources into Data Warehouses and Data Marts using Informatica Power Center (Designer, Workflow Manager, and Workflow Monitor).  • Experience in integration of various data sources like SQL Server, MS Access, Oracle, DB2, Cobol Files and Flat files.  • Extensively used Power exchange to import source files from Mainframe systems  • Experience in PL/SQL stored procedures along with tools like TOAD, Rapid SQL  • Well versed in developing the complex SQL queries, unions and multiple table joins and experience with Views.  • Extensively used Teradata (BTEQ, FLOAD, MLOAD,TPT)  • Extensively used Bteq scripts to update the status and loading Landing zone tables of the load for Audit purposes  • Good understanding of Teradata parallel architecture and its physical implementation  • Sound knowledge in understanding of Star Schema, Snowflake Schema using Data Modeling Tool Erwin.  • Knowledge in Teradata and its utilities.  • Extensively Used Scheduling tools like Maestro,WLM,CA-7  • Proficient in all phases of the System Development Lifecycle (SDLC) - Analysis, Design and Modeling, System Implementation, System Testing, Acceptance and Maintenance  • Ability to work with team as well as independently.  • Team player with Analytical, Problem-Solving and Excellent Communication Skills. Good Learn ability. A fast learner with an open mind to learning new technologies.  • High level of Self-Motivation and an ability to motivate others. Work Experience Programmer Analyst PepsiCo, Inc November 2017 to Present PepsiCo is a global food and beverage leader with a product portfolio including 22 brands that generate more than $1 billion each in annual retail sales.  Quaker, Tropicana, Gatorade, Frito-Lay and Pepsi-Cola - make hundreds of enjoyable foods and beverages that are loved throughout the world.    The main objective of the project is to make selling data available for Walmart which can be used in analytical tools like tableau so that insights can be gathered to allow more products to be sold.    Responsibilities:  • Gathering requirements and data mart designs from Business user and Analytical teams and translating the same into useable blueprint  • Determine structural requirements by analyzing the requirements  • Develop database design and architecture documentation for the teams  • Participate in modeling discussions and share the inputs  • Extensively used Teradata Sql Assistant 15.0 to analyze the data, to build views and validation of data in Dev, QA and PROD.  • Created View's (Joining different Sources based on Requirement) in Teradata for Tableau to access and create dashboards.  • Perform business & functional analysis for project requirements.  • Coordinate, gather and evaluate the process requirements of PepsiCo to Implement the Software Solutions.  • Conduct Design Review and other related Meetings for Master Data Management projects.  • Translate business objectives, design documents into clear Technical /functional requirements.  • To interact with the client on critical issues, manage customer expectations and align customer business goal with Cognizant.  • Responsible to develop and maintain code using tools Informatica power center, UNIX, Control-m scheduler,Teradata.  • Responsible for technical design, development, unit/system testing of ETL mappings and scripts.  • Conduct peer reviews with the team for design, coding, integration and system testing to ensure 100% accuracy    Environment: Informatica 10.1, SQL, Teradata, SQL Assistant 15.0, UNIX, SSH, Control-M Informatica Developer StateFarmInsurance - Richardson, TX July 2015 to September 2017 As part of new CDE ("Customer Driven Evolution") strategy, OM ("Opportunity Management") has established several marketing initiatives. One of which is to create NAR ("Need Analysis and Recommendation") marketing database for campaign management application (SAS) to operates. Worked in Multiple projects and Loaded consolidated data into EDW    Responsibilities:  • Designing, developing informatica mappings and jobs  • Performing code migration from lower environment to higher.  • Analyzing the data anomalies and defect fixes.  • Created UNIX scripts to perform operations like gun zip, remove and touch files and SecureFTP file to business user.  • Worked with Pre-Session and Post-Session UNIX scripts for automation of ETL jobs. Also involved in migration/conversion of ETL processes from development to production.  • Developed, tested Stored Procedures, Cursors, Functions and Packages using PL/SQL for Data ETL.  • Extensively used Trac to log the Incidents and tracking, Fix the Incident  • Experience working with customers and peers to develop solutions to complex business problems  • Extensively worked on loading and extracting data from Oracle,DB2 database  • Used Workflow manager for Creating, Validating, Testing and running the sequential and concurrent Batches and Sessions  • Migration and supported the data migration from current system to IBM Unica for Campaign management  • Created Test Data and executed SQL queries in SQL loader to validate and test the data  • Unit Testing has been extensively done at development level  • Responsible for Unit Testing and Integration testing of mappings and workflows.  • Extensively Used Control-m scheduling tool to schedule daily, weekly and monthly jobs    Environment: Informatica 9.6/10.1, CA7,SQL,Oracle, TOAD, IBM Control Center, IBM Unica,UNIX,IpswitchWS_FTP12,FileZilla Informatica Developer Allegis Group Services Inc - Hanover, MD February 2014 to June 2015 Allegis Group is the largest privately held staffing company in the United States and serves a wide variety of industries. Allegis Global Solutions represents the union of Allegis Group Services - a subsidiary of Allegis Group, Inc., the largest privately held staffing and recruiting services company in the US - and Australian-based Talent2 RMS, the leading HR and recruitment provider in Asia Pacific.    The goal of this project is to cleanse the report data from the Vendor Management Systems like Field Glass, Beeline, IQN etc. and store in a data repository and generate invoice in PDF files for each client and supplier separately.    Responsibilities:  • Interacting with SFDC (salesforce.com) to retrieve the parameter required in Informatica.  • Worked on SFDC objects  • Worked on generating invoices for different clients like Microsoft, TD Bank, Alere, Rackspace, Amgen, Fanniemae, and Experian based on their different business requirements.  • Worked on data cleansing and standardization using the cleanse functions in IDQ 9.5.1  • Worked with Informatica Data Quality IDQ 9.5.1 Developer/Analyst Tools to remove the noise of data using different transformations like Standardization, Merge and Match Case Conversion Consolidation, Parser, Labeler, Address Validator  • Created Reference/Master data for profiling using IDQ Analyst tools. Used the Address Doctor Geo-coding table to validate the address and performed exception handling reporting and monitoring the data.  • Performed match/merge and ran match rules to check the effectiveness of IDQ process on data.  • Configured Address Doctor which can cleanse address Data and enhanced by making some modifications during installation  • Executed, scheduled workflows using Informatica Cloud tool to load data from Source to Target.  • Extensively used B2B data transformation studio to develop DT services to convert Excel to CSV and to PDF  • Generated Self bills for Microsoft Sweden, Microsoft Norway and Microsoft Netherlands in PDF format using Macros and Informatica.  • Extensively Used Labeler and Standardizer for Data cleansing and Data standardization and Data harmonization  • Understanding of source systems thoroughly by going through the Integration specification documents and one-to-one interaction.  • Used transformations like Unstructured Data transformation, Aggregator, Filter, Router, Sequence Generator, lookup, Expression, update strategy and Union, Transaction Control to meet business logic in the mappings.  • Created workflows to run sessions sequentially one after the other and concurrent worklets to start all the sessions in the workflow at once.  • Extracted data from Source files transformed and loaded into the target using Informatica.  • Loaded the data into XML targets using XML target Definition.  • Developed various Mappings with the collection of all Sources, Targets, and Transformations using Designer.  • Used Debugger to monitor data movement, identified and fixed the bugs/issues.  • Worked on Xcel file Source file convert Xcel file to CSV using VBA Macros, Data Transformation  • Created target .CSV flat files and converted to PDF using Macro.  • Responsible for the implementation of ETL processes  • Develop and maintain technical documentation relating to ETL processes  • Work with BI technical lead to design ETL solutions and improvements  • Ensure timely completion of nightly data load process and provide analysis of load delays or failures  • Created UNIX shell scripts to run the Informatica workflows and controlling the ETL flow  • Participate in data modeling and data design with BI technical lead  • Create and support ETL development standards  • Work with manager, BI technical lead and analysts to review and assess business requirements relating to data loads  • Perform Informatica administration and performance tuning of ETL processes  • Optimize processes, programs and functions to improve data quality, data security and data consistency  • Prepare test strategies, test scripts and conduct system tests  • Assist, coordinate and mentor ETL development staff  • Other responsibilities as assigned    Environment: Informatica 9.5.1/9.6, Informatica Data Quality 9.5.1, Salesforce, SQL,TOAD, FileZilla3.6.0.2, IDQ, Data Analyst, Database Developer, UNIX, IpswitchWS_FTP12 Team Lead/ Informatica Developer StateFarmInsurance - Bloomington, IL September 2012 to March 2013 As part of new CDE ("Customer Driven Evolution") strategy, OM ("Opportunity Management") has established several marketing initiatives. One of which is to create NAR ("Need Analysis and Recommendation") marketing database for campaign management application (SAS) to operates. Worked in Multiple projects and Loaded consolidated data into EDW    Responsibilities:  • Designing, developing informatica mappings and jobs  • Performing code migration from lower environment to higher.  • Analyzing the data anomalies and defect fixes.  • Performing supervisory activities for the team members.  • Created UNIX scripts to perform operations like gun zip, remove and touch files and SecureFTP file to business user.  • Worked with Pre-Session and Post-Session UNIX scripts for automation of ETL jobs. Also involved in migration/conversion of ETL processes from development to production.  • Extensively used Trac to log the Incidents and tracking, Fix the Incident  • Experience working with customers and peers to develop solutions to complex business problems  • Used Workflow manager for Creating, Validating, Testing and running the sequential and concurrent Batches and Sessions  • Created Test Data and executed SQL queries in IBM DB2 to validate and test the data  • Unit Testing has been extensively done at development level  • Responsible for Unit Testing and Integration testing of mappings and workflows.  • Extensively Used CA-7 scheduling    Environment: Informatica 8.6.1/9.1, CA7,SQL, TOAD, Embarcadero Rapid, IBM Control Center, UNIX,SAP, IpswitchWS_FTP12 Team Lead/ Informatica Developer WellPoint INC - Richmond, VA August 2011 to July 2012 AHR Fully Insured Project is to expand the delivery of flexible and consistent incentive offerings to Anthem's customers. This will be achieved through the rollout of a new incentive product offering to the Fully-Insured market: AHR as well as to continue to support the current GenC and Comprehensive Health Solutions (CHS) supported incentives.  Hallmark as a vendor is processing the incentives for WellPoint (WLP). It would send a Monthly file to WLP for gift card disbursement. EDWard would process this feed and send out file to different billing systems (WGS, STAR, ACES, FACETS, CHIPS VA)  Data for Fully Insured Members will start coming in the source file (source file sent from Hallmark) from Feb-2012 onwards    Responsibilities:  • Schedule meetings with WellPoint business teams to gather WellPoint Data warehousing Business requirements for Maintenance requirements.  • Schedule meetings with Technical reference architecture teams/DBA/Informatica admin to finalize on the design aspects of the project  • Preparing design documents and Technical design documents for the extract, transform and loading data portion of the Project•  • Meetings with Offshore team to provide details of the design and project requirements•  • Reviewing the test case documents, test plans, coding and deliverables from offshore team to ensure they meet the project requirements and are delivered on time•  • Reviewing the ETL design documents prepared by offshore teams, reviewing Informatica and Teradata components to load the data to Data warehouse for reporting purposes.  • Extensively used Transformation to load the data into Landing zone and .dat files.  • Extensively used Teradata SQL Assistant  • Created scripts using Fast Load, Multi-Load to load data into the Teradata data warehouse  • Used utilities of FLOAD, MLOAD of Teradata and created batch jobs using BTEQ.  • Extensively used IpswitchWS_FTP12 for file transfer.  • Extensively Used Work Load Manager (WLM) scheduler to load the data to Landing zone table and to create .dat files for different source systems.  • Responsible for Unit Testing and Integration testing of mappings and workflows, Bteq and Unix Scripts.  • Write Shell script running workflows in UNIX environment.  • Extensively used Bteq scripts to update the status and loading Landing zone tables of the load for Audit purposes.  • Utilizing a host of Cognizant's internal tools that include:  MCAT - To validate the components between source and target environments. It will generate a report in a spread sheet indicating if the migration is successful or done with errors.  • Q-View for quality assurance and Q-Smart to automate quality assurance through powerful built in workflow mechanisms    Environment: Informatica 8.6.1/9.0.1, WLM scheduler, SQL, TOAD, XML, IBM Rapid Sql, UNIX, Erwin 8.0, Teradata, IpswitchWS_FTP12,TeradataSqlAssistant Team Lead/ Informatica Developer BlueCross BlueShield - Eagan, MN September 2010 to July 2011 The Enterprise Data Warehouse (EDW) is much more than a typical data warehouse. It comprises all the data and infrastructure necessary to support reporting, analysis, data extracts and data services for the enterprise. It includes the IDW comprising of general data which is comprehensive and historical. The EDW also includes optimized and dimensional data which is organized to meet specific business data requirements    Responsibilities:  • Worked on Onsite/Offshore Model.  • Worked on Power Center Designer client tools like Source Analyzer, Warehouse Designer, Mapping Designer, Mapplet Designer and Transformations Developer.  • Responsible for testing and validating the Informatica mappings against the pre-defined ETL design standards  • Extensively used Power exchange to import source files from Mainframe systems  • Worked on flat file and mainframe sources  • Developed Slowly Changing Dimension Mappings for Type 1 SCD and Type 2 SCD  • Extensively used Power Exchange to convert mainframe source.  • Extensively worked with History and Incremental Loading using Parameter Files, Mapping Variables and Mapping Parameters  • Developed Informatica parameter files to filter the daily data from the source system  • Extensively used PL/SQL stored procedures  • Extensively worked with the Debugger for handling the data errors in the mapping designer  • Responsible for determining the bottlenecks and fixing the bottlenecks with performance tuning  • Responsible for error handling using Session Logs, Reject Files, and Session Logs in the Workflow Monitor  • Worked with Shortcuts across Shared and Non Shared Folders  • Performance tuning on sources, targets, mappings and SQL queries in the transformations. Worked on flat files as sources, targets and lookups  • Extensively used SQL tool TOAD, RapidSql to execute SQL queries  • Extensively used ClearQuest to log the Incidents and tracking, Fix the Incident  • Experience working with customers and peers to develop solutions to complex business problems  • Involved in SQL Performance Tuning  • Used Workflow manager for Creating, Validating, Testing and running the sequential and concurrent Batches and Sessions  • Created Test Data and executed SQL queries in IBM DB2 to validate and test the data  • Unit Testing has been extensively done at development level  • Responsible for Unit Testing and Integration testing of mappings and workflows.  • Involved in writing UNIX shell scripts to automate ETL jobs  • Extensively Used CA-7 scheduling    Environment: Informatica 8.6.1/9.0.1, IMS, CA-7, Oracle 11g, DB2, SQL, TOAD, IBM Rapid Sql, SQL Server, UNIX, Erwin 8.0, Teradata ETL/Informatica Developer Country Insurance and Financial - Bloomington, IL April 2010 to August 2010 Country Insurance and Financial Services is a corporate firm broadly engaged in providing  Solutions for Financial and Insurance applications. Worked as Informatica Developer in Sales Work Station Phase-2 Project.  • Automated Scrubbing  • Automatic Load vendor Leads  • Schedule vendor email and mail Activities    Responsibilities:  • Involved in meetings with Business System Analysts to understand the functionality  • Worked on Power Center Designer client tools like Source Analyzer, Warehouse Designer, Mapping Designer, Mapplet Designer and Transformations Developer.  • Responsible for testing and validating the Informatica mappings against the Pre-defined ETL design standards  • Responsible for mapping and transforming existing feeds into the new data structures and standards utilizing Router, Lookups (Connected, Unconnected),Expression,Aggregator,Updatestrategy&storedprocedure transformations.  • Developed ETL mappings in Informatica Power Center Designer using various transformations like Source Qualifier, Normalizer, Expression, Connected and Unconnected Lookup, Update Strategy, Java Transformation, Joiner etc  • Developed SQL's to extract the data from Siebel as per requirements  • Developed Java Transformation to send email, to call Java and Siebel web services  • Developed Java Transformation to create Unique ID instead of Sequence generator  • Developed complex mapping for Scrubbing the data based on contact method i.e.(email,phone,address) and used web service to get Request and Response from Siebel  • Developed ETL scripts using BTEQ, MLOAD and UNIX.  • Extensively used Web service to call Siebel and to Update the Siebel based on the Requirements  • Extensively used PL/SQL and SQL  • Extensively used Informatica 8.6.1 to load data from source tables i.e. DB2,oracle  and then loaded the data into target table i.e., flat files,DB2,oracle  • Involved in source code management using Version Control option in Informatica  • ETL experience using Informatica 8.6.1 (Power Center/ Power Mart)  (Designer, Workflow Manager, Workflow Monitor and Server Manager  • Used Workflow manager for Creating, Validating, Testing and running the sequential and concurrent Batches and Sessions  • Created reusable transformations and Mapplet and used them to reduce the development time and complexity of mappings and better maintenance  • Created Test Data and executed SQL queries in SQL Squirrel to validate  and test Siebel data  • Unit and Integration testing has been extensively done at development level  • Involved in SQL Performance Tuning  • Responsible for developing Reports using Cognos 10 in Report Studio  • Tuned performance of Informatica mappings using components using Parameter files, Variables and Dynamic Cache  Environment: Informatica 8.5.1/8.6.1,Oracle, DB2, Control-M, Windows XP, Sql, Squirrel, Siebel, Cognos 10 XML, Java Informatica Developer MassMutual Financial Group - Springfield, MA February 2008 to March 2010 The EB PRP COLI/BOLI conversion project is one of the most complex modernization projects in-flight at MassMutual today. This initiative includes converting 60,000+ policies and all of the history, the standing up of a new admin platform (ALP), the development of a new data hub (EBIF) and related downstream feeds.    Responsibilities:  • Performed requirements analysis, design, coding and testing of mainframe programs including maintaining the existing programs.  • ETL experience using Informatica 8.5.1/8.6.1 (Power Center/ Power Mart) (Designer, Workflow Manager, Workflow Monitor and Server Manager)  • Created new JCL's and Modified existing JCL's. Created new and modified existing programs using COBOL, CICS, DB2 and VSAM.  • Worked on File Aid, Endeavor.  • Worked on Quality center 9.2 to Track the status  • Verified defects and Perform database functional, integration and regression testing as needed to minimize defects  • Extensively used Power Exchange.  • Created and executed SQL queries in Embarcadero Rapid SQL 7.4.2 to validate and test data  • Created mappings, workflows and Shell Scripts to extract, validate, transform data according to the business rules.  • Worked on Maestro scheduling, UNIX Scripting.  • Performed Unit, Functional and System Testing.  • Developed several Mappings and Mapplet using corresponding Sources, Targets and Transformations.  • Designed and developed UNIX shell scripts as part of the ETL process, automate the process of loading, pulling the data.  • Responsible for testing and validating the Informatica mappings against the pre-defined ETL design standards  • ETL experience using Informatica 8.5.1/8.6.1 (Power Center/ Power Mart) (Designer, Workflow Manager, Workflow Monitor and Server Manager)  • Involved in performance testing, functional testing and integration testing.  • Retrieve the latest version of programs from ENDEVOR, Which was the change management tool.  • Used Maestro for Informatica batch scheduling.    Environment: COBOL, VSAM, DB2, JCL, Maestro, UNIX Shell scripting, Embarcadero RapidSql, Quality Center 9.2, CICS, File-Aid, Endeavor, Cognos 8.3, Informatica 8.5.1 Software Engineer Accenture - IN November 2006 to January 2008 BCBSM - Blue Cross Blue Shield of Michigan is a nonprofit corporation. It provides health care Benefits to 4.8 million members through a variety of plans. The objective of the MOS (Michigan Operating System) project is to unify BCBSM processing systems on a single NASCO platform for Benefit Administration, Membership, Billing, Claims and Servicing.  The expectation is to achieve the following: Increased speed and flexibility in delivery of BCBSM products and services to market. Reduced Administrative costs, improved provider satisfaction through common practices, Enhancing operating functionality given current BCBSM system capabilities    Responsibilities:  • Gathering user Requirement Specification  • Involved in all phases of SDLC from requirement, design, development, testing, training and rollout to the field user and support for production environment.  • Prepared the ETL Design Documents as per the Business Specifications.  • Translated business requirements into Informatica mappings/workflows.  • Created reusable transformations and Mapplet.  • Created High level/detailed level design documents and also involved in creating ETL specifications.  • Created mappings, workflows and Shell Scripts to extract, validate, transform data according to the business rules.  • Involved in writing the PL/SQL Packages, Stored Procedures, Functions to accomplish the business rules and tuning the SQL queries using EXPLAIN Plan in Toad.  • Designed and developed the sessions for the relevant mappings or Informatica jobs and organized as sequential / concurrent Scheduled the main workflows as per the business rules using the Informatica scheduler using UNIX shell scriptinging for some of the existing data jobs.  • Used Informatica to extract and transform the data from existing, new and modified sources and finally loaded into the Staging area, Data Mart, Target databases and Data warehouse depending upon the requirements.  • Informatica will be used for job scheduling and session/job level monitoring and Notification.  • Created Unix script to automate the process  • Used Informatica debugger for Unit Testing the mappings.  • Performed Unit, Systems and Regression testing of the mappings. Involved in writing the test cases and also assisted the users in performing UAT.    Environment: Informatica Power Center 6, UNIX shell scripting, Oracle 8i, SQL, PL/SQL, Toad, Windows NT. Software Engineer Microline Infosystems - IN August 2005 to October 2006 The Objective of this Program is to create various data marts like the Individual Sales Mart, which caters to specific requirements for the policyholders and the agents. This system helps to analyze and grow their business.    The source data, which includes  * Types of Policies  * Information about the policy holder  * Policy covered item details  * Premium payment details.    • Involved in all phases of SDLC from requirement, design, development, testing, training and rollout to the field user and support for production environment.  • Worked with Informatica Power Mart client tools like Source Analyzer, Warehouse Designer, and Mapping Designer.  • Developed ETL mappings in Informatica Power Center Designer using various transformations like Source Qualifier, Normalizer, Expression, Connected and Unconnected Lookup, Update Strategy, Sequence Generator etc.  • Implemented Slowly Changing Dimensions using Power Center to insert/update the dimensions  • Implemented delta loading for data loading from source to target.  • Created Mapplet and used them in different Mappings for maintaining the standards.  • Developed the Test Scripts and performed the Unit Tests on the ETL mappings.  • Developed and scheduled Workflows using workflow designer in Workflow manager.  • Involved in analyzing source systems and designing the processes for Extracting Transforming and Loading the data to Data marts.  • Scheduled sessions for pulling the data from transactional databases in order to maintain the Daily and weekly based loads.  • Created UNIX script for scheduling the informatica workflows.    Environment: Informatica Power Center 6, UNIX shell scripting, Oracle 8i, SQL, PL/SQL Education Bachelor's Degree in Information Technology in Information Technology JNTU 2004 Skills Db2, Etl, Informatica, Teradata, Ms access