Sr Python Developer Sr <span class="hl">Python</span> <span class="hl">Developer</span> Sr Python Developer - EASTBAY/FOOTLOCKER New York, NY • Over 7 years of experience as a Web/Application Developer and coding with analytical programming using Python, Java  • Experienced with full software development life-cycle, architecting scalable platforms, object oriented programming, database design and agile methodologies  • Experienced in MVW frameworks like Django, Angular.js, Java Script, JQuery and Node.js.  • Expert knowledge of and experience in Object oriented Design and Programming concepts.  • Experience object oriented programming (OOP) concepts using Python, C++ and PHP.  • Experienced in WAMP (Windows, Apache, MYSQL, and Python/PHP) and LAMP (Linux, Apache, MySQL, and Python/PHP) Architecture.  • Experience in leading multiple efforts to build Hadoop platforms, maximizing business value by combining data science with big data.  • Advised organizations about big data, a big data strategy, the implementation of big data, which technologies best fit the needs of the organization and even implements the selected big data solution  • Write MATLAB code to create discretized computer models of sloped levy geometries.  • Experienced in developing web-based applications using Python, Django, PHP, C++, XML, CSS, HTML, DHTML, , JavaScript and Jquery.  • Experienced in installing, configuring, modifying, testing and deploying applications with Apache.  • Well versed with design and development of presentation layer for web applications using technologies like HTML, CSS, and JavaScript.  • Familiar with JSON based REST Web services and Amazon Web services.  • Experienced in developing Web Services with Python programming language.  • Experience in writing Sub Queries, Stored Procedures, Triggers, Cursors, and Functions on MySQL and PostgreSQL database.  • Experienced in agile and waterfall methodologies with high quality deliverables delivered on-time.  • Experience in utilizing SAS Procedures, Macros, and other SAS application for data extraction, data cleansing, data loading and reporting.  • Maintained detailed documentation and architectural solutions in IT infrastructure and sales systems.  • Very strong full life cycle application development experience.  • Strong knowledge on Dev Express Controls.  • Strong database design and programming skills in SQL Server 2012/2008/2005, SQL Stored  Procedures, functions, triggers, Cursors, Indexing, importing/exporting data from varied data sources  • Experience with continuous integration and automation using Jenkins.  • Experience with Unit testing/ Test driven Development (TDD), Load Testing.  • Have the ability to understand complex systems and be in command of the details to provide solutions.  • Ability to learn and adapt quickly to the emerging new technologies and paradigms.  • Excellent communication, interpersonal and analytical skills and a highly motivated team player with the ability to work independently.  • Practical experience with working on multiple environments like development, testing, production.  • Hands-on experience in writing and reviewing requirements, architecture documents, test plans, design documents, quality analysis and audits.  • Excellent analytical and problem solving skills and ability to work on own besides being a valuable and contributing team player. Work Experience Sr Python Developer EASTBAY/FOOTLOCKER - Wausau, WI July 2017 to Present with Hadoop  Eastbay is a supplier of athletic footwear, apparel and sports equipment, which sells by direct mail. Since 1997, it has been the direct-to-mail division of Foot Locker, Inc.  Resposibilities:  • Participated to develop a data platform from scratch and took part in requirement gathering and analysis phase of the project in documenting the business requirements.  • Worked with team of Hadoop developers on maintaining the data platform applications for RISK management.  • Worked in designing tables in Hive, MYSQL using SQOOP and processing data like importing and exporting of databases to the HDFS.  • Experienced in processing large datasets of different forms including structured, semi-structured and unstructured data.  • Developed rest API's using python with flask and django framework.  • Experienced the integration of various data sources including Java, JDBC, RDBMS, Shell Scripting, Spreadsheets, and Text files.  • Exposure to various mark-up languages including XML, JSON, CSV.  • Good Understanding of Hadoop architecture and the daemons of Hadoop including Name-Node, Data Node, Job Tracker, Task Tracker, Resource Manager.  • Hands on experience in ingesting data into Data Warehouse using various data loading techniques.  • Developed python scripts to load data to hive from HDFS.  • Participated in developing ETL components for executing various workflows  • Developed pig scripts and hive scripts for processing the data.  • Handled the JSON, XML, Log data using Hive (SERDE), Pig and filter the data based on query factor.  • Worked in agile methodology with 2 weeks sprints.  • Scheduled Jobs using crontab, rundeck and control-m.  • Performed Branching, Tagging, Release Activities on Version Control Tools: GIT and GITLAB.  • Importing and exporting data job's, to perform operations like copying data from HDFS and to HDFS using Sqoop.  • Developed Spark code and Spark-SQL/Streaming for faster testing and processing of data.  • Data was Ingested which is received from various database providers using Sqoop onto HDFS for analysis and data processing.  • Wrote and Implemented Apache PIG scripts to load data from and to store data into Hive.  • Managed the imported data form different data sources, performed transformation using Hive, Pig and Map- Reduce and loaded data in HDFS.  • Executed Oozie workflow engine to run multiple Hive and Pig jobs, which run independently with time and data availability.  • To achieve Continuous Delivery goal on high scalable environment, used Docker coupled with load-balancing tool Nginx.  • Developed Oozie workflow to run job onto data availability of transactions.  • Created User Defined Functions (UDF's) for maintaining Incremental ID's.  • Used Shell scripting to analyse the data from SQL Server source and processed it to store into HDFS.  • Had good exposure to spark Mlib,Streaming and sql by closely working with data scientists.  • Generated reports from Hive data using Microstrategy.  • Worked with complex sql queries to make joins.  • Increased the time efficiency of the HIVEQL and reduced the time difference of executing the sets of data by applying the compression techniques for Map-Reduce Jobs.  • Created Hive Partitions for storing Data for Different Companies under Different Partitions.  Environment: Hadoop,Hive,sqoop,pig, Python 2.7,java, Django 1.4,Flask, XML, MySQL, MS SQL Server,Linux, Shell Scripting,mongodb,SQL. Sr Python Developer FM Global - Jhonston,RI January 2015 to June 2017 This is a mutual insurance company, with offices worldwide, that specializes in loss prevention services primarily to large corporations throughout the world in the Highly Protected Risk(HPR) property insurance market sector.  Responsibilities:  • Participate in requirement gathering and analysis phase of the project in documenting the business requirements by conducting workshops/meetings with various business users.  • Worked with team of developers on Python applications for RISK management.  • Developed Python/Django application for Google Analytics aggregation and reporting.  • Used Django configuration to manage URLs and application parameters.  • Worked on Python Open stack API's.  • Used Python scripts to update content in the database and manipulate files.  • Generated Python Django Forms to record data of online users  • Detailed Understanding on existing build system, Tools related for information of various products and releases and test results information.  • Designed and implemented map reduce jobs to support distributed processing using java, Hive and Apache Pig.  • Configured ec2 instances and configured IAM users and roles.  • Created s3 data pipe using Boto API to load data from internal data sources.  • Configured Jboss cluster and mysql database for application access.  • Developed UDF's to provide custom hive and pig capabilities.  • Built a mechanism for automatically moving the existing proprietary binary format data files to HDFS using a service called Ingestion service.  • Comprehensive knowledge and experience in process improvement, normalization/de-normalization, data extraction, data cleansing, data manipulation  • Performed Data transformations in HIVE and used partitions, buckets for performance improvements.  • Ingestion of data into Hadoop using Sqoop and apply data transformations and using Pig and HIVE.  • Used Python and Django creating graphics, XML processing, data exchange and business logic implementation  • Developed Pl-Sql store procedures to convert the data from Oracle to MongoDB.  • I have used Pandas API to put the data as time series and tabular format for east timestamp data manipulation and retrieval.  • Automate report generation in MongoDB using Javascript, shell scripting, sed, java  • Added support for Amazon AWS S3 and RDS to host static/media files and the database into Amazon Cloud.  • Systems automation utilizing Control-M for scheduling and Powershell/C# for script development.  • Used Pandas library for statistical Analysis.  • Developed tools using Python, Shell scripting, XML to automate some of the menial tasks. Interfacing with supervisors, artists, systems administrators and production to ensure production deadlines are met.  • Worked very closely with designer, tightly integrating Flash into the CMS with the use of Flashvars stored in the Django models. Also created XML with Django to be used by the Flash.  • Used HTML, CSS, JQuery, JSON and Javascript for front end applications.  • Designed and developed the UI of the website using HTML, XHTML, AJAX, CSS and JavaScript.  • Also used Bootstrap as a mechanism to manage and organize the html page layout.  • Used Django configuration to manage URLs and application parameters.  • Wrote and executed various MYSQL database queries from Python using Python-MySQL connector and MySQLdb package.  • Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format.  • Worked on development of SQL and stored procedures on MYSQL.  • Responsible for debugging the project monitored on JIRA (Agile).  • Performed troubleshooting, fixed and deployed many Python bug fixes of the two main applications that were a main source of data for both customers and internal customer service team.  Environment: Python ,hive,oozie,Amazon AWS S3,MySQL,HTML, Python 2.7, Django 1.4, HTML5, CSS, XML, MySQL, MS SQL Server, JavaScript, AWS, Linux, Shell Scripting ,AJAX , Mongodb. Sr Python Developer Deustche bank - NJ December 2013 to November 2014 Deustche bank is a German global banking and financial services company offering financial products and services for corporate and institutional clients along with private and business clients. Services include sales, trading, research and origination of debt and equity; mergers and acquisitions (M&A); risk management products, such as derivatives, corporate finance, wealth management, retail banking, fund management, and transaction banking.[    Responsibilities:  • Worked with team of developers on Python applications for RISK management.  • Designed the database schema for the content management system.  • Designed and developed the UI of the website using HTML, XHTML, AJAX, CSS and JavaScript.  • Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format.  • Wrote Python routines to log into the websites and fetch data for selected options.  • Performed testing using Django's Test Module.  • Skilled experience in installing, configuring and using Apache Hadoop ecosystems such as Pig and Spark.  • Built the entire Hadoop platform from scratch.  • Experience in ingesting real time/near real time data using Flume, Kafka, Storm  • Evaluated suitability of Hadoop and its ecosystem to the above project and implementing / validating with various proof of concept (POC) applications to eventually adopt them to benefit from the Big Data Hadoop initiative.  • Estimated the Software & Hardware requirements for the Name Node and Data Node in the cluster.  • Extracted the needed data from the server into HDFS and Bulk Loaded the cleaned data into HBase using MapReduce  • Written the Map Reduce programs, Hive UDFs in Java.  • Develop HIVE queries for the analysts.  • Created an e-mail notification service upon completion of job for the particular team which requested for the data.  • Defined job work flows as per their dependencies in Oozie.  • Closely observed building the Reporting Application, which uses the Spark SQL to fetch and generate reports on table data  • Knowledge in performance troubleshooting and tuning Hadoop clusters in Cloudera  • Worked on middle tier and persistence layer. Created service and model layer classes and Value objects/POJO to hold values between java classes and database fields.  • Exported/Imported data between different data sources using SQL Server Management Studio.  Maintained program libraries, users' manuals and technical documentation.  • Responsible for debugging and troubleshooting the web application.  • Successfully migrated all the data to the database while the site was in production.  • Implemented the validation, error handling, and caching framework with Oracle Coherence cache.  • Worked on scripts for setting up the discovery client with attribute data. Worked on scripts(granite reference data scripts) for setting up adapter attributes in granite system.    Environment: Python 2.7,Hadoop, Django 1.4, HTML5, CSS, XML, MySQL, JavaScript, JQuery, Mongo DB, MS SQL Server, JavaScript, GitHub, AWS, Linux, Shell Scripting, AJAX. Sr Python Developer Mangstor - Austin, TX January 2012 to November 2013 Mangstor is a leading developer of NVMe Flash Storage Solutions to accelerate application workloads in Web-scale and Enterprise Data Centers. Mangstor's NVMe over Fabric Flash Storage Arrays and NVMe Solid-State devices provide the building blocks necessary for high-performance, highly-available, and cost-effective storage infrastructure.  Responsibilities:  • Data warehouse migration Sybase 12.5 to Sybase 15.7  • More than 70% of work is developing code in Python, remaining time spent on database development and data modelling.  • Worked on requirement gathering, high level design, implementation, testing and deployment of code.  • Creating and following the production deployment runbook.  • Did Proof of Concept on DB2 BLU(column organized database)  • Task prioritization with clients  • Designed and developed reusable Autosys jobs parsing and documentation software in Python using object oriented features, being implemented in other projects in firm.  • Designed and developed database object parsing, dependency builder and documenting software in Python using object oriented features  • Developed ETL (Extract Transform Load) software for DB2 columnar database fact and dimension tables.  • Massive data processing (Sybase ~15TB and Db2 ~2TB)  • Data modelling in Sybase and DB2.  • Setting up schema, users, permissions, creating database objects  • Database performance tuning - procedures, table functions(db2)  • Provide L2/L3 support on rotational basis  • Database objects refactoring, removing legacy duplication  • Built pluggable software for housekeeping and clean-up, being used by other projects in firm  • Data load analysis package to reveal errors during load based on historic trend  • Data reconciliation program for Sybase vs Db2, and Db2 vs Db2 (prod vs qa) in Python  • Other tools developed in Python to automate daily activities in python like monitoring DB  • Worked with shell scripts to build wrapper around ETL to do one time historic load.  Environment: Python, Java, MySQL, Linux, HTML, XHTML, CSS, AJAX, JavaScript, Apache Web Server. Python Developer Bosch Designs - Novi, MI March 2010 to December 2011 Bosch designs and produces precision automotive components and systems sold to vehicle and powertrain manufacturers worldwide. These include systems and components for gasoline and diesel injection, airbag components, ABS and conventional braking systems, telematics, as well as small motors, electrical and electronic equipment. Worked on development of Warranty and defects tracking system.    Responsibilities:  • Wrote Python routines to log into the websites and fetch data for selected options.  • Used Python modules such as requests, urllib, urllib2 for web crawling.  • Used other packages such as Beautifulsoup for data parsing.  • Worked on writing and as well as read data from csv and excel file formats.  • Developed a MATLAB algorithm which determines an object's dimensions from digital images.  • Web-services backend development using Python (CherryPy, Django, SQLAlchemy).  • Participated in developing the company's internal framework on Python. This framework became a basement for the quick service's development. Framework based on CherryPy with GnuPg encryption (reGnuPg module) on the top.  • Worked on resulting reports of the application and Tableau reports.  • Worked on HTML5, CSS3, JavaScript, AngularJS, Node.JS, Git, REST API, Mongo DB, intelliJ IDEA.  • Design and Setting up of environment of Mongodb with shards and replicasets. (Dev/Test and Production).  • Private VPN using Ubuntu, Python, Django, CherryPy, Postgres, Redis, Bootstrap, Jquery, Mongo, Fabric, Git, Tenjin, Selenium, Sphinx, Nose.  • Modifying data using SAS/BASE, SAS/ MACROS.  • Extracting data from the database using SAS/Access, SAS SQL procedures and create SAS data sets.  • Performed QA testing on the application.  • Developed approaches for improving NLP pipeline.  • Create custom VB scripts in repackaging applications as needed.  • NLP File Prep Settlement-Prepare files for review for Settlement.  • Held meetings with client and worked all alone for the entire project with limited help from the client.  • Participated in the complete SDLC process.  • Developed rich user interface using CSS, HTML, JavaScript and JQuery.  • Created a Python based GUI application For Freight Tracking and processing  • Used Django framework for application development.  • Developed and maintained various automated web tools for reducing manual effort and increasing efficiency of the Global Shipping Team.  • Created database using MySQL, wrote several queries to extract data from database.  • Setup automated cron jobs to upload data into database, generate graphs, bar charts, upload these charts to wiki, and backup the database.  • Wrote scripts in Python for extracting data from HTML file.  • Effectively communicated with the external vendors to resolve queries.  • Used Perforce for the version control.    Environment: Python, Django 1.4, MySQL, Windows, Linux, HTML, CSS, JQuery, JavaScript, Apache, Linux. Software Engineer Python - Chicago, IL March 2008 to February 2010 Responsibilities:  • Django Framework was used in developing web applications to implement the MVT architecture.  • Exposure on Multi-Threading factory to distribute learning process back-testing and into various worker processes.  • Worked on user portal, creating forms and adding users.  • Developed dynamic web pages using Python Django Frameworks  • Used Django Restful API for database access.  • Wrote Python scripts to parse JSON documents and load the data in database.  • Features for dashboard were developed and tested using CSS, AngularJS and Bootstrap  • Developed forms using HTML and performing client-side validations using JavaScript, jQuery and Bootstrap  • Cleansing data generated from weblogs with automated scripts in Python  • Used PostgreSQL database for storing the information and experience in performing CRUD operations to operate Schema objects to browse functions in python  • Experience in Performance Tuning, Query Optimization, Client/Server Connectivity and Database consistency checks, Backup and Recovery  • Involved in Continuous Integration (CI) and Continuous Delivery (CD) process implementation using Jenkins along with Shell script.  • Used GIT repository for software configuration management and version control  • Used JIRA for tracking and Updating Project issue  • Performed troubleshooting, fixed and deployed many Python bug fixes of the two main applications that were  • Maintained main source of data for both customers and internal customer service team.  • Maintained technical documentation for resolved issues for future reference  • Coded test programs and evaluated existing engineering process  • Effectively communicated with the external vendors to resolve queries.  • Performed research to explore and identify new technological platforms.  Environment: Python 3.5.2, Django, Rest, HTML, JavaScript, JIRA, CSS, Bootstrap, PostgreSQL, Jenkins. Education Bachelor's Skills DJANGO, Git, HTML, JAVASCRIPT, FLASK Additional Information TECHNICAL PROFICIENCY:  OS Platforms Linux/Unix, Windows-98/NT, MAC OSX  Languages Python 2.7/2.4, Java, Shell ,c  Databases MySQL, SQL Server 2008, PostgreSQL  Web Technologies AJAX, AWS EC Cloud, Amazon S3 JavaScript, HTML, XML  Versioning Tools Git, SVN,STASH  Web servers Apache, Nginix ,Tomcat  Framework Django,Flask  Other Tools Putty,SQl developer,Photoshop,JIRA