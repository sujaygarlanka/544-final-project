Sr. Python Developer Sr. <span class="hl">Python</span> <span class="hl">Developer</span> Sr. QA Engineer/Lead Jersey City, NJ • Over 11+years of extensive hands-on experience in diversified fields of the Software Development life Cycle (SDLC), specializing in Quality Assurance Process and Methodologies.  • Developed Test Plans, Test Scenarios and Test Cases from the business, technical and functional requirements.  • Experience in testing applications built in ASP.NET, Mainframe GUI, VB, XML, HTML, DHTML, Java-bld, and J2EE.  • Experience in Web Base testing with different browsers Safari, Mozilla Firefox, Google Chrome, IE  • Experienced testing in Dev. Environment.  • Conversant with all phases of Software Testing Life Cycle (STLC), Software Development Life Cycle (SDLC) in Req. gathering, Analysis, Design, Development, Implementation and testing.  • Have worked in different SDLC Methodologies like Agile and Waterfall.  • Skilled in creating test traceability matrix, test plan, Use Case, test cases, test procedures, test, RTM, defect reporting.  • Analyzed the System architecture for developing performance strategy.  • Developed Scheduling strategies for performance analysis for large organization.  • Experienced in all phases of Software Development Life Cycle, which includes Analysis.  • Experience in performance of Build Validation and Verification, Positive and Negative, Boundary Values Analysis, Equivalent Classes partitioning, Smoke (Sanity), Functional, Integration, System, Regression, Ad-hoc (Exploratory), Cross-Browser, User Interface and User Acceptance tests.  • Extensive experience of Manual & Automation testing of PeopleSoft HCM, Financials, E commerce, Banking, Retail, Health care applications.  • Deep knowledge in the Analysis of Bug Severity, Bug Tracking system and Bug Reporting.  • Excellent analytical and programming abilities in using technology to create elegant, flexible and maintainable solutions for complex development problems.  • Analyzed the Load Runner results to measure the Average CPU usage, Response time, Transactions per second.  • Liaison with upstream & downstream systems to ascertain technical requirements from test perspective for both manual and automation testing  • Knowledge on Jenkins  • Mentored Software Development Engineers in Test I & II and offshore test teams by defining short term daily tasks and establishing long term goals  • Collaborated with development teams and business partners to ensure successful delivery and implementation of application solutions  • Managed and be accountable for the functional and non-functional testing of one or more applications through SIT and UAT. Performance Center Admin for across the broad. Authorized to work in the US for any employer Work Experience Sr. Python Developer V-Soft Consulting Group, Inc - Philadelphia, PA February 2014 to Present Sr. QA Engineer/Lead Ciber Global LLC - Orlando, FL January 2016 to February 2018 • Tested Performance of Web Application and Generated Automation Test scripts using Load Runner.  • Conducted load and performance testing using Load Runner by creating rendezvous points to simulate heavy user load, and transaction points to test application response time.  • Ramped up Virtual users in a load test to achieve a maximal transaction volume of 1200 concurrent users in a two hour time limit.  • Monitored the run scenario using various monitors to identify functionality and performance issues including: Deadlock conditions, database connectivity problems, system crashes under load.  • Extensively used Performance Monitors to analyze the System Bottlenecks like Memory Leaks.  • Analyzed load balancer settings to perform Spoofing.  • Parameterized the scripts and enhanced them according to the test case.  • Regulating and coordinating testing processes and standards across multiple and simultaneous projects and departments, for various browsers, countries and mobile devices  • Managing the test preparation phases for the production of test effort, resource calculation, test data, test cases, test execution plans, and defect report life cycle flows (it is essential to set up for each project as developers use different life cycles)  • Manages global technical resources, including planning, estimating, and developing test cases in compliance with the established engineering process  • Takes full responsibility for all phases of testing and the management of the testing activities within a development and complex project  • Recommend the selection of and be responsible for the performance management of staff members  • Extensively worked on Virtual User generator and Controller.  • Worked extensively in testing boundary coverages to ensure the functionality of the application worked as expected.  • Executed the baseline performance tests for each release to verify the performance changes for significant business transactions.  • Developed test plans, test cases, test scripts and procedures, traceability matrix, and test result reports for manual and automated testing.  • Performed failover testing by shutting down one of the database and making sure that the overall system runs well.  • Coordinating and arranging UAT after completing the basic, core and delivery testing.  • Select project testing standards for all phases, influencing all parties, i.e. project managers, to conform to those standards and manage the client relationship with respect to all testing matters. Responsible for designing, developing and implementing cost-effective methods of testing and troubleshooting systems  • Identifying, managing and recording risks and issues across the full testing life cycle  • Writing, implementing, and reporting status for system test cases for testing  • Defining and maintaining the testing strategy and various test plans. This should cover both automation and manual testing  • Defining and owning testing component project plans, fully understand all project and testing assumptions, dependencies and associated risks.    Environment: Java, .NET, Load Runner 9.0, 8.1, Quality center 8.2, Winrunner, Oracle, HTML, Oracle Applications, TOAD, Java Script. HP Performance Manager Performance Center - Gurnee, IL June 2011 to January 2014 Environments: Performance Center 11.52, Load Runner 11.52, Willy Introsope J2EE/.NET Application Diagnostics, AWS reports, Web Page Diagnostics, Web Sphere, HP Performance Manager, SOA, SOAP UI, Dynatrace, Web (HTTP/HTML), Web Services, IVR Telephony, Peoplesoft Financial HCM    Solution Partners, Inc. Gurnee, IL Jun 11 - Jan 14  Test Lead    • Key person in QA process in the company, coordinated onshore and offshore activities  • Took a leading role in test automation and manual testing, actively involved in creation of detailed test plans test cases and test scenarios for different application modules according to functional requirements and business specifications.  • Tested Performance of Web, WebServices Applications and Generated Automation Test scripts using Load Runner.  • Conducted load and performance testing using Load Runner by creating rendezvous points to simulate heavy user load, and transaction points to test application response time.  • Extensively used Performance Monitors to analyze the System Bottlenecks like Memory Leaks.  • Worked on Performance tuning of java applications.  • Extensively worked on Vugen and Performance Center.  • Executed the baseline performance tests for each release to verify the performance changes for significant business transactions.  • Develop and Monitor performance reports, dashboards and status reports to verify test execution including test and issue tracking, daily meetings, test summaries and test case movement progress according to plan.  • Execute performance tests on applications to assure capacity and stability of the applications meets requirements for production deployment.  • Analyzed continuous integration needs, and developed and implemented test strategy so that different tests were run at appropriate times  • Automating and implementing performance test  • Developing scenarios for performance test  • Giving feedback to the development team with possible improvements and discovered issues/benchmarks  • Evaluating product reliability and scalability  • Finding efficient ways of automating of performance tests  • Performing load tests and validating system performance and stability  • Collaborating with developers and engineers and analyzing tests results to fix bug  • Providing technical assistance and improving system performance, reliability, capacity, and scalability  • Took an active role in static testing activities such as design review and requirements inspections to identify ambiguity and inconsistency in requirements as well as improve UI\UX Design of company software products.  • Responsible for defect tracking, defect reporting and defect reproducing.  • Responsible for test strategy, test plans, test execution, and results reporting.  • Conduct interviews, hire, conduct performance evaluation, train and mentor QA Engineers; Report Status.  • Handle responsibilities of analyzing data to provide necessary plans in quality testing.    Environments: Performance Center 11.52, Load Runner 11.52, Web Sphere, HP Performance Manager, SOA, AWS, SOAP UI, Web (HTTP/HTML), Web Services , IVR Telephony, HP QC, CRM ,TF Central    Thinkfind Corporation Fort Worth, TX Apl 09 -May 11  Test Lead    Responsibilities:  • Executed Stress test by creating Virtual users using Controller.  • Tested Performance of Web Application and developed Test scripts using Load Runner.  • Conducted load and performance testing using Load Runner by creating rendezvous points to simulate heavy user load, and transaction points to test application response time.  • Created various scenarios in Performance Center for performing baseline, benchmark, stress tests and reliability tests.  • Develop test strategies using project artifacts as well as in depth interviews of architects and lead developers.  • Capacity and performance engineers to ensure that any application deployed will properly scale and meet expected SLAs.  • Managed multiple levels of management and be able to work well with various technical groups including: network, database, applications and systems.  • Responsible for establishing and managing key testing milestones and working with Test Phase Leads to adjust project schedules and/or resources to meet the needs of customers.  • Managed one or more projects of medium to high complexity, or manages multiple test coordinators in a program environment.  • Analyzed load balancer settings to perform Spoofing.  • Parameterized the scripts and enhanced them according to the test case.  • Extensively worked on Virtual User generator and Controller.  • Executed the baseline performance tests for each release to verify the performance changes for significant business transactions.  • Developed test plans, test cases, test scripts and procedures, traceability matrix, and test result reports for manual and automated testing.  • Performed Regression Testing of each new release of the system through Manual after corrections and enhancements were made.  • Performed Negative testing to find how the functions and variables perform when it encounters invalid and unexpected values.  • Performed failover testing.  • Performed User Acceptance Testing in the final phase of software development process to check the functionality of the software.  • Also analyzed the Load Runner reports to calculate Response time and Transactions per Second (TPS)  • Coordinated communication with all areas of the enterprise that impact the scope, budget, risk, and resources of the test effort being managed.  • Assists Test Phase Leads and/or Project/Program Manager(s) in identifying and prioritizing testing related risks and mitigation approaches to achieve the goals of the project.  • Must possess advanced knowledge and proficiency in the use of project management methodologies and tools, resource management practices and change management techniques.  • Communicate testing strategy and performance testing services for the project development teams. Ensure that project team members understand performance measurement and analysis process.  • Monitor all impacted systems for network connection disruptions as well as any obscure application behavior resulting from a network anomaly.  • Communicate performance testing results and identify areas for improvement with stakeholders at all level  • Research the issues in application /DB, UI layer and identify performance bottlenecks and assist engineers with improving application performance and reducing latency  • Comfortable and confident Interfacing with architects, developers, managers, customers and other engineers over performance issues/concerns.  • Develop and Monitor performance reports, dashboards and status reports to verify test execution including test and issue tracking, daily meetings, test summaries and test case movement progress according to plan.  • Create or modify performance scripts for Web based applications using HP LoadRunner with the Web and Web Services protocols.  • Execute performance tests on applications to assure capacity and stability of the applications meets requirements for production deployment.  • Provide interim reports as well as a final report that summarize results to senior management  • In-depth knowledge and working experience with performance testing tools (Apache JMeter, SiteScope, LoadUI, etc), Linux/Unix/Windows monitoring utilities, performance Center and other peripheral tools & technologies. DBA skill set will be a relevant qualification.  • Excellent written and oral communication skills that can traverse multiple levels of management and be able to work well with various technical groups including: network,database, applications and systems.    Environment: QTP/UFT, Quality Center/ALM, UNIX, Oracle SQL Developer, Jenkins, Selenium, Eclipse, Putty, Windows 7, Oracle 11g, MS Office, Log4j, XML. Test Lead Alltech Consulting Services, Inc - New York, NY February 2007 to March 2009 Responsibilities:  • Conducted load and performance testing using Load Runner by creating rendezvous points to simulate heavy user load, and transaction points to test application response time.  • Extensively used Performance Monitors to analyze the System Bottlenecks like Memory Leaks.  • Worked on Performance tuning of java applications.  • Parameterized scripts in Vugen.  • Extensively worked on Vugen and Performance Center.  • Executed the baseline performance tests for each release to verify the performance changes for significant business transactions.  • Developed test plans, test cases, test scripts and procedures, traceability matrix, and test result reports for manual and automated testing.  • Performed Negative testing to find how the functions and variables perform when it encounters invalid and unexpected values.  • Developed Performance Test Scripts.  • Extensively used Performance Monitors to analyze the System Bottlenecks like Memory Leaks.  • Coordinated meeting for determining Production readiness of applications  • Analyzing data from Wily and providing the team with root cause of issues.  • Prepared Test Plan and actively involved in preparation of Test Strategy, Microsoft Project Plan & Integrated Plan.  • Individual developed a comprehensive Performance Plan and execute all aspects of the plan. Ensured that the implementation, key user types, locations, and scenarios are assessed as part of the overall plan.  • Works with the Wal-mart Business team to identify critical business processes and key application transactions to define testing requirements.  • Worked extensively on JD Edwards performance planning and execution.  • Developed performance test plan for PeopleSoft application.  • Run performance tests on PeopleSoft application and analyzed the performance.  • Performed Testing Reports developed for entering the customers.  • Documented test execution results, test reports, gathered and evaluated test metrics for reporting.  • Performed testing front-end screens to populate the data from tables & manipulate the data as per the user requirements.  • Tracked and reported the bugs with Quality center.  • Conducted Analysis of Testing Results and recommends solutions for software and infrastructure tuning.  • Coordination all aspects of test design, planning, and execution with product vendor and service providers.  • Worked with the business team to set expectations and establish key performance benchmarks to use in end user communications.  • Managing the activities, deliverables (i.e. test strategies, test plans, test case/execution, RTM, testing solutions, results, budgets, schedules and quality) and results of the Quality Assurance / Test teams  • Identifying and documenting noncompliance issues and ensures that noncompliance issues are addressed  • Delivering testing progress and status metrics updates via written and verbal communication methods  • Objectively evaluating processes, work products, and services against applicable process descriptions, standards, and procedures  • Facilitating test kickoff meetings including User Acceptance Testing meeting.  • Ensuring the appropriate test environments and data requirements are outlined, agreed upon and understood.  • Ensuring defects are effectively recorded, communicated and understood.  Performing System and UAT test defect triage, follow through on defects, issues, risks, etc. and escalate as needed.  • Developed status reports and communicates appropriate level of detail on Testing, Results, and Issues to team members and senior management  • Oversee performance management for a web-based application, including performance benchmarking and design, performance testing, diagnostics, and tuning prior to go live, as well as monitoring and tuning efforts post go-Live.  • Defect Triage - Actively worked with Developers, Infrastructure team for defect process.    Environment: Load Runner, HP Performance Center, JD Edwards, Data Stage, HP Performance Manager SOAP UI, Dynatrace Education Bachelor's Bachelor's