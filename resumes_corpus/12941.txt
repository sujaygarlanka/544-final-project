Big data architect Big data architect Big data architect - Turner Broadcasting Columbus, OH • Above 8+ years of experience as Big Data Engineer Analysis, Design, Development, Deployment and Maintenance of software in Java/J2EE technologies and BigData applications.  • Expertise in Data Development in Hortonworks HDP platform & Hadoop ecosystem tools like Hadoop , HDFS, Spark, Zeppelin, Hive, HBase, Sqoop, flume, Atlas, SOLR, Pig, Falcon, Oozie, Hue, Tez, Apache NiFi, Kafka.  • Built streaming applications using SPARK Streaming.  • Knowledge on big-data database HBase and NoSQL databases Mongo DB and Cassandra  • Expertise in Java Script, JavaScript MVC patterns, Object Oriented JavaScript Design Patterns and AJAX calls.  • Experience includes Requirements Gathering, Design, Development, Integration, Documentation, Testing and Build.  • Experience in working with MapReduce programs, Pig Scripts and Hive commands to deliver the best results.  • Good Knowledge in Amazon Web Service (AWS) concepts like EMR and EC2 web services which provides fast and efficient processing of Teradata Big Data Analytics.  • Experienced in collection of Log Data and JSON data into HDFS using Flume and processed the data using Hive/Pig.  • Strong knowledge and experience in Object Oriented Programming using Java.  • Extensively worked on development and optimization of MapReduce programs, PIG Scripts and HIVE queries to create structured data for data mining.  • Expertise in developing the presentation layer components like HTML, CSS, JavaScript, JQuery, XML, JSON, AJAX and D3.  • Good knowledge of coding using SQL, SQL Plus, T-SQL, PL/SQL, Stored Procedures/Functions  • Worked on Bootstrap, Angular JS and Node JS, knockout, ember, Java Persistence Architecture (JPA).  • Hands on experience in advanced Big-Data technologies like Spark Ecosystem (Spark SQL, MLlib, SparkR and Spark Streaming), Kafka and Predictive analytics  • Knowledge of the software Development Life Cycle (SDLC), Agile and Waterfall Methodologies.  • Strong experience in developing Enterprise and Web applications on n-tier architecture using Java/ J2EE based technologies such as Servlets, JSP, Spring, Hibernate, Struts, EJBs, Web Services, XML, JPA, JMS, JNDI and JDBC.  • Developed applications based on Model-View-Controller (MVC)  • Working knowledge on Oozie, a workflow scheduler system to manage the jobs that run on PIG, HIVE and SQOOP.  • Expertise in developing test cases for Unit testing, Integration testing and System testing.  • Extensively development experience in different IDE like Eclipse, Net Beans, IntelliJ and STS.  • Experienced with programming language such as C, C++, Xpath, Core Java and JavaScript.  • Good experience in Installing, Upgrading and Configuring Redhat Linux using Kickstart Servers and Interactive Installation.  • Good experience in Tableau for Data Visualization and analysis on large data sets, drawing various conclusions.  • Extensive experience in building and deploying applications on Web/Application Servers like Web logic, Web sphere, and Tomcat.  • Expertise in core Java, J2EE, Multithreading, JDBC, Hibernate, Spring, Shell Scripting and proficient in using Java API's for application development.  • Good at problem-solving skills to identify areas of improvement and incorporating best practices for delivering quality deliverables  • Have good experience, excellent communication and interpersonal skills which contribute to timely completion of project deliverable well ahead of schedule. Authorized to work in the US for any employer Work Experience Big data architect Turner Broadcasting - Atlanta, GA April 2018 to Present Responsibilities:  • Provide technical leadership and contribute to the definition, development, integration, test, documentation, and support across multiple platforms.  • Design/ architected and implemented complex projects dealing with the considerable data size (GB/ PB) and with high complexity.  • Provide deployment solutions based on customer needs with Sound knowledge about the clustered deployment architecture.  • Able to guide / partner with VP / Directors for architecting solutions for the Big data Organization  • Created detailed AWS Security groups which behaved as virtual firewalls that controlled the traffic allowed reaching one or more AWS EC2 instances.  • Data modeling, Design, implement, and deploy high-performance, custom applications at scale on Hadoop /Spark.  • Data processing with MapReduce and Spark.  • Stream processing on Spark/storm thru Kafka message broker.  • Review and audit of existing solution, design and system architecture.  • Perform profiling, troubleshooting of existing solutions.  • Create technical and designing documentation.  • Creation of a User Interface to search and/or view content within the cluster by using solar cloud.  • Worked on AWS provisioning EC2 Infrastructure and deploying applications in Elastic load balancing.  • Cluster management and analytic in Cloudera and Horton work.  • Distributed database Design, Data modeling, Development and Support in Datastax Cassandra distribution.  • Cassandra products strengths and weakness to produce efficient schema designs that serves effective and high performance queries.  • Maintain and work with our data pipeline that transfers and processes several terabytes of data using Spark, Scala, Python, Apache Kafka, Pig/ Hive & Impala  • Apply data analysis, data mining and data engineering to present data clearly.    • Ensure high-quality data and understand how data is generated out experimental design and how these experiments can produce actionable, trustworthy conclusions.  • Full life cycle of Data Lake, Data Warehouse with Big data technologies like Spark, Hadoop , Cassandra  • Working with Spark, RDD, Data Frames, Data Pipelines.  • Building complex ETLs, Data Warehousing or custom pipelines from multiple data sources.  • Setting up connector for security logs and Splunk data use cases.  • Building the Hadoop cluster (MTS) to host the three use cases  • Analyzing the data using Tableau.  • Extract and analysis the data before load into cluster.  • Review and understand data architecture, data models, Source to target mapping rules and Match and merge rules.  • Evaluate Hadoop infrastructure requirements and design/deploy solutions (high availability, big data clusters, elastic load tolerance, etc.).  • Hadoop ecosystem components in our open source infrastructure stack specifically: HBase, HDFS, Map/Reduce, Yarn, Oozie, Pig, Hive, Kafka, Storm, Spark, Spark-SQL and Flume.  • Estimate and obtain management support for the time, resources and budget required to perform in different projects.  • Keep track of the new requirements / change in requirements of the Project.  • Understand Inbound and outbound data flow requirements, data models for Landing, Staging and base objects, Mapping documents, Match and Merge rules.  • Proof of Concept (POC) and Proof of Technology(POT) execution and evaluation on MTS platforms.  • Installing and Configuring required ecosystem tools for each use case    Environment: Big Data, Spark, YARN, HIVE, Pig, Scala, Python, Hadoop , AWS, Dynamo DB, Kibana, Cloudera, EMR, JDBC, Redshift, NOSQL, Sqoop, MYSQL. Sr. Big Data/ Hadoop Developer ADP - Florhan - Park, NJ, US March 2016 to April 2018 Responsibilities  • Coordinated with business customers to gather business requirements. And also interact with other technical peers to derive Technical requirements and delivered the BRD and TDD documents.  • Implement Big Data systems in distributed cloud environment (AWS) using Amazon EMR  • Extensively involved in Design phase and delivered Design documents. Experience in Hadoop eco system with HDFS, HIVE, PIG, SQOOP and SPARK with SCALA.  • Worked on analyzing Hadoop cluster and different Big Data Components including Pig, Hive, Spark, HBase, Kafka, Elastic Search, database and SQOOP. Installed Hadoop , Map Reduce, HDFS, and developed multiple Map-Reduce jobs in PIG and Hive for data cleaning and pre-processing.  • Importing and exporting data into HDFS and Hive using SQOOP.  • Migration of 100+ TBs of data from different databases (i.e. Netezza, Oracle, SQL Server) to Hadoop. Wring code in different applications of Hadoop Ecosystem to achieve the required output in a sprint time period  • Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data.  • Involved in creating Hive tables, loading with data and writing Hive queries that will run internally in map reduce way. Generate OBIEE reports to verify the Hive tables data.  • Experienced in defining job flows. Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting. Experienced in managing and reviewing the Hadoop log files.  • Used Pig as ETL tool to do Transformations with joins and pre-aggregations before storing the dataonto HDFS. Responsible to develop data pipelines from different sources.  • Wrote Hive and Impala queries to load and processing data in Hadoop File system.  • Utilized Apache Hadoop environment by Cloud era Distribution. Exported data from HDFS environment into RDBMS using Sqoop for report generation and visualization purpose.  • Worked on Oozie workflow engine for job scheduling. Involved in Unit testing and delivered Unit test plans and results documents.    Environment: Hadoop , MapReduce, HDFS, Hive, Pig, Hue, Ganglia, Nagios, Java, Kafka, Elastic Search, SQL, Scala, Oracle, Netezza, Ambari, Sqoop, Flume, Oozie, Java (jdk 1.6), Eclipse. Service Controller Disney - Orlando, FL February 2014 to March 2016 Orlando, FL Feb2014-Mar2016  Full Stack Java Devoloper    Responsibilities:  • Done the design, development and testing phases of Software Development using AGILE methodology and Test-Driven Development (TDD)  • Designed the application using Front Controller, Service Controller, MVC, Spring DAO, Factory, Data Access Object, Service Locator and Session Facade Design Patterns.  • Involved in development of the applications using Spring Web MVC and other components of the Spring Framework, the controller being Spring 3.0 Core (Dispatcher Servlet).  • Used Hibernate for Object relational model for handling server side / database object data.  • Work closely with our partners and clients to develop and support ongoing API integrations.  • Used SOAP and REST based web service protocol.  • Bootstrap used along with Angular 2 and EcmaScript7 with TypeScript in creating the Application.  • Provide leadership in developing the company's IT software and ongoing strategy for internal and external purposes.  • Ensure Client & Subsidiaries are in compliance with all domestic and international data privacy lAWS ensuring that the flow of confidential data is secured and in compliance with local jurisdictions.  • Developed Linux bash & MS DOS Scripts for internal use.  • Used PostgreSQL as back end and developed Stored procedures, Batch jobs, triggers.  • Used Jenkins to perform software build with Gradle, run shell Scripts and worked on integration tests.  • Develop customized reports for clients or internal customers using Pentaho - ETL tools.  • Utilized React for its efficient data flow architecture to create a lightweight and render efficient web app that searched projects via the GitHub API through keywords  • Designed Frontend with in object oriented JavaScript Framework like Bootstrap, Node.js, Express.js and Angular.js, Redux.  • Wrote a Python module to connect and view the status of an Apache Cassandra instance.  • Implemented log4j by enabling logging at runtime without modifying the application binary.  • Developed Mean Stack from scratch including Mongo DB server setup and Express JS server development.  • Worked in using React JS components, Forms, Events, Keys, Router, Animations, and Flux concept.  • Used popular Node.js frameworks like Express and RESTify to create a RESTful Mock API.  • Provide estimates, designs, and specifications for AEM templates, components, and workflows.    Environment: MVC, Factory, Session Facade Design Patterns, Spring, SOAP, RESTful web services, Angular.js, Linux bash, MS DOS, Hibernate, PostgreSQL, Dynatrace, Git, Github, Bootstrap, Node.js, log4j, Rally, AWS. Sr. Java Developer Actionet - Germantown, MD January 2013 to February 2014 Responsibilities:  • Worked on the existing application, wireframes, FDN and BRD documents to get the requirements and analyzed.  • Hands-on Experience with Cassandra to provide Scalability along with NoSQL.  • Developed Agile processes using Groovy, JUnit to use continuous integration.  • Integrated Automated functional tests (Groovy) with Continuous-Integration in Jenkins.  • Parse requests and built response data using Groovy's JSON tools, and Grails web services.  • Imported data from various resources to the Cassandra cluster using Java APIs.  • Used Eclipse SWT for developing the applications.  • Involved in preparation of TSD documents using UML diagrams - Class, Sequence and Use case diagrams using Microsoft VISIO tool.  • Wrote RESTful services on the server in NodeJS to listen to requests from devices.  • Built a Grails web application that allows admin users to manage detailed data for all types of Target locations  • Have worked with Standard Widget Toolkit (SWT).  • Conversion of major Openwork's components in to Eclipse RCP/SWT platform along with support of Swing-SWT components.  • Involved in to develop view pages of desktop portal using HTML, Java Script, JSP, Struts Tag libraries, AJAX, JQUERY, GWT, DOJO, XML, and XSLT.  • Developed and deployed Web services to interact with partner interfaces, and client interfaces to consume the web services using CXF, WSDL, SOAP, AXIS and JAX-WS technologies.  • Integrating third party libraries to augment those lacking or inefficient in ExtJS.  • Used RESTful web services using JERSEY tool to develop web services easily and to be invoked by different channels.  • Developed service objects as beans by using Spring IOC/DI.  • Developed Web API using NodeJS and hosted on multiple load balanced API instances.  • Implementation of enterprise application with jQuery, angularJS, node.js and SpringMVC.  • Used Spring Beans to encapsulate business logic and Implemented Application MVC Architecture using Spring MVC framework.  • Implemented Hibernate (ORM Mapping tool) framework to interact with the database to update, retrieve, insert and delete values effectively.  • Used Java Swing for few components in accordance with SWT application with multithreading environment with Concurrency and Java Collections.  • Used EH Cache for second level cache in Hibernate for the application.  • Involved in to pass messages like payload to track different statuses and milestones using EJB, JMS.  • Involved in unit testing, integration testing, SOAP UI testing, smoketesting, system testing and user acceptance testing of the application.  • Used Spring programmatic transaction management for Java Persistence.  • Involved in integration of Spring and Hibernate frameworks.  • Involved in setting server properties, DSs, JNDI, queues & deploying app in WebSphere Application Server.  • Followed the test driven development using the JUNIT and Mockito framework.  • Created continuous integration builds using Maven.  • Involved in fixing QA/UAT/Production issues and tracked them using QC.    Environment: Java, JSP, Servlets, JavaScript, Spring DI, Spring IOC, Spring AOP, Hibernate 3.0, AJAX, XML, XSLT, JAXP, JAXB, AXIS, CSS, CXF, WSDL Java Developer Capital One Bank - Plano, TX February 2011 to January 2013 Responsabilites:  • Developed Controller Servlets and Action Servlets to handle the request and responses.  • Developed and coordinated complex high quality solutions to clients using IBM Products/Tools, Apache Tools, J2SE, J2EE, EJB, Servlets, JSP, HTML, JavaScript, JQuery, JSON and XML.  • Developing the web applications using Spring Framework, Hibernate  • Applying Spring Framework for transaction Management and Spring JDBC for building ORM, and for AOP and Dependency Injection  • Responsible for using AJAX framework with JQuery, Dojo, ExtJs implementation for Widgets and Events handling.  • Customizing Log4J for maintaining information and debugging.  • Customizing third party vendor information using Web services (SOAP and WSDL).  • Developed Request Cash Message and get Cash plugins using Java Beans.  • Worked with development of data access beans using Hibernate, middle ware web service components.  • Develop the GUI using JSP, Spring web flow following Spring web MVC pattern.  • Implemented persistence layer using Hibernate that use the POJOs to represent the persistence database tables.  • Used SVN for version control across common source code used by developers.  • Written the JUNIT test cases for the functionalities.    Environment: J2EE/J2SE, Java1.5, JSP, Ajax4JSF, JSF 1.2, Spring Frame Work 3, Hibernate, JMS, CSS3, Apache CXF, XML, HTML, Oracle. Education Bachelor's Skills JAVA, JAVASCRIPT, APACHE, MODEL VIEW CONTROLLER, MODEL-VIEW-CONTROLLER Additional Information Skills  • JAVA (7 years), APACHE (6 years), JAVASCRIPT (5 years), MODEL VIEW CONTROLLER (5 years), MODEL-VIEW-CONTROLLER (5 years)