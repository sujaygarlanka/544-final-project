Senior Manager - Projects Senior Manager - Projects Senior Manager - Projects - Cognizant Technology Solution USA EDISON • Architecting complex data warehouse and reporting systems with integrated MDM and Metadata management solutions.  • Experience with data architecture and data modeling with Entity-Relationship Modeling, Dimensional Modeling and Data Vault Modeling techniques.  • Experience in new generation cloud databases like Snowflake Database and Amazon Redshift.  • Experience in Amazon Web Services with special emphasis on the cloud databases Amazon Redshift & Dynamo DB in Architecting, data modeling and performance tuning.  • Proficient in creating architectural artifacts and presentations to demonstrate an Architectural opinion to the client.  • Experienced in Healthcare, Insurance, Banking and Media & Entertainment domain  • Experienced in Performance Tuning of Oracle Database Instances from both SQL Query Tuning and Database Parameter tuning perspective.  • Experienced in working with Client Architects and IT Stakeholders to advice about Industry Standards and Best Practices for various Data Management areas, like Data Quality, Master Data Management, Metadata Management Data Warehousing and Reporting Systems.  • Technically managed projects in multi-cultural and multi-vendor environment  • Worked for esteemed clients like NBC Universal, Horizon BCBS NJ, State Farm Insurance, John Deere, Deutsche Bank, Standard & Poor's, PepsiCo, Motorola, American Transit Insurance Corporation and AMEX. Work Experience Senior Manager - Projects Cognizant Technology Solution USA December 2014 to Present Defining Data Architecture, Data Modeling, and preparing Architectural Decision artifacts for various Transactional Processing, Master Data Management (MDM) and Data Warehousing Systems for on-premises and Cloud platforms.    Senior Data Architect / Modeler - for NBC Universal, NYC  NBC Universal is moving their data analytics platform from on-premise to Azure Cloud. Within this initiative they are re-architecting the solutions from an existing Hadoop/Hive based system to Snowflake Database on Azure Cloud platform. The initiative spans the Film Market Analytics and Sports Analytics platforms.    Contribution:  1. Responsible for the Data Architecture for the Enterprise Data Warehouse and Reporting platform.  2. Logical and Physical Data Modeling for the EDW layer with Data vault 2.0 data modeling methodology with Hub, Link and Satellites.  3. Logical and Physical Data Modeling for the Data-Mart with Dimensional Modeling methodology.  4. Designing the integration of the various internal and external data sources coming from Teradata, Wikipedia, Facebook, YouTube and Google Analytics platform,  5. Defining the physical database architecture with Snowflake Database on Azure.  6. Enabling the data security policies for data at rest and in motion.  7. Leading Data Modelers and Snowflake Database team.    Technology: ER-Studio Data Architect, Azure Cloud, Azure BLOB Storage, AWS Cloud, Snowflake DB, Python 3, Talend    Senior Data Architect / Modeler - for Taco Bell, CA  Taco Bell is implementing their new e-Commerce platform in AWS Cloud to enhance their business with real-time customer, order and sales analysis based on stores, offers and products. Apart from the OLTP system they are also implementing a data lake and various data marts to have real-time reporting with Tableau. The solution is implemented in AWS Cloud using Amazon Redshift, S3 Buckets, AWS Step Functions and Lambda along with Talend ETL and Tableau Reporting tool.    Contribution:  1. Responsible for the AWS Cloud based Data Architecture for the S3 bucket and Amazon Redshift.  2. Logical and Physical Data Modeling for the Data-Lake with Data Vault 2.0 data modeling methodology with Hub, Link and Satellites.  3. Logical and Physical Data Modeling for the Data-Mart with Dimensional Modeling methodology.  4. Designing the integration of the various data sources coming from SAP Hybris (orders), IBM Watson Analytics tool Silver Pop (Marketing analytics), First Data (Card Payments) and Google Big Query (click-stream analysis).  5. Designing the ETL layer with Talend to enable data movement from Source to Data Vault and Data Vault to Data Mart database.  6. Leading Data Modelers and the ETL team.  7. Defined the Data Modeling Standards and Best Practices for Amazon Redshift database.    Technology: ER-Studio Data Architect, AWS Cloud, AWS S3, Amazon Redshift, Athena, Talend  Senior Data Modeler - for PepsiCo, NY    PepsiCo is a well-known Food and Beverage manufacturing company in North America with world-wide presence. Recently they implemented an integrated Master Data Management (MDM) System for which normalized MDM structures are required for various subject areas. E.g. Material, Product, Customer, Location, Person, Sales Unit, Route etc. Apart from the MDM they are also implementing an Enterprise Data Warehouse (EDW) for their Management Reposting needs.    Contribution:  1. Responsible for the Logical and Physical Data Modeling for the Master Data Management (MDM) and Enterprise Data Warehouse (EDW) systems.  2. Responsible for overall Architecture and Design of the MDM and EDW  3. Managing the Data Modelers and Designers in the MDM & EDW Teams  4. Defined the Data Modeling Standards and Best Practices for MDM, PBC and EDW teams.    Technology: ER-Studio, Teradata Advisory IT Architect (Information) IBM India Pvt. Ltd October 2008 to December 2014 Defining Data Architecture, Data Modeling, and preparing Architectural Decision artifacts for various Transactional Processing and Data Warehousing Systems for various projects from Healthcare, Banking, Insurance and Industrial sectors.    Data Architect & Data Modeling Lead - for Horizon BCBSNJ  Horizon BCBS NJ is a Health Care Company in Newark, NJ providing Health Insurance and renovating their IT systems. As a data modeling lead worked for various projects like Blue Health Intelligence, Pricing Single Source of Truth, Integrated Data Services, Master Data Management and Enterprise Data Warehouse. This work involved both Dimensional modeling and ER modeling techniques.    Contribution:  1. Responsible for the Logical and Physical Data Modeling for the e-Business, Enterprise Data Warehouse (EDW), Master Data Management (MDM) and Integrated Data Services (IDS) systems  2. Responsible for Maintaining the Data Models in ER-Studio Repository  3. ER-Studio Repository Administration  4. Managed the Data Modelers in the EDW Team  5. Defined the Data Modeling Standards and Best Practices    Technology: ER-Studio  Enterprise Policy and Account Data Management System (ePAD) - for State Farm Insurance  State Farm is remodeling their IT system for their Insurance Business with the aim of better customer experience. Their various Lines of Businesses like Auto, Life, Health and Fire were consolidated, and the single system called ePAD is built.    Contribution:  1. Responsible for the Logical Data Modeling for the e-PAD system  2. Responsible for Analysis of the various Master and Transactional Database System in State Farm.  3. Mainframe COBOL Copybook to ER-Studio Data Model Transformation    Technology: ER-Studio, DB2 (Mainframe)  Reporting Data Marts - for John Deere  John Deere is a large automotive manufacturing organization producing various kinds of Tractors for Agricultural, Construction and Forestry purposes along with some residential machines for non-commercial use. John Deere Reporting Services has multiple medium and large projects for defining the Data-Warehouse and Data Marts for various kinds of Business reports to support their business. The Business areas covered for the reports are Order, Inventory, Sales, Forecasting, and Services.    Contribution:  1. Architected the overall Data Warehouse and Data Marts for the Reporting Services.  2. Designed the Shared Data mart (Master Data Management System) for storing common data across all reporting platforms in John Deere.  3. Defined the Conceptual, Logical & Physical Data Modeling for the Data Marts using Dimensional Modeling Techniques.  4. Leaded a pool of Data Analysts (IBM & non-IBM) for the various source analysis and Data modeling tasks.  5. Review of the HLD and LLD & Providing technical guidance to team members for Data Modeling and ETL related issues.  6. Contributed to the John Deere Enterprise Data Warehouse by providing improvement ideas.    Technology: ER-Studio, Oracle 10g    Order Management & Pricing Engine - for ProQuest  Automating the Order Management Process and Building a Pricing Engine for the ProQuest's Dialog and Data Star products. The Pricing Engine is based on the complex pricing requirements of the ProQuest products sold in various different Plan Types, formats and methods.    Contribution:  1. Defining and Finalizing the Data Architecture with client Architects at Onsite.  2. Conceptual, Logical & Physical Data Modeling for the Pricing Engine.  3. Designing the Data Access mechanism (the API that acceded by external systems) for the Pricing Engine.  4. Designing the MDM Integration with the Pricing Engine  5. Defining the Data Flow and Data Cleansing Rules for the Usage Data.  6. Defining the Order Management Data Integration with the Oracle eBS Service Contract module.  7. Leading the team of PL/SQL Developers to develop the Order Management and Pricing Logic.    Technology: Oracle Data Modeler, Oracle 10g Database    Mortgage Re-Platforming - for Deutsche Bank  The existing System was in Mainframe with part of the System being semi-manual e.g. working with Access and Excel databases. The Account and Repayment part of the system needs to be automated and the Mainframe Loan System will be replaced with Oracle Database and Java J2EE Web Application.    Project Contribution:  1. Defining and Finalizing the Architecture with Senior IBM Architects and Client Architects.  2. Defining the Database Infrastructure for Maximum Availability Architecture with IBM HACMP and Oracle 10g RAC and Data Guard.  3. Capacity Planning and Performance Modeling  4. Defining the Data Migration Strategy from Mainframe DB2 to the Oracle 10g database.  5. Creating the Conceptual, Logical & Physical Data Models for the Oracle target system.  6. Designing the Proof of Concepts for checking technical feasibility of the Architectural Decisions.    Technology: Rational Data Modeler, Oracle 10g Database Database Architect NESS Technologies August 2007 to October 2008 Leaded the Database Architecture effort in Arrow Global and Risk Analysis for Standard & Poor's, NYC and was responsible for the Architecture and the Data Models for various subsystems for the projects. Enriched the Data Management Team by defining the organization wide standards for Data Modeling.    Default & Recovery System (Risk Analysis) - Standard & Poor's, USA  S&P's Default Recovery Department needs a consolidated data source for their analysis. There were 23 heterogeneous source systems comprising of Oracle, DB2, SQL Server, MS Access and various Flat files with Rating Data which needs to be migrated to a single Data Model for Defaulter Analysis. The Data after this Defaulter Analysis is migrated to the Risk Solutions Data Mart for reporting purpose.    Contribution:  1. Defining, Presenting and Finalizing the Data Architecture with client Architects at Onsite.  2. High Level Designing the Master Data management (MDM) and Metadata Management database components.  3. Conceptual, Logical & Physical Data Modeling for the Default Recovery Staging and ODS system using the Entity Relationship Modeling techniques with ER-Win version 7.  4. Dimensional Modeling for the Risk Solutions Data Mart.  5. Defining the Oracle 10g Database Architecture along with the Informatica 8.1 ETL components in the system.    Technology: ER-Studio, Oracle 10g Database    Arrow Global Financial Analysis Platform - Standard & Poor's, USA  Financial Data from 16 different source systems (Oracle, DB2, SQL Server, MS Access, Flat files etc) were consolidated into a Staging area and the data is cleaned and adjusted before putting into the Rating Analysis platform called AG (Arrow Global). AG had a Normalized Data Model to hold all the Financial Data in a Single Data Model for Analysis Modeling of Rating Data. The Data in moved to a Data warehouse after the analysis in the AG platform.    Contribution:  1. Defining and Finalizing the Architecture with client Architects at Onsite.  2. Conceptual, Logical & Physical Data Modeling for the Staging, ODS and AG.  3. Dimensional Model for the Data Warehouse.  4. Defining the Oracle Database Architecture  5. Defining Data Modeling Standards throughout the Organization.  6. Defining the MDM Architecture  7. Defining the Metadata Management Architecture  8. Modeling and Designing the DB2 Staging Area for Extraction of the Source Data and Source Data Cleansing.    Technology: ER-Studio, Oracle 10g Database Database Administrator Megasoft Ltd June 2006 to August 2007 Database Administration, Data Modeling, Database Design and leading the database team for Motorola Public Security Solutions & ATIC Insurance projects. Leaded and guided a group of DBA and Developers in the Database Competency.    No-Fault Claim System - for American Transit Insurance Corporation (ATIC), USA  The system was designed to process Insurance Claims resulting from Car Accidents and automates the workflow of the claim processing.    Contribution:  1. Data Modeling for the Claim System  2. Database Design Reviews and Performance Tuning    Technology: ERwin, Microsoft SQL Server  Motorola ISD Public Service Solution - Motorola, USA  The system was designed to locate the nearest helping resources available at the 311 caller's site automatically and redirecting the resources towards the area. The whole system was controlled with automated GPS navigations.    Contribution:  1. Logical & Physical Data Modeling for the Resource Management & Automatic Navigation modules  2. Designing the Oracle PL/SQL and SQL Server T-SQL modules.  3. Guiding the Development team in writing Database Stored Procedures, Functions, and Triggers.  4. Code Reviews & Performance Tuning of the Modules.    Technology: ERwin data modeler, Oracle 9i Database, Shell Scripting Consultant Self Employed February 2004 to June 2006 Conceptualized and designed a Multi-Component String Matching Algorithm using combination of Phonetic and Artificial Intelligence techniques. Fuzzy Indexing is also possible with this system. This was designed as an API for Data Warehousing and CRM products. Built a database with Fuzzy matching / indexing capability as a proof of concept of this API.    Contribution:  1. Designed and developed the fuzzy matching / indexing algorithm  2. Prepared documentation and artifacts for VC funding request.  3. Contributed in the marketing and strategic partnership discussions.    Technology: Visual C++ Database Team Lead CIANT Technologies December 2001 to January 2004 Designed PL/SQL engine for the cleansing Parsing and Matching modules and leaded the database team in the development of the highly acclaimed tool named DataGym(TM), a Data Cleaning and De-duplication tool built around Oracle Database and is designed to be robust with massive Data handling capacity, extremely fast processing capabilities and extensive dynamism to adhere to flexible functional requirements.    Contribution:  1. Designed the Address Parsing, De-duplication, Merge-Purge, Master Data Management and Metadata Management modules for the DataGYM Product  2. Development of Stored Procedures, Functions, Database Triggers in Oracle 9i database using PL/SQL Developer.  3. In the Client implementations of DataGYM for AMEX and ICICI bank provided Technical Database Administration (DBA) support, Performance Tuning Support to the client implementation team.    Technology: Oracle 9i / 10g Database, Microsoft SQL Server, C, Shell Scripting Software Developer Binary Fusion International January 2001 to December 2001 Started as a Developer and subsequently leaded the Project for SAREGAMA India Limited (previously known as HMV) Payroll System. In this period worked with Oracle8i and Developer 2000(Forms 5.0 and Reports 3.0). Leaded a team of 6 developers and successfully implemented the project.    Factory Payroll System  The system was designed to store the production data from the factory floor along with other Master data, Income Tax & Payroll data and automatically generates the overtime allowance while generating payroll. This system was also integrated with CMS attendance system.    Contribution:  1. Oracle Database Designing  2. Database Performance Tuning.  3. Developing PL/SQL Stored Procedures  4. Designing Forms and Reports with Oracle D2K    Technology: Oracle 8, Oracle Form and Reports 2000 Education Information Technology Management APTECH Bachelor of Commerce in Done Calcutta University - Kolkata, West Bengal 'O' Level Done DOEACC 'O Skills Data modeling, Database, Db2, Microsoft sql server, Microsoft sql server 2008, Sql server, Sql server 2008, Oracle, Pl/sql, Sql, Sybase, Data architecture, Data governance, Data management, Etl, Informatica, Master data management, Mdm, Metadata, Teradata Additional Information TECHNICAL SKILLS    Data Architecture: Enterprise Data Warehouse (EDW), Data-Marts, Master Data Management (MDM), Metadata Management, Data Quality, Data Governance  Data Modeling Methodology: Entity-Relationship (3NF), Dimensional and Data Vault 2.0  Data Modeling Tools: ER-Studio 2016, Erwin 9.16  Cloud Platform: Amazon Web Services (AWS)  Cloud Storage: Amazon S3 & Glacier  Modern Databases: Snowflake Database, Amazon Redshift, AWS Dynamo DB  Databases: Oracle 12c / 11g, DB2 9, Teradata 5, Microsoft SQL Server 2008. Sybase ASE 15.5  Database Tools: SnowSQL, DBeaver, TOAD, PL/SQL Developer, Teradata SQL Assistant  Scripting: UNIX Shell Scripting, Python 3  Programming Languages: Visual Basic, C  ETL Tools: Informatica Power Center 9  Industry Standard Models: IBM Insurance Industry Warehouse Model (IIW)  Insurance Application Architecture (IAA)  Methodologies: Agile, IBM Unified Method Framework, RUP, Waterfall