Talend Developer Talend <span class="hl">Developer</span> Talend Developer - Big Data • Over 8+ years of IT industry experience in all aspects of Analysis, Design, Testing, Development, Implementation and Support of Relational Database (OLTP), Data Warehousing Systems (OLAP) and Data Marts in various domains.  • Around 4+ years of experience with Talend Open Studio & Talend Enterprise platform for Data Management.  • Experience in working with Data Warehousing Concepts like OLAP, OLTP, Star Schema, Snow Flake Schema, Logical Data Modeling, Physical Modeling and Dimension Data Modeling.  • Utilized t-Stats Catcher, t-Die, t-Log Row to create a generic job to store processing stats.  • Involved in extracting user's data from various data sources into Hadoop Distributed File Systems (HDFS)  • Automated all the jobs, for pulling data from FTP server to load data into Hive tables, using Oozy workflows.  • Experience in managing and reviewing Hadoop log files.  • Excellent understanding and knowledge of NOSQL databases like MongoDB, HBase, Cassandra.  • Experience in Hadoop administration activities such as installation and configuration of clusters using Apache, Cloudera and AWS.  • Experience in importing and exporting data using Sqoop and writing custom shell scripts from HDFS to Relational Database Systems and vice-versa.  • Experience with MapReduce, Pig, Programming Model, Installation and Configuration of Hadoop, HBase, Hive, Pig, Sqoop and Flume using Linux commands.  • Experienced in creating Generic schemas and creating Context Groups and Variables to run jobs against different environments like Dev, Test and Prod.  • Thorough knowledge of addressing Performance Issues and Involved in query tuning, Index tuning, Data profiling and other database related activities.  • Extensively created mappings in Talend using t-Map, t-Join, t-Replicate, t-Parallelize, t- Java, t- Java row, t-Die, t-Aggregate Row, t-Warn, t-Log Catcher, t-Filter, t-Global map etc.  • Wrote Hive and Pig queries for data analysis to meet the business requirements.  • Experienced in scheduling Talend jobs using Talend Administration Console (TAC)  • Experience with Talend DI Installation, Administration and development for data warehouse and application integration.  • Expertise in Data modeling techniques like Data Modeling- Dimensional/ Star Schema, and Snowflake modeling, Slowly Changing Dimensions (SCD Type 1, Type 2, and Type 3)  • Tracking Daily data load, Monthly data extracts and send to client for their verification.  • Strong experience in designing and developing Business Intelligence solutions in Data Warehousing using ETL Tools.  • Excellent understanding and best practice of Data Warehousing Concepts, involved in Full Development life cycle of Data Warehousing.  • Experienced in working with different data sources like Flat files, Spreadsheet files, log files and Databases.  • Worked extensively with slowly changing dimensions. Hands-on experience across all stages of Software Development Life Cycle (SDLC) including business requirement analysis, data mapping, build, unit testing, systems integration and user acceptance testing.  • Excellent interpersonal and communication skills, and is experienced in working with senior level managers, business people and developers across multiple discipline Work Experience Talend Developer Big Data October 2015 to Present Bank Of New York Mellon, New Jersey    Responsibilities:  • Worked closely with Business Analysts to review the business specifications of the project and to gather the ETL requirements.  • Closely worked with Data Architects in designing of tables and even involved in modifying technical Specifications.  • Since this is Migration project we are doing migrating from data stage to Talend Using Big data Components.  • We are working sprint wise. Using Talend big data components like Hadoop and S3 Buckets and AWS Services for redshift  • Involved in Extraction, Transformation and Loading of data.  • Utilized Big Data components like tHDFSInput, tHDFSOutput, tHiveLoad, tHiveInput, tHbaseInput, tHbaseOutput, tSqoopImport and tSqoopExport.  • Work with the offshore team for the day to day work and review the tasks done by them get the status updates in the daily meetings.  • Data ingestion with different data sources and load into redshift.  • Developed jobs to send and read data from AWS S3 buckets using components like tS3Connection, tS3BucketExist, tS3Get, tS3Put.  • Designed and Implemented the ETL process using Talend Enterprise Big Data Edition to load the data from Source to Target Database.  • Involved in Data Extraction from Flat files and XML files using Talend by using Java as Backend Language.  • Using Talend to load the data into our warehouse systems  • Used over 20+ Components in Talend Like (tMap, Tfilelist, Tjava, Tlogrow, ToracleInput, ToracleOutput, tsendEmail etc)  • Used debugger and breakpoints to view transformations output and debug mappings.  • Load and transform data into HDFS from large set of structured data /Oracle/Sql server using Talend Big data studio.  • Used Big Data components (Hive components) for extracting data from hive sources.  • Wrote HiveQL queries using joins and implemented in tHiveInput component.  • Utilized Big Data components like tHiveInput, tHiveOutput, tHDFSOutput, tHiveRow, tHiveLoad, tHiveConnection, tOracleInput, tOracleOutput, tPreJob, tPostJob, tLogRow.  • Develop ETL mappings for various Sources (.TXT, .CSV, .XML) and also load the data from these sources into relational tables with Talend Enterprise Edition.  • Worked on Global Context variables, Context variables, and extensively used over 70+components in Talend to create jobs.  • Extracting transformed data from Hadoop to destination systems, as a one-off job, batch process, or Hadoop streaming process.  • Worked on Error handling technique's and tuning the ETL flow for better performance.  • Worked Extensively TAC (Admin Console), where we Schedule Jobs in Job Conductor.  • Extensively Used Talend components tMap, tDie, tConvertType, tFlowMeter, tLogCatcher, tRowGenerator, tOracleInput, tOracleOutput, tfileList, tDelimited etc  • Migrated the code and release documents from DEV to QA (UAT) and to Production.  • Design and Implemented ETL for data load from heterogeneous Sources to SQL Server and Oracle as target databases and for Fact and Slowly Changing Dimensions SCD-Type1 and SCD-Type2.  • Created complex ETL jobs for data exchange from and to Database Server and various other systems including RDBMS, XML, CSV, and Flat file structures.  Environment: Talend Data Integration 6.4/6.3.1, Talend Enterprise Big Data Edition 5.1, Talend Administrator Console, MS SQL Server 2012/2008, Oracle 11g, HDFS, Hive, Impala, Sqoop, Cloudera (CDH 5), TOAD, UNIX. Sr. Talend Developer Intercontinental Hotels Group - Atlanta, GA August 2013 to September 2015 Responsibilities:  • Participated in all phases of development life-cycle with extensive involvement in the definition and design meetings, functional and technical walkthroughs.  • Created Talend jobs to copy the files from one server to another and utilized Talend FTP components  • Created and managed Source to Target mapping documents for all Facts and Dimension tables  • Used ETL methodologies and best practices to create Talend ETL jobs. Followed and enhanced programming and naming standards.  • Created and deployed physical objects including custom tables, custom views, stored procedures, and Indexes to SQL Server for Staging and Data-Mart environment.  • Utilized Big Data components like tHDFSInput, tHDFSOutput, tPigLoad, tPigFilterRow, tPigFilterColumn, tPigStoreResult, tHiveLoad, tHiveInput, tHbaseInput, tHbaseOutput, tSqoopImport and tSqoopExport.  • Extensively used tMap component which does lookup & Joiner Functions, tjava, tOracle, txml, tdelimtedfiles, tlogrow, tlogback components etc. in many of my Jobs Created and worked on over 100+components to use in my jobs.  • Used Talend most used components (tMap, tDie, tConvertType, tFlowMeter, tLogCatcher, tRowGenerator, tSetGlobalVar, tHashInput & tHashOutput and many more)  • Created many complex ETL jobs for data exchange from and to Database Server and various other systems including RDBMS, XML, CSV, and Flat file structures.  • Created Implicit, local and global Context variables in the job.  • Worked on Talend Administration Console (TAC) for scheduling jobs and adding users.  • Worked on various Talend components such as tMap, tFilterRow, tAggregateRow, tFileExist, tFileCopy, tFileList, tDie etc.  • Developed stored procedure to automate the testing process to ease QA efforts and reduced the test timelines for data comparison on tables.  • Automated SFTP process by exchanging SSH keys between UNIX servers. Worked Extensively on Talend Admin Console and Schedule Jobs in Job Conductor.  • Involved in production n deployment activities, creation of the deployment guide for migration of the code to production, also prepared production run books.    Environment: Talend Data Integration 6.1/5.5.1, Talend Enterprise Big Data Edition 5.5.1, Talend Administrator Console, Oracle 11g, Hive, HDFS, Sqoop, Netezza, SQL Navigator, Toad, Control M, Putty, Winscp ETL Developer Talend - Dallas, TX November 2012 to July 2013 Responsibilities:  • Coordinated with Business Users for requirement gathering, business analysis to understand the business requirement and to prepare Technical Specification documents (TSD) to code ETL Mappings for new requirement changes.  • Analyze and create low level design document (LLD) and mapping document.  • Created Hadoop cluster connections to access HDFS.  • Extensively used components like tWaitForFile, tIterateToFlow, tFlowToIterate, tHashoutput, tHashInput, tMap, tRunjob, tJava, tNormalize and tfile components to create Talend jobs.  • Used Talend components such as tmap, tFileExit, tFileCompare, tETLAggregate, tOracleinput, tOracleOutput etc.  • Developed jobs to move inbound files to HDFS file location based on monthly, weekly, daily and hourly partitioning.  • Developed jobs to expose HDFS files to Hive tables and Views depending up on the schema versions.  • Worked on Talend Administrator Console (TAC) for scheduling jobs and adding users.  • Created jobs to perform record count validation and schema validation.  • Created contexts to use the values throughout the process to pass from parent to child jobs and child to parent jobs.  • Developed joblets that are reused in different processes in the flow.  • Developed error logging module to capture both system errors and logical errors that contains Email notification, updating tables and moving files to error directories.  • Performed unit testing and integration testing after the development and got the code reviewed.  • Extensively worked on AutoSys to schedule the jobs for loading data.  • Implemented Error handling in Talend to validate the data integrity and data completeness for the data from the flat file. Designed and Developed Oracle PL/SQL and UNIX Shell Scripts, Data Import/Export.  • Used mapping parameters and variables for pulling incremental loads from source. Identified and fixed the Bottle Necks and tuned the Mappings and Sessions for improving performance. Tuned both ETL process as well as Databases.  Environment: Talend Studio 5.2.2 /5.6, Talend Studio Big Data Platform, HBase, XML files, Flat files, HL7 files, HDFS, Hive, Oracle 11g, Business Objects, UNIX, WinSCP, Clear Case, Clear Quest, Erwin, PL/SQL, Toad, Windows 7 Pro, TFS, JIRA. Java Developer Wisdom I.T.S - Hyderabad, Telangana June 2009 to October 2012 Responsibilities:  • Worked on both WebLogic Portal for Portal development and WebLogic for Data Services Programming.  • Developed the presentation layer using JSP, HTML, CSS and client validations using JavaScript.  • Used GWT to send AJAX request to the server and updating data in the UI dynamically.  • Developed Hibernate 3.0 in Data Access Layer to access and update information in the database.  • Used JDBC, SQL and PL/SQL programming for storing, retrieving, manipulating the data.  • Involved in designing and development of the ecommerce site using JSP, Servlet, EJBs, JavaScript and JDBC.  • Used Eclipse 6.0 as IDE for application development Configured Struts framework to implement MVC design patterns.  • Validated all forms using Struts validation framework and implemented Tiles framework in the presentation layer.  • Designed and developed GUI using JSP, HTML, DHTML and CSS. Worked with JMS for messaging interface.  • Used Hibernate for handling database transactions and persisting objects deployed the entire project on WebLogic application server.  • Used AJAX for interactive user operations and client-side validations Used XSL transforms on certain XML data.  • Used XML for ORM mapping relations with the java classes and the database. Developed ANT script for compiling and deployment. Performed unit testing using Junit.  Environment: Java/J2EE, Oracle 10g, SQL, PL/SQL, JSP, EJB, Struts, Hibernate, WebLogic 8.0, HTML, AJAX, Java Script, JDBC, XML, JMS, XSLT, UML, JUnit, Log4j, Eclipse 6.0