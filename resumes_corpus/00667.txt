ETL/Application Developer ETL/Application Developer business intelligence specialist/Application Developer - Remarkable Technology Group Washington, DC • 11 years of IT experience with 6 years in the ETL developing, testing, deployment, maintenance, and production support and database administration.  • Proficiency utilizing ETL tool Informatica Power Center 9.1/8.x/7.x for developing the Datawarehouse loads with work experience focused in Data Integration as per client requirement.  • Expertise in designing confirmed and traditional ETL Architecture involving Source databases Oracle, Flat Files (fixed width, delimited), DB2, SQL server, and Target databases Oracle, Teradata and Flat Files (fixed width, delimited), MySQL.  • Expertise in Extraction, Transformation and Loading (ETL) process, Dimensional Data Modeling, experience using Data modeling, Star schema/Snowflake modeling, Fact and Dimensions tables, dimensional, multidimensional modeling and De-normalization techniques. Thorough understanding of Kimball and Inmon methodologies.  • Created ETL mappings, Mapplets, reusable transformations, sessions, workflows, worked on performance tuning of ETL mappings and debugged.  • Experience in creating pivot tables for reporting  • Expert in data visualization using tools such as tableau, ssrs, microstratgegy and excel.  • Knowledge of medical coding systems, i.e. ICD9, CPT, HCPCS, ACGs is desirable.  • Expertise in using Tableau to develop reports and data visualization  • Expertise in using DB DESIGNER and ERWIN in creating database tables with entity relationships  • Expertise in loading data into operating data storage  • Experience in using Sql Server integration and visual studio tools  • Signifiant expérience with data quality, data profiling, metadata management and reporting.  • Expérience in working with customer development team to ingest approximately 175 datasets of varying types  • Expérience in Performing extraction, transformation, and load of data sets in differing formats.  • Expérience in the enhancement of reports for dissemination or offline viewing  • Expérience design enhancements to the data base schema, dictionary and normalize data into a consistent schema to accept new data types  • Experience in writing SQL queries/scripts for both source and target databases.  • Experience in Production Validation Testing and Source to Target Testing (Validation Testing)using DVO(INFORFMATICA VALIDATION OPTION).  • Experience in developing database schemas and data marts  • Experience in using REST/SOAP API  • Experience in using TFS for versioning  • Experience in creating Test cases  • Experience in loading and defining data from and into Amazon Redshift and S3 buckets  • Experience in ingesting data from API sources to ETL process  • Experience in using ETL in SLDC environment  • Experience in loading data into Datawarehouse and modeling Work Experience ETL/Application Developer Remarkable Technology Group May 2014 to Present Collected and analyzed data obtained from subscribers; audit the data to ensure compliance with the required Data Requirements Standards to write a comparison analysis report on the extracted data for both internal and external use.  • Coordinated data gathering analysis for marketing teams from multiple internal and external sources to improve marketing strategies.  • Designed queries for monthly and quarterly reports for marketing and senior management.  • Extensively used Excel VBA functions in development, Focusing on read/write integration to databases  • Extracted, transformed and analyzed data from data warehouse, generated reports, identified trends, gaps in data quality and provided vital information to address root causes.  • Supported Clients Services Teams and improved sales by producing ad-hoc analysis of key trends in the financial services sector.  • Created and documented findings from analyzed data in report and presentation formats for proper data visualization.  • Manipulated, cleansed & processed data using Excel, Access and SQL, while maintaining quality control of data at the point of collection and entry.  • Worked closely with the clients, clarifying the data requirements to ensure that data captured by the clients are accurate and to minimize data discrepancies.  • Processed, generated and produced regular and ad-hoc reports with Data unit head.  • Performed Detailed Data Analysis (DDA) and Data Quality Analysis (DQA) on source data.  • Used Tableau to get reports and populate on live portal to check for data compliance  • Define connections for Talend metadata ( DATABASE IN USE IS MYSQL)  • Create Mappings in Talend open studio using tMap,tJoin, to fulfill business requirements  • Explore and access source files to know what am trying to migrate  • Talend is a code generation tool, define dtPrejob to both connection component (Amazon redshift and DB (mysql))  • Using tMysqlTablelist to specify what tables to migrate. Use tMysqlglobal variable  • Configure tMysqlglobal variable so that I can pull columns from tables being gathered by MysqlTablelist  • Add a tFixedFlowInput to generate the "tableName" and "columnName" columns.    • Adding a tLogRow after the fixed flow allows seeing the names of the tables and columns that the job is pulling from by displaying the information on the run console.  • Set schema to be dynamic by using tSetDynamicSchema component of Talend open studio to set the schema based on the value Colunmname, to avoid having to move tables individually  • After above processes have been completed, because the data is on premises, need an input component that reads from database(MYSQL),tMysqlInput to make sure data is being pulled.  • Write out data into CSV file by using tFileOutputeliminited  • Files are then moved out to Amazon S3 using tFileList and TS3Put (assuming S3 instance this name) into specific folder  • Using tPostJob tMysqlClose and tS3Close I can control what happens after the loop is complied  • Schedule or run the job  • Once data is moved to Amazon S3, import/copy directly to Amazon Redshift ETL Engineer Remarkable Technology Group - Buffalo, NY April 2018 to March 2019 • Experience in using Informatica Client Tools and Talend open studio  • Developed various mappings using different transformations like Source Qualifier, Expression, Filter, Joiner, Router, Union, Unconnected / Connected Lookups and Aggregator.  • Closely worked with other IT team member's business partner's data steward's stakeholders steering committee members and executive sponsors for all MDM and Data governance related activities  • Experience in creating High Level Design and Detailed Design in the Design phase.  • Experience in integration of various data sources like Oracle, DB2, MS SQL Server and Flat Files with different delineation  • Experience in packages to extract, transform and load data (ETL) using SSIS, designed packages which are utilized for tasks and transformations Data Conversion and Pivot tables.  • Extracted Data from multiple operational sources of loading staging area, Data Warehouse and data marts using CDC/ SCD (Type1/Type2) loads.  • Experienced in creating Reusable Tasks (Sessions, Command, Email) and Non-Reusable Tasks (Decision, Event Wait, Event Raise, Timer, Assignment, Worklet, Control)  • Worked with multiple Informatica repositories, version control, deployment groups, mapping and folder migrations.  • Fixed the invalid mappings and troubleshoot the technical problems of the power center  • Co-ordinated with various team members across the globe i.e. Application teams, Business Analysts, Users, DBA and Infrastructure team to resolve any technical and functional issues in UAT and PROD.  • Created various technical documents required for the knowledge transition of the application which includes re-usable objects (Informatica & UNIX)  • Responsible for gathering requirements for developing mapping documents for OFSSA application and also populating required fields for OFSSA  • Responsible for mapping M&T mortgage data on a daily and weekly basis  • Define sources and target using Talend open studio for Talend projects  • Responsible for updating dimensional table from data warehouse environment using Talend.  • created mappings in Talend using tMap, tJoin, tReplicate, tParallelize  • Developed shell scripts in Unix environment to support scheduling of the Talend jobs.  • Worked on Talend components like tReplace, tmap, tsort and tFilterColumn, tFilterRow  • Experienced in Exporting and importing Talend jobs  • Performed data migration from Mysql to Amazon Redshift/S3 bucket using Talend Open Studio  • Performed unit testing on populated fields  • Responsible for analyzing mapping document and understanding business requirements  • Experienced in creating data in staging layer (cleansing landing and enterprise)  • Proficient in documenting mapping documents and updating them  • Responsible for data modelling using DB DESIGNER 4  • Responsible for exporting newly created database table design into the database ETL Developer Communicare Healthcare services - Washington, DC January 2013 to April 2018 • Converted the business requirements into technical specifications for ETL process.  • Design source definitions and target definitions based on requirements  • Creating Informatica Mappings and workflows to migrate data from one environment to another and data enhancements.  • Involved in optimization of sql queries  • Created New Staging tables in Staging DB to store data from client files (CMS, MD, Medicaid Files etc.)  • Parsing high-level design specs to simple ETL coding and mapping standards.  • Analysis/Preparation of High Level Design (HLD) and preparation of Low level design (LLD).  • Populated the Fact and Dim. Table of data warehouse from various sources like Oracle, Ms Access and Flat files.  • Created staging tables to do validations against data before loading data into original fact and dim. Tables.  • Extensively used Update Strategy and Lookup transformations for updating the target tables and also extensively used the Debugger to check the data flow and the mappings were modified depending on the requirements.  • Handled alerting mechanisms, system utilization issues, performance statistics, capacity planning, population and maintenance.  • Designed and developed complex Aggregator, Joiner, Router, Lookup and Update strategy transformation rules.  • Created sessions, database connections and worklets using Informatica Power center  • Involved in performance tuning by identifying bottlenecks, eliminating them and tuning the PL/SQL used in Transformations.  • Used Pre-session and Post-session procedures to Drop and recreate indexes while the session runs to improve performance while loading to the target.  • Created session partitions to increase the performance of Informatica Server Manager.  • Responsible for data and table modelling using data modeler tools such as VBA and ERWIN  • Involved in writing UNIX shell scripts for Informatica ETL tool to run the Sessions.  • Automated the daily scheduling of ETL process for populating the data warehouse.  • Scheduled the ETL jobs daily, weekly and monthly based on the business requirement.  • Performing Review/Checks for component's Design, Codes and Quality as per Informatica Standards.  • Performs data cleansing and Data quality  • Environment: Informatica Power Center 9.1.1, PL/SQL and SQL Developer, Informatica/Autosys MYSQL Database Administrator Communicare Health Care Services January 2011 to December 2013 • Pioneered the development and implementation of MySQL databases.  • Provided Development, test and production databases DBA support and everyday maintenance.  • Proficient in Standard query languages (DML, DDL, DCL, Stored Procedures, triggers).  • Experienced installing and configuring MySQL databases with proven best practice configurations. Comfortable with my.cnf parameter file customizations.  • Expertise in InnoDB storage engines. Comfortable with InnoDB storage engines and migration from one to the other as best needed. * Performed 5.5 to 5.6 upgrade and patching of MySQL database software.  • Knowledge in Bash shell and VB scripting for task automation and very comfortable navigating and configuring UNIX environment.  • Comfortable with Performance tuning. Ability to identify performance issues, Query optimization, Indexing, Partitioning, Explain plan and rectifying performance issues in the most efficient way to maximum performance. Utilizing Performance Schema views, slow-query log, Percona tools - pt query digest, pt-osc, pt-table-checksum etc. Table maintenance - Fragmentation, error, check table, repair table.  • Strategized Backup for maximum uptime and efficiency using tools like xtrabackup, mydumpyer, mysqldump.  • Tracked Issues and provided troubleshooting as needed in a timely manner. Using BMC ticketing system and automated tracking system like MONyog alerts, Zabbix, Grafana.  • Replication & Recovery organization's best practices. - Master-Slave, Master-Master. Cloned and set up new environments.  • Provided Security management - Users/Grants, privileges, database remediation based on NIST standards Education Bachelor of Science Ogun State University Nigeria