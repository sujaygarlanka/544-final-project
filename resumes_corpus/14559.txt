Hadoop Developer Hadoop <span class="hl">Developer</span> Hadoop Developer Edison, NJ • Overall 5 years 10 months of experience in software development life cycle like Analysis, Design, Implementation, Testing and partial support of Core JAVA 7, Big Data eco system and Big Data Analytics.  • Worked as anIT Analyst with Tata Consultancy Services, Pune and Hyderabad.  • Over 2+ years of experience in web application development using JAVA/J2EE technologies.  • Worked entirely in Banking domain.  • Exposure of end-to-end development of software development from system study, designing, coding, testing, de-bugging, documentation and implementation.  • Acquires good understanding of JIRA and maintaining JIRA dashboards.  • Experience on working with Amazon Web Services like EC2 Linux operating system.  • Expert level of skills and experience in internet and GUI technologies: Web based application development such as JAVA, SERVLET, JSP, JDBC and XML.  • Expertise in Client Side Designing and Validations using HTML5, CSS3, Java Script, JSP.  • Expert in using Java IDE's like Eclipse and IntelliJ  • Used Maven for building projects.  • Over 3+ years of experience in Hadoop architecture and various components such as HDFS Namenode, Datanode and MapReduce Job Tracker, Task Tracker and programming paradigm.  • Experience in using Hadoop Technologies such as HDFS, SQOOP, HIVE, Impala, Flume, Spark.  • Strong experience in writing Map Reduce jobs in Hive.  • Experience with Databases like SQL, MySQL and MongoDB.  • Extensively worked on importing and exporting data from different systems to HDFS using SQOOP.  • Extensively works on using Hadoop ecosystem components for storage and processing data, exported data into Tableau using Live connection.  • Experienced in working with Hadoop eco system using Hive-QL on different formats like Text file, CSV file.  • Good experience on creating databases, tables and views in HiveQL  • Implemented Hadoop stack and different bigdata analytic tools, migration from different databases (i.e. SQL, MySQL) to Hadoop.  • Load and transform large sets of structured, semi-structured data using Hadoop ecosystem components.  • Worked on building hive and map-reduce scripts.  • Having experience on using OOZIE to define and schedule the jobs.  • Good Knowledge on Zookeeper Sentiment Analysis.  • Having experience on Managing HDFS file system.  • Having knowledge on Python, MongoDB.  • Experience and knowledge of real time data analytics using Spark Streaming and Flume.  • Implementing Spark using Python and Spark SQL for faster testing and processing of data responsible to manage data from different sources.  • Good experience on all flavours of Hadoop(Cloudera, Hortonworks, MapR etc).  • Excellent communication skills, interpersonal skills, problem solving skills a very good team player along with a can-do attitude and ability to effectively communicate with all levels of the organization such as technical, management and customers.  • Ability to easily adapt and learn any new technology or software. Authorized to work in the US for any employer Work Experience Hadoop Developer Tata Consultancy Services, Pune May 2014 to April 2017 Description: Creatingdashboard for data visualization based on everyday critical and as well as failed transactions in order to monitor their status rather than checking each and every transaction status using middleware applications status manually. GUI provides level of criticality for a particular transaction and charts for number failures. It allows the monitoring team take appropriate action within SLA - Service level agreement.    Responsibilities:  • Involved in analyzing the system and business.  • Involved in database connection by using SQOOP.  • Involved in importing data from MySQL to HDFS using SQOOP.Involved in migrating tables from RDBMS into Hive tables using SQOOP  • Written Hive UDFs to sort Structure fields and return complex data type.  • Writing Hive queries to load and process data in Hadoop File System.  • Creating Hive tables, loading with data and writing hive queries which will run internally in map reduce way.  • Involved in loading data from UNIX file system to HDFS.  • Installed and configured Hive and also written Hive UDFs.  • Worked with NoSQL databases like Hbase in-making Hbase tables to load expansive arrangements of semi structured data.  • Zookeeper Sentiment Analysis on reviews of the products on the client's website.  • Involved in transferring files from OLTP server to Hadoop file system.  • Involved in writing queries with HiveQL and Pig.  • Export and Import data into HDFS, HBase and Hive using Sqoop.  • Load and transform large sets of structured and semi structured data.  • Loading data into Hive partitioned tables  • Create reports for the BI team using Sqoop to export data into HDFS and Hive.  • Importing and Exporting Data from Oracle to HiveQL.  • Importing and Exporting Data from HiveQL to HDFS.  • Process and analyze the data from Hive tables using HiveQL.    Environment:Amazon EC2, HartonWorks-Ambari, Hadoop, HDFS, Map Reduce, Hive, Hbase, Sqoop, Flume, CentOS, Linux, Spark Core JAVA Developer Tata Consultancy Services, Pune October 2011 to April 2014 Worked as Hadoop Developer atTata Consultancy Services, Puneand Hyderabad from May'2014-April'2017.    Project Responsibilities: Java Developer Tata Consultancy Services, Pune October 2011 to April 2014 Description: Creating web based GUI/ dashboard for Autosys jobs in order to monitor their status rather than checking each and every jobs status manually. GUI helps in tracking file-watcher and all other jobs. It allows the monitoring team to contact upstream/downstream teams in time and to run the jobs complete successfully.    Responsibilities:  • Utilizing Java, Java EE, JSP, Apache Server and SQL.  • Created roles to the customers based on their designation and teams.  • Responsible for creating front end applications, user interactive (UI) web pages using web technologies like HTML5, CSS3, JavaScript.  • JUnit test cases for the classes post development.  • Using SVN delivered and pushed code to Integration and QA environments on time for BA and QA signoffs.  • Extreme attention to accuracy, detail, presentation and timeliness of delivery.  • Used PCF for monitoring application stability and continuous integration.  • Object storage service Amazon S3 is used to store and retrieve DB information.  • Debugging production issues, root cause analysis and fixing.  • Involved in setting up Maven configuration and helping Continuous Integration (CI) Issues.  • Extensively used SVN as the version controlling Tool for check-ins and check-outs.  • Involved in debugging the defects, code review and analysis of Performance issues.  • Involved in Jenkins configuration.    Environment: Java, Java EE, JSP, Apache Server, SQL, Windows7, SVN, GIT, Jenkin Builds Education Master's in Computer Science and Engineering New Jersey Institute of Technology - Newark, NJ September 2007 to May 2011 Skills Hdfs, Mapreduce, Sqoop, Hbase, Flume, Hadoop, Mongodb, Hadoop, Hbase, Hive, Javascript, Mapreduce, Python, Scripting, Database, Mysql, Sql, Apache, Linux, Unix Additional Information Technical Expertise:    Hadoop Framework:  • HDFS, MapReduce  • Hive, Hbase  • Sqoop, Flume    Database Technology:  • HiveQL  • SQL, MySQL  • MongoDB    Operating Systems:  • Windows 10  • Linux, CentOS 7    Application/Web servers:  • Apache Tomcat    Scripting:Unix Shell, Javascript Languages:Core Java, Python