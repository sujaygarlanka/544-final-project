Database Administrator <span class="hl">Database</span> <span class="hl">Administrator</span> Database Administrator - Unissant Inc Woodbridge, VA 15402 Weldin Dr, Woodbridge, VA - 22193  Phone: (847) 331-9062  E-mail: stanleyreddy@gmail.com    • Active DHS Full BI Security Clearance: Customs & Border Protection [suitability determination - validity till 12/12/2022]  • Public Trust (Level 4) Security Clearance: Office of Personnel Management (Completed BI, & waiting adjudication in OPM).  • IBM Certified DB2 for z/OS Database Administrator  • IBM Certified Database Associate  • IBM Certified Application Developer  • Oracle, SQL Server, MySQL, NoSQL, DB2 Database Admin  • Hortonworks Hadoop Admin (academic credential)  • Hortonworks HDP Developer: Enterprise Apache Spark (academic credential)  • Hortonworks HDP Developer: Apache Pig, & Hive (academic credential)  • MongoDB for DBA's (MongoDB University) (academic credential)    • Database Administrator @ Unissant Inc, 12901 Worldgate Dr #600, Herndon, VA 20170 (Client: Office of Personnel Management, 1900 E St NW, Washington, DC 20415: Oct 2018 - Current.  • Data Engineer @ ManTech International Corporation, 2250 Corporate Park Dr, Herndon, VA 20171 (Client: Customs & Border Protection, 5971 Kingstowne Village Parkway, Alexandria, VA.): Oct 2017 - Sept 2018  • Database Administrator @ Randstad Technologies, 150 Presidential Way, Woburn, MA 01801(Client: Anthem BlueCross Blue Shield of Ohio, 4361 Irwin Simpson Rd, Mason, OH 45040): Aug 2017 - Sept 2017  • Database Administrator @ Blue Cross Blue Shield of IL (HCSC), 300 E Randolph St, Chicago, IL: Feb 2007 - May 2017  • Database Administrator @ Ciber Inc, 205 N Main St, Bloomington, IL 61701 (Client: STATEFARM Corporate South(SF), Bloomington, IL - 61701): Jun 2006 - Feb 2007  • Database Administrator @ Apex Systems Inc, Columbia, MD 21045 (Client: Consultant DBA @ Lockheed Martin @ Social Security Administration, Baltimore, MD): May 2006 - Jun 2006  • Information Systems Engineer @ Blue Cross Blue Shield of SC, I-20 & Alpine Rd, Columbia, SC 29219: Jan 1999 - Feb 2006 Work Experience Database Administrator Unissant Inc - Herndon, VA October 2018 to Present 12901 Worldgate Dr #600, Herndon, VA 20170 (Client: Office of Personnel Management, 1900 E St NW, Washington, DC 20415: Oct 2018 - Current. Data Engineer ManTech International Corporation - Herndon, VA October 2017 to September 2018 2250 Corporate Park Dr, Herndon, VA 20171 (Client: Customs & Border Protection, 5971 Kingstowne Village Parkway, Alexandria, VA.): Oct 2017 - Sept 2018 Database Administrator Randstad Technologies - Woburn, MA August 2017 to September 2017 150 Presidential Way, Woburn, MA 01801(Client: Anthem BlueCross Blue Shield of Ohio, 4361 Irwin Simpson Rd, Mason, OH 45040): Aug 2017 - Sept 2017 Database Administrator Blue Cross Blue Shield of IL (HCSC) - Chicago, IL February 2007 to May 2017 Database Administrator Ciber Inc - Bloomington, IL June 2006 to February 2007 205 N Main St, Bloomington, IL 61701 (Client: STATEFARM Corporate South(SF), Bloomington, IL - 61701): Jun 2006 - Feb 2007 Database Administrator Apex Systems Inc - Columbia, MD May 2006 to June 2006 Columbia, MD 21045 (Client: Consultant DBA @ Lockheed Martin @ Social Security Administration, Baltimore, MD): May 2006 - Jun 2006 Information Systems Engineer Blue Cross Blue Shield of SC - Columbia, SC January 1999 to February 2006 Unissant Inc (Client: OIG of Office of Personnel Management): Washington DC.    Database Admin, Systems Admin, Data Modeler, & Data Architect:    • Lead Database Administrator to Create Cloud (AWS) platform for DB2 V 11.1.4 on Linux BLU Data analytics, and migrate existing SAS based DBMS system to DB2 on Linux, & RDS(PostgreSQL 11.1).  • Setup, Install DB2 V11.1.1 on AWS RHEL 7.6x86_64, applied Mod4 Fixpack patches to upgrade to V11.1.4. Did install Data Server Manager V2.1.5.2.  • Prepare Data Architecture, Data Migration, & Data Security Documents for the Clients.  • Working to setting up & Configuring HADR on DB2 for Linux for Failover. on AWS Cloud.  • Working to setting up & Configuring IBM Security Guardium 10.6.0 Database Activity Monitoring (DAM).  • Setup, & Configure PostgreSQL 11.1 (upgraded from PostgreSQL 10.6) with pgAdmin 4 v3.6 in AWS Cloud, and associate it as a Data hub repository for erwin Data Modeler Workgroup Edition, Concurrent User License 2018 R1.  • Provide client connectivity using with DataStage V11.7, IBM DataStudio client V4.1, SQuirreL SQL Client V3.9.  • Installation, Configuration, setup and deployment of (Hortonworks) HDP Hadoop cluster 7 Data nodes, 2 Master Nodes, & 4 Edge nodes in AWS Cloud.  • Installation, Configuration, setup and deployment of Cloudera Hadoop cluster 5 Data nodes, 2 Master Nodes, & 4 Edge nodes in AWS Cloud.  • Installation, Configuration, setup 2 node Microsoft Windows Server enabling "Kerberos constrained delegation with Active Directory support" feature to specify and enforce application trust boundaries by limiting the scope where application services can act. in AWS Cloud.    Mantech International Corporation(Client: Customs and Border Protection): Alexandria.    Data Engineer:  • Part of Data Engineering group to capture, & document the data sources, provenance and Architecture of the CBP EMIS data warehouse. This effort should provide better tools for data mining and Data Science for Data warehouse, & Big Data efforts.  • Involved in developing, re-engineering PL/SQL procedures with Regular Expressions for very critical analytics project.  • Involved with analyzing the Metadata, Data structures, Relationships, Data Sources for both Cargo, & Passenger Systems.  • Involved as Database Engineer providing support to BigData, & Oracle for OLAP.  • Importing data from Multiple sources into Data Lake / Delta Lake using Talend, Kafka, Apache NiFi.  • Data mining, Analtics, & Visualzation using Anaconda, Python, R, Elastic Search, Elastic Lucene etc.    Randstad Technologies (Client: Blue Cross Blue Shield of OH (Anthem)- Mason, OH    Consultant Database Administrator:  • I was involved as Database Administrator with Modernization expert of Legacy IMS, & DB2 database systems, to Oracle in addition to everyday monitoring, maintenance, enhancements and new project development for applications.    Blue Cross Blue Shield of IL (HCSC): Chicago, IL  Sr Database Administrator:  • HCSC is one of Nations Premier Healthcare Insurance provider, with a membership of over 13 million, & am currently involved as Database Administrator with zero down time, and supporting with maintenance, enhancements and new Database development for applications, Database Migrations, DBMS Systems, Performance Tuning, Capacity planning of applications - DB2, Oracle, Hadoop, MongoDB DBMS systems.  • Responsible for High Availability, & accessibility of databases. This effort involves 24X7 support of 230 million per day OLTP Health Claims transactions in DB2, & oracle Sub systems on z/OS, AIX, & RHEL.  • Worked, & supported development teams (Integrated Medical Management) develop Python applications to come up with pricing models for first-time un-insured health insurance subscribers to generate pricing for actuarial team. Tuples of first-time insurers were mined from historical data, for generating health costs associated with those health subscribers for first three years with age range.  • Was part of team(Integrated Medical Management) to analyze the impacts of migration of ICD-7 to ICD-10 Implementation on Health claims by providers, & institutions utilizing Python. Python applications were also developed to make old claims to be compatible to new format of medical procedure codes.  • Extending support to Oracle, Hadoop, & MongoDB along with DB2 to embrace multi DBMS support concept, and embrace paradigm of management for Open Source Software, & Commodity Hardware to reduce costs.  • Install, configure, & setup 3 Master Node, 47 Slave Node, & 15 Edge Node Cluster. Developed process to ingest Structured, & Semi-structured data into HDFS using Scoop, Talend, Elastic Search, KAFKA. Helped in supporting Flume Ingestion, and configuring Source, Channel, Sink, & setup automated backup's using Oozie etc.    Ciber Inc (Client: Statefarm Corporate South): Bloomington, IL    DB2, Oracle Database Administrator, & Data Specialist:  State Farm Corporate South is a centralized headquarters for its data processing functions. Statefarm is the Nation's leading auto insurer.    • Was involved as project DBA in FSS group with maintenance, enhancements and new Database development for applications. Responsible for Physical database design, development of databases, maintenance Jobs, Performance validations involving Unicode, Ebcdic, Ascii translations using SQL Developer, and Power BI.    Apex Systems Inc (Client: Lockheed Martin @ Social Security Administration, Baltimore    DB2 Database Administrator/ High Performance Design Analyst:  Lockheed Martin supports the complete range of services related to Social Security Administration's major systems modernization initiatives.    • Involved with analysis and design of implementation of DB2 Ver. 8 Release on very large Social Security Administration databases and migration of databases from BDAM to DB2. The objective is to migrate with zero issues to DB2 Version 8 and analyzing its impact on Applications and DB2 systems with version upgrade.  • DB2 Administration Tool, CA tools, Platinum Products, BMC Mainview, BMC tools, IBM tools    Blue Cross Blue Shield of SC: Columbia, SC  Information Systems Engineer:  • Modernization of systems for 24X7 availability of systems, adjudication of health claims, Smart Health card.  • Migration of VSAM to DB2 in line with the paradigm of the organization to have a single data infrastructure for growing business and technical needs, like faster processing of Claims, access of Data by customers and deployment of new applications, consistency in quality of data, real time availability of data 24X7, facilitating running of Sysplex environments, and concurrency in data updates between online and batch. Education Bachelor's Skills Database administration, Database, Db2, Dbms, Sql server, Sql server 2008, Mysql, Oracle, Pl/sql, Postgres, Postgresql, Sql, Stored procedures, Vsam, Ambari, Hdfs, Mahout, Mapreduce, Oozie, Sqoop Additional Information TECHNICAL PROFILE    • Be pro-actively engaged in new methodologies like Agile, DevOps, SCRUM, Waterfall, & Extreme Programming etc. The future state of Architecture has contained Mainframe, & transition to Open Source Platforms, & Architectures.  • Adept at managing multiple concurrent projects, being attentive to detail and maintaining the ability to make rational decisions in pressure situations. Willingness to own responsibility and work to resolve issues.  • Ability to work with commitment, passion, and thrust to exceed Customer's expectation both as an Individual, and as a Team member with the knowledge that each Customer's expectations are unique.  • DB2 for LUW V 11.1.4 on Linux, PostgreSQL V 11.4, pgAdmin v4.10, Oracle V 12.2, MySQL V 8.0, Red Hat Enterprise Linux 7.5, SQL Server 2008, 2008 R2, 2012, 2014, Hadoop V2.4, V2.6, V2.8, V3.0 (Cloudera, & Horton works), Legacy z/OS systems IMS, DB2, & IDMS to Oracle, Oracle, & PL/SQL Development expertise.  • Administration of Anaconda Enterprise. My responsibilities include installation, configuration, and management of the application, proficiently navigating within the linux.  • Create custom python environments and install and maintain them on various Hadoop clusters.  • Work with Data Scientists as they launch python and pyspark jobs, or deploy models, that interact with data on hdfs and hive.  • Engage with users transitioning from traditional data sources to Python  • Working to migrate a SAS Data warehouse on premise to AWS Cloud's RDS, DB2, by building data pipelines using NiFi  • Developed ETL and analytics focused Spark Core applications in Python, including Spark SQL applications to interact with HIVE tables  • Building data pipelines to migrate data from Teradata to Hadoop env (HDP)  • Initiated poc to automate data pipelines using Oozie  • Worked towards building a Data Lake that is discoverable, accessible, and usable given diverse constraints: on-prem, hybrid, and cloud environments  • Ensure that data hydrating into the data lake has appropriate metadata/tags with it using various enterprise metadata management tools  • Capture metadata throughout the data life cycle and consolidate it so as to enable search/browse/sample data  • Import of unstructured data into HDFS using Flume, using both File and Memory channels  • Develop Pig Latin scripts, and writing UDF's in Python when needed, to extract, transform, and mine data  • Write Hive Queries for analyzing data in Hive warehouse using Hive Query Language (HQL).  • Develop novel Abstract Data Structures, and Functional Abstractions to process data (text analysis, human-machine interactions, weblogs, and social media streaming)  • Apply principles of Object Oriented Programming in designing and implementing applications, with a specific focus on controlling complexity  • Focused on creating warehouses, databases to meet the rapidly changing needs of various business units:  • Create, execute, and schedule both master and standalone jobs, with a focus on parallel execution, when writing large files onto databases  • Extensive experience in supporting Multi-DBMS data migrations (IMS to DB2 Migration, VSAM to DB2 Migration, CA-IDMS to DB2 Migration, SAS to DB2 for Linux V11.1 Migration, Legacy to AMS Migration), DB2 to Oracle migration, & DB2 to SQL Server Migration.  • High proficiency in designing, building and administering Oracle clustered server configurations supporting 11g and 12c Real Application Clusters (RAC) installations on Linux, & AIX.  • Ability to work on developing stored procedures, Functions and Triggers, PL/SQL procedures, Korn shell scripts.  • Installing & configuring Oracle cluster ware & Database Software (Troubleshooting & handling common issues that arise during integration of the whole - bugs, network issues, configuration files issues, OCR issue, de-installation & cleanup of cluster ware)  • Ability to Work with customers for various RAC related issue: [RAC recovery, RMAN, ASM, OCR corruptions, voting disk loss etc.]  • Troubleshoot performance issues for the RAC instances (GC events)  • Backup and Recovery issues related (loss of OCR, Voting Disk and Oracle cluster ware) & also issues with corruption of individual disk data blocks, loss of OS configuration files, loss of net configuration files. Also used the Merge backup for backing terabyte databases. Issues with block change tracking & also flash recovery area.  • Add or Remove Node from RAC.  • Importing / Ingesting data from Multiple sources into Data Lake / Delta Lake using Talend, Kafka, Apache NiFi, Flume.  • Automated all the jobs thru Zena scheduler for extracting the data from different Data Sources like DB2, Oracle, MongoDB, MySQL, & Postgres to pushing the result set data to Hadoop Distributed File System.  • Installation, Configuration, setup and deployment of Hadoop cluster 46 Data nodes, 2 Master Nodes, & 15 Edge nodes.  • Configured High Availability cluster for Automatic failover.  • Creates a SOLR schema from the Indexer settings, & Implemented SOLR index cron jobs. Experience in writing SOLR queries for various search documents.  • Responsible for defining the data flow within Hadoop eco system and implement them.  • MongoDB DBA Administration, MongoDB Schema Design, & Installation, Configuration and Deployment of MongoDB.  • Ability to Support creating shards, replica sets, monitoring, and projections for Mongo Systems.  • SQL Server 2008, 2008 R2, 2012, & 2014 Database Administration, and support T-SQL, SSIS, SSRS, & HADR.    TECHNICAL & PROFESSIONAL EXPERIZE    • Data Tools: Hadoop (Cloudera, Hortonworks) V2.7, V3.0, Spark V2.0, MapReduce, Yarn, Hive, HDFS, Pig, Hbase, Flume, Sqoop, R: Regression, Predictive Analysis, Data Mining, Sentiment Analysis, Hue, Apache Ambari, NIFI, SOLR, RapidMiner, R, Mahout, Tableau, SQL (T-SQL, PL/SQL, MySQL, Hive SQL, PostreSQL), Python, Data frames, Lab, Juniper, PyCharm, IDLE, Anaconda, IPython, PyScripter, Pig, HCatalog, Java, JSP, Eclipse, Maven, SparkSQL, HTML, XML etc.  • Oracle 12C, SQL Server 2008, 2008 R2, 2012, 2014, MYSQL V8, DB2 for LUW V11,1,4, DB2 for z/OS V13, IMS V13, PostGreSQL V10.8, Amazon RDS, Red Hat Enterprise Linux 7.5.  • Operating Systems: LUW, AIX, IBM z 13 Series, Red Hat Enterprise Linux, Hortonworks Data Platform, Spark V1, V2.  • Systems: AWS Cloud Computing, Clustered Computing, Distributed File Systems, Business Intelligence Systems, Data Mining Systems, Reporting and Dash boarding Systems.