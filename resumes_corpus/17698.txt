AWS DEVELOPER AWS <span class="hl">DEVELOPER</span> AWS DEVELOPER - PYTHON • Strong experience in AWS Cloud Engineer (Administrator) and working on AWS. Services IAM, EC2, VPC, EBS, EFS, EIP, AMI, SNS, RDS, Dynamo DB, Cloud Watch, Cloud trail, Auto. Scaling, S3, and Route 53.  • Worked in various Linux server environments from DEV all the way to PROD and along with cloud powered strategies embracing Amazon Web Services (AWS) . Good understanding of the principles and best practices of Software Configuration Management (SCM) in SDLC methodologies like Agile/Scrum and Waterfall.  • Strong experience in CI (Continuous Integration) /CD (Continuous Delivery) software development pipeline stages like Commit, Build, Automated Tests, Deploy etc.  • Experience to build servers using AWS which includes importing necessary volumes, launching the EC2 instance, creating security groups, auto-scaling, load balancers, Route 53 and SNS as per the architecture.  • Experience on setting up the life cycle policies to back the data from AWS S3 to AWS Glacier, Worked with various AWS, EC2 and S3 CLI tools.  • Experience in Linux container technology including LXC, Docker and Rocket. Have setup custom lxc.  • Deployment experience with Docker and Kubernetes.  • Expertise in AWS platform and its dimensions of scalability including EC2, ECS, Elastic Beanstalk, S3, EBS, VPC, ELB, SNS, RDS, IAM, Route 53, Auto scaling, Cloud Front, Cloud Watch, Security Groups.  • Expertise in DevOps, Release Engineering, Configuration Management, Cloud Infrastructure, Automation. It includes Amazon Webservices (AWS), Ant, Apache Maven, Jenkins, SVN, GitHub, Tomcat, JBoss, and Linux etc.  • Excellent knowledge of Amazon EC2, Amazon S3, Kinesis, Amazon Elastic Load Balancing and other services of the AWS family.  • Experienced with various AWS services like EC2, VPC, EBS, AMI, SNS, SQS, SES, RDS, Cloud Watch, EMR, Cloud Trail, Auto-Scaling, Route 53, Cloud Front, IAM, S3, Lambda, Cloud Formation, Dynamo DB.  • Experienced in creating User/Group Accounts, Federated users and access management to User/Group Accounts using AWS IAM service.  • Set-up databases in AWS using RDS, storage using S3 bucket and configuring instance backups to S3 bucket.  • Experience in project deployment using Heroku/Jenkins and using web services like Amazon Web Services (AWS) EC2 and AWS S3.  • Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format.  • Experience in AWS Cloud platform and its features which includes EC2, AMI, EBS Cloudwatch, AWSConfig, Auto-scaling, IAM user management, and AWS S3.  • Expertise experience working on the Plugins, Frameworks & Packages: Django, Flask, Webapp2, MapReduce, Celery, SQL Alchemy, PyMongo, WTForms, jinja2, Bootstrap, jQuery, pycrypto, Mock, Nose, Sphinx.  • Experience in project deployment using Google Cloud/Jenkins, Elastic Search and using web Services like Amazon Web Services (AWS)  • Experience with Python development under Linux OS (Debian/Ubuntu/SUSE Linux/Red Hat Linux/Fedora)  • Experienced in WAMP (Windows, Apache, MYSQL, Python /PHP) and LAMP (Linux, Apache, MySQL, and Python) Architecture. Experience in writing Sub Queries, Stored Procedures, Triggers, Cursors, and Functions on MySQL and PostgreSQL database.  • Having experienced in Agile Methodologies, Scrum stories and sprints experience in a Python based environment, along with data analytics, data wrangling and Excel data extracts.  • Good conceptual understanding and experience in cloud computing applications using Amazon EC2, S3, EMR.  • Very strong experience writing API's and Web Services in PHP and in Python and utilized Python Pandas framework to provide data analysis.  • Experience in NoSQL databases like Apache Cassandra (1.2, 2.0, and 2.1) and MongoDB (2.6, 2.4), Orient DB.  • Proficient in using Amazon Web Services (AWS) . Extensive experience focusing on services like EC2, Elastic Beanstalk, CloudWatch, CloudFront, Cloud Formational RDS, Elastic ache, S3, SQS, SNS, AMI.  • Experience in using AWS Services like S3, Lambda, EC2, DynamoDB, Kinesis Streams, EMR and Cloud Formation Template. Work Experience AWS DEVELOPER PYTHON - Bakersfield, CA January 2017 to Present • Implemented AWS solutions using EC2, S3, RDS, EBS, Elastic Load Balancer, VPC, Auto scaling groups.  • AWS Cloud management and Chef automation. Experience in Infrastructure Development and Operations, involved in designing and deploying utilizing almost all the AWS stack like EC2, EBS, S3, VPC, LDAP, VPN, RDS, SES, ELB, Autoscaling, CloudFront, CloudFormation, Elastic beanstalk, CloudWatch, SNS, Route 53, LDAP, VPN.  • Used Elastic Search and DynamoDB Streams to automatically index chat messages for future analysis.  • End to End solution for hosting the web application on AWS cloud with integration to S3 buckets. Managing AWS Infra and automation with CLI, API.  • It is a user interface where data scientists are using for searching for particular data from the Hadoop, Teradata, Hana and DB2 This data is extracted from the database using extraction process and loaded into elastic search server.  • Configure AWS VPC with public and private subnets, configured Routing tables, Internet gateway, ACL's, Security groups.  • Created the AWS VPC Architecture; planned, built and configured network infrastructure within the VPC including public and private subnets, Elastic IP address, NAT Gateway, databases, Elastic Load Balancers, Routes and Routing Tables, Security Group, NACLs, Direct Connect, Virtual Private Gateway, Internet Gateway and private endpoint and more.  • Worked as a software developer for developing applications that will assist in migration and also worked on application helping in better functioning of API Gateway.  • Deployment of the web application using the Heroku server and designed database model for the entire application. Monitoring AWS services EC2, S3, RDS through Cloud Watch.  • Writing AWS Terraform templates for any automation requirements in AWS services. Created database objects in AWS Redshift. Followed AWS best practices to convert data types from oracle to redshift.  • Responsible to write Terraform modules for automating the creation of VPC's and launching AWS EC2 Instances. Modules are written for creation of VPC and VPN connection from data center to production environment and cross account VPC peering.  • Supported application deployment in IAAS Cloud platforms like Rackspace, AWS and Google Compute engine and was responsible for resource planning of various virtual compute engines.  • Developed views and templates with Python and Django's view controller and template language to create a user-friendly website interface. Migrated applications to the AWS cloud Environment.  • Created Python and Bash tools to increase efficiency of retail management application system and operations; data conversion scripts, AMQP/Rabbit MQ, REST, JSON, and CRUD scripts for API Integration.  • Developed tools using Python, Shell scripting, XML to automate some of the menial tasks.  • Working on project deployment using Heroku/Jenkins and using web services like Amazon Web Services (AWS) EC2 and AWS S3.  • Migrated the database to AWS RDS using Database Migration Service; Used Kinesis for zero downtime migration.  • Create Micro Services to access AWS service like S3, DynamoDB, EC2, Lambda, Cloud Watch, and SNS.  • Perform analysis and studies of current DHS Data Framework Accumulo, Solr and Elastic Search Architecture between Neptune and Cerberus projects, prepared presentations for DHS stakeholders and lead Spark and MapReduce development efforts.  • Built custom transformations using AWS Glue, Lambda and Kinesis, helped reduce costs in ETL tool & on-premise infrastructure.  • Running of Apache Spark, CDH distros, Elastic MapReduce (EMR) on (EC2)  • Built Lambda functions to spin-up the EMR clusters for automating the data ingestion process using the PySpark  code.  • Implementing a Continuous Delivery framework using AWS Code pipeline, code commit Jenkins, Chef, Maven & Nexus in Linux server's environment.  • Deployed microservices in Docker Swarm cluster using Docker Compose.  • Implementing continuous integration and deployment (CI/CD) systems using AWS code pipeline, Jenkins, ANT, Maven.  • Used Terraform for managing the infrastructure through the terminal sessions and executing scripts in creating alarms and notifications for EC2 instances using AWS Cloud Watch. Wrote Ansible Playbooks for various applications and deploying them in AWS using Terraform.  • Used Jenkins and pipelines to drive all micro services builds out to the Docker registry and then deployed to Kubernetes, Created Pods and managed using Kubernetes.  • Representation of the system in hierarchy form by defining the components, subcomponents using Python and developed set of library functions over the system based on the user needs.  • Development of Python APIs to dump the array structures in the Processor at the failure point for debugging.  • Written Programs in Spark using Scala and Python for Data quality check.  • Wrote and executed several complex SQL queries in AWS glue for ETL operations in Spark data frame using sparksql.  • Worked with Python Libraries/Packages such as Py Query, Pickle, PyQT, Pymock, PySide, wxPython, PyTables, Data Frames, SQLAlchemy, Twisted, Pygame, pyGtk, Pandas and Pyglet, PyQT, PyGtkpywin32, ntlk, nose, SymPy, Ipython.  • Developed a fully automated continuous integration system using Git, Gerrit, Jenkins, MySQL and custom tools developed in Python and Bash. Developer AWS - Sunnyvale, CA February 2014 to December 2015 • Worked on core AWS services (S3, EC2, ELB, EBS, Route53, VPC, Auto scaling etc.) and deployment services (Elastic Beanstalk, Ops Works and Cloud Formation) and security practices (IAM, Cloud Watch and Cloud trail)  • Developed data modeling for analytics by building queries using Map/Reduce (pymongo/mongodb) and implementing for visualization using Elastic search, kibana and logstash on Apache server.  • Migrated present Linux environment to AWS/CentOS/RHEL by creating and executing a migration plan per scheduled timeline to complete the migration.  • Deployed JSON template to create a stack in Cloud Formation which include services like Amazon EC2, Amazon S3, Amazon RDS, Amazon Elastic Load Balancing, Amazon VPC, SQS and other services of the AWS infrastructure.  • Implemented and established best practices around using elastic search, Install and configuring monitoring scripts for AWS EC2 instance.  • Developing microservice applications in the AWS Cloud Native environment.  • Involved in designing and deploying multitude applications utilizing almost all the AWS stack (Including EC2, Route53, S3, RDS, Dynamo DB, SNS, SQS, IAM) focusing on high-availability, fault tolerance, and auto-scaling in AWS Cloud Formation.  • Created AWS Laun5ch configurations based on customized AMI and use this launch configuration to configure auto-scaling groups and Implemented AWS solutions using EC2, S3, RDS, Dynamo DB, Route53, EBS, Elastic Load Balancer, Auto-scaling groups.  • Creation of Subnets and Route tables, Internet gateway, Virtual gateway. Included security groups, Network ACLs, Internet Gateways, and Elastic IP's to ensure a safe area for organization in AWS Public cloud.  • Creating Slack Bots, AI Bots involving AWS Serverless (AWS SAM, Lambda, API Gateway, DynamoDB, S3 Website) for assisting Operations team.  • Recommended for critical services requiring high availability, mutating schemas, and consistent, single-digit millisecond latency at scale.  • Used Amazon Aurora for recommending for services with stable schemas requiring high availability and strong referential integrity.  • Used Amazon Relational Database Service (Amazon RDS) for PostgreSQL or MySQL recommending for non-critical services and ease of set up, operation, and scaling.  • Used AWS Database Migration Service (AWS DMS) which helped in migrating databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. DMS can migrate data to and from most widely used commercial and open-source databases.  • Used AWS Schema Conversion Tool (AWS SCT) which made heterogeneous database migrations predictable by automatically converting the source database schema and a majority of the database code objects, including views, stored procedures, and functions, to a format compatible with the target database.  • Collected and processed website clickstream data using Amazon Kinesis (real-time analytics)  • Implemented the log archival process for the transient Amazon EMR clusters using UNIX shell scripting.  • Utilized AWS Cloud platform and its features which include EBS, EC2, SNS, RDS, Cloud Watch, Cloud Trail, EMR, Lambda, Cloud Formation, Auto scaling, Cloud Front, S3 and Route 53.  • Configuration of Continuous Integration and Continuous Delivery using Code Pipeline and Code Deploy for automation.  • Integrated services like GitHub, AWS Code Pipeline, Jenkins and AWS Elastic Beanstalk to create a deployment pipeline.  • Used Identify and Access Management (IAM) to assign roles and to create and manage AWS users and groups, and user permissions to AWS resources.  • Working with best DevOps practices using AWS, Elastic Bean stalk and Docker with Kubernetes.  • Wrote Chef Recipes for various applications and deployed them in AWS using Terraform. Python Developer Vodafone - Bengaluru, Karnataka July 2011 to November 2013 • Involved in analysis, specification, design, and implementation and testing phases of Software Development Life Cycle (SDLC) and used agile methodology for developing application.  • Wrote and executed various MySQL database queries from python using Python-MySQL connector and MySQL DB package.  • Analyzed current legacy infrastructure, established business case, and built consensus for virtualized IaaS/ PaaS near term strategy with SaaS mobile cloud services end state.  • Used several Python libraries like wxPython, NumPy and matPlotLib.  • Used JSON based and RESTful API for information extraction.  • Worked with python editors like PyCharm, PyScripter, PyStudio, PyDev, Wing IDE and Spyder.  • Involved in converting Hive/SQL queries into Spark transformations using Spark RDDs, Python and Scala.  • Worked on Hadoop technologies like Apache Spark, Scala, and SparkSQL.  • Created new connections through applications for better access to MySQL database and involved in writing SQL & PLSQL - Stored procedures, functions, sequences, triggers, cursors, object types.  • Developed tools using Python, Shell scripting, XML, BIG DATA to automate some of the menial tasks.  • Closely involved in performance evaluation and User Acceptance Test (UAT)  • Developed a portal to manage and entities in a content management system using Flask.  • Wrote programs for performance calculations using Numpy and SQLalchemy.  • Worked on CSV files while trying to get input from the MySQL database.  • Wrote python scripts to parse XML documents and load the data in database.  • Worked in stages such as analysis and design, development, testing and debugging.  • Organized data exchange and integration with customers and third-party systems using CSV, XLS, XML, DBF, JSON, REST, and SOAP.  • Deployed the project into Heroku using GIT version control system.  • Used Amazon Cloud EC2 along with Amazon SQS to upload and retrieve project history and experienced in project deployment using Heroku/Jenkins and using web services like Amazon Web Services (AWS) EC2, AWS S3, Auto scaling, CloudWatch and SNS.  • Used Apache Couchdb (NoSQL) in AWS Linux instance in parallel to RDS MySQL to store and analyze job market info.  • Working on NoSQL technologies like MongoDB, Cassandra and relational databases like Oracle, SQLite, PostgreSQL, Dynamo DB and MySQL databases.  • Cloud Native Practice Lead helping with Thought Leadership (public speaking, client sessions, community  building), internal capability building, creating sales material etc.  • Leveraging queueing architectures with Rabbit MQ for scalability, performance and building.  • Worked on Application servers like Weblogic and Apache Tomcat.  • Wrote Python scripts to manage the AWS resource from API calls using BOTO SDK.  • Used Python 3.X (numpy, spicy, pandas, scikit-learn, seaborn)  • Created Business Logic using Python to create Planning and Tracking functions and developed multi-threaded standalone applications using Python and PHP.  • Developing automation tools for various projects, adding new features to existing ones.  • Installed the application on AWS EC2 AMI, Red hat, Ubuntu Instances.  • Implemented AWS solutions using EC2, S3, RDS, EBS, Elastic Load Balancer, Auto scaling groups, Optimized volumes and EC2 instances.  • uses Amazon Elastic Compute Cloud (Amazon EC2) instances in Auto Scaling groups to create a web services tier for its mobile applications  • Deployed and maintained production environment using AWS EC2 instances and Elastic Container Services.  • Created A search tool where we are using Elastic Search tool for searching metadata form the database.  • Maintained the user accounts (IAM), RDS, Route 53, VPC, RDB, Dynamo DB, SES, SQS and SNS services in AWS cloud.  • Helped create and manage Cloud instances of the API Gateway.  • Used Lambda functions to insert and retrieve data into DynamoDB.    Skills  Operating systems Windows, Linux, UNIX    Languages C++, Python 3.x/2.x, Ruby on Rails    Scripting languages CSS, AJAX, PHP    Markup languages HTML, XML, JSON, Bootstrap    Servers Apache Tomcat  Databases Oracle, My SQL, Postgress,  Virtualization openstack, vagrand, docker, lxc, lxd,    IDEs/ Tools Sublime text, PyCharm, Pgadmin, SQLite    Revision Controlling Systems CVS, GitHub  Methodologies & tools Agile Scrum, Waterfall    MS office tools Microsoft Excel (Super user), Microsoft word, Microsoft PowerPoint,    PPTPlex  Microservices Docker, Ansible, Jenkins    Cloud Services AWS, Azure, Kubernetes    Testing Frameworks PyUnit, Splunk, Selenium.    AWS Services Internet Gateway, API Gateway, Lambda, Load Balancer, Dynamo DB, Cloud trail, Kinesis,  SNS, Appsync, Elastic Search, Elastic Beanstalk, Batch, EMR, Code Pipeline, S3, SNS,  Terraform, Athena, Glue, Cloud Native, Data Visualization, AWS DMS, AWS SCT, Amazon  RDS, Neptune, SQS, IAM.