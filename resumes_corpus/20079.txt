Site Reliability Engineer Site Reliability Engineer Software Developer • Experience in Analysis, Design, Development, Testing, Customization, Bug fixes, Enhancement, Support and Implementation of various stand-alone, client-server enterprise applications  • Extensive experience in PYTHON PROGRAMMING.  • Achived extensive experience in Business Intelligence BI tool, MSBI.  • Working experience in Microsoft SQL SERVER.  • Solid experince in creating ETL Packages using SSIS  • Real Time experience in ROBOTIC AUTOMATION PROCESS(RPA) used KAPOW and BLUE PRISM tools for automation process.  • Skillful experience in Data analysis concepts like - Machine learning, Stasitical methods, Scrapy, Pandas, Beautiful soup, numpy, Scipy, Pickle, SQLITE, NLTK, JSON, PYQT4 ..  • Experince in data Visulization and hands on Matlablib, Tableau, SSRS  • Knowledge on data analysis tools like R, Microsoft Azure, SAS  • Extensive Automation Testing done using PYTEST, Unit Test for GUI's and analytics.  • Working experience in Unix (Ubuntu) for morethan 1.5 years and excellent shell scripting programming skills.  • Familiar with data architecture including data ingestion pipeline design, Hadoop information architecture, data modeling and data mining, machine learning and advanced data processing.  • Hands on experience in AWS, Git, JIRA.  • Working knowledge on IOT stack tools like AWS Green Grass, Raspberry Pi etc  • Experience in writing Sub Queries, Stored Procedures, Triggers, Cursors, and Functions on MySQL and PostgreSQL database.  • Familiar with JSON based REST Web services.  • Having experienced in Agile Methodologies, Scrum stories and sprints experience in a Python based environment, along with data analytics, data wrangling and Excel data extracts.  • Excellent Interpersonal and communication skills, efficient time management and organization skills, ability to handle multiple tasks and work well in a team environment.  • Excellent written and oral communication skills with results-oriented attitude.  • Experience in working with different operating systems WINDOWS, LINUX. Work Experience Site Reliability Engineer Bank Of America December 2017 to Present Software Developer Union Bank - San Diego, CA June 2017 to August 2017 Web Scraper)    Description: The Project involves data retrieving from multiple websites, formatting them and saving back to FileNet. The process automation is the main objective of the    Responsibilities:  • The main languages used for the project are Python, SQL, JS  • Finished a project that deals with 100's of gigabytes of data and used ETL processes for the whole project.  • The ROBOTIC PROCESS AUTOMATION was done using KAPOW tool  • Used KAPOW Tool to extract data from websites.  • Implementing database usage for time taking tasks and reduced time consumption.  • Java Script was used a lot for many tasks and proper outcomes.  • The data visulazation was done using tableau  • Used SQL to build tables and run queries.  • Converted data into JSON, CSV, XML files according to the requirements.  • Fixed bugs for first two months and made several changes in the existing codes.  • Automated several tasks for easy usage of business users.  • Testing is done extensively and SQL reporting service is used to report the activities.  • Finished a project from start point to end point by my own and got a good feedback from the manager and client.  • Have experience in working multiple projects a time.  • Created ETL packages in SSIS for process quality betterment.    Environment: Python, Kapow, Tableau, ETL, SQL, JAVA SCRPIT, XML, JSON, HIVE, Shell Scripting, HTTP web services, Windows. California University November 2016 to March 2017 Data analysis  • Finished the whole project using PYTHON and R  • Used a lot of OOPs concepts for valid output  • The modules Like PANDAS was handy for all statistical analysis  • The WEB SCRAPING part was done using SCRAPY modules and tools like Blue prism  • MACHINE LEARNING for forecasting purpose and modules like SCI-SKIT, NLTK Learning and used regression Models for process improvement  • By using SQL express 2014 many tables were created and reports were also made.  • Used API's for ML or data analytics.  • Data Was stored using JSON, CSV formats  • The Data visualization was implemented using libraries like SeaBorn, Matlablib, tableau.  • Used AWS Elastic Search and S3 storage for data storage.  • Coding was done on Windows OS IPYTHON editor California University March 2016 to August 2016 of management and Sciences March 2016- Aug 2016  DJANGO Web Development  • The whole project was developed using PYTHON and DJANGO Framework  • Used a lot of OOPs concepts for a valid output  • The Frame work DJANGO was used vastly and the apps like HTML  • The WEB SCRAPING part was done using SCRAPY modules and tools like Blue prism  • Came across of REST web services in developing process.  • The Developing process was done in IPYTHON editor  • Testing was done using PYTEST module to check the output.  • Used AWS Elastic Search and S3 storage for data storage.  • Coding was done on Windows OS California University March 2015 to August 2015 Python Programming  • The multiple projects using were finished using PYTHON 3.X version  • The usage of OOP's concepts was handy for many task  • Over 600 's programs were written using Sublime text editor  • Mastered many intermediate level Python concepts like Decorators, Generators, Modules  • Worked on both the versions of python 2.X and 3.X later changed to 3.X  • Coding was done on LINUX OS Python developer WeSecureapp - Hyderabad, Telangana January 2014 to November 2014 Description: The assignment involves in the data collection from Clients and storing them and available for user friendly.    Responsibilities:  • Developed entire backend modules using Python on Web2py framework.  • Extensively worked in backend development using Python.  • Retrieved data using ELASTICSEARCH and used all other modules of Elastic's  • Developed API modularizing existing python module with the help of pyYAML libraries.  • Used several python libraries like wxPython, numpy and matPlotLib.  • Responsible for user validations on client side as well as server side.  • Automated the existing scripts for performance calculations.  • Worked with JSON based REST Web services and AWS.  • Used ML extensively for analyzing and forecasting.  • Built development environment with JIRA, Stash/Git.  • Followed AGILE development methodology to develop the application.  • Placed data into JSON files using Python  • Participated in the complete SDLC process.  • Testing was done using PYTEST module to check the output.  • Involved in Python OOP code for quality, logging, monitoring, and debugging code optimization.  • Used Django Database API's to access database objects.    Environment: Python, SQL, SCI-Kit learn, XML, JSON, Eclipse, Shell Scripting, HTTP web services, Windows, Linux. Internship/JAVA Intugine Technologies Pvt. Ltd - Bengaluru, Karnataka June 2013 to December 2013 developing  • Contributed to servlet based application development.  • Assisted in maintaining and updating existing applications and modules.  • Helped design form validation programs using HTML  • Provided assistance and support to programming team members as require. Internship ING - Hyderabad, Telangana January 2013 to April 2013 • Conducting market research and finding local vendors in the city  • Getting vendors on board by pitching the benefits of partnering with company  • Brainstorming and coming up with ideas and suggestions for the benefit of the company Education Master of Science in Computer Information System California University of Management and Sciences January 2015 to March 2017 Skills AWS (1 year), JSON (1 year), SQL (1 year), DJANGO (1 year), REST (1 year) Additional Information Skills    Languages PYHON, SQL, [JAVA, C, Java script, Shell Scripting](basics)  Framework KAPOW, MSBI, Blue Prism, HTML, DJANGO, AWS, Tableau  Software development skills Agile methodologies, Water fall Methods, Iterative process, Project management, Operations and managements  Knowledge on Concepts HTTP, HTTPs, AJAX, REST API'S, API's, JSON, R, AZURE, SAS  Platforms Windows/Vista/XP, UNIX, Linux  Hadoop MapReduce, ETL, Hive, Spark(Basics)