Big Data Engineer SQL ETL Big Query Big Data Engineer SQL ETL Big Query Vancouver, WA, WA Innovative and self-motivated, I create robust solutions to unique challenges using open source software. Work Experience Big Data Engineer SQL ETL Big Query Bill.com - Palo Alto, CA April 2019 to Present • Developed Google Big Query SQL scripts to extract and transform data  for Bill.Com's Product Growth Unit Department.  • Created ETL scripts that extracted data from MixPanel (MP) using MP's  API scripted in Google Cloud Shell and converted to Google Big Query  tables for the Data Analytics teams.  • Extracted raw JSON data from MixPanel and converted to Google Big  Query tables using Linux Debian commands such as JQ, BQ, CURL,  SED, BASH  • Automated ETL for A/B Web Experiments so Analysts could see results of  experiments automatically without needing to preprocess their data.  • Extracted data from Google Big Query and ran Linear and Polynomial  Regression analysis using R.  • Created Google Data Studio and Google Sheets graphs such as  GeoMaps, Scatter Plots and Bubble Charts to display and analyze data  extracted.  • Created SQL scripts using Google's Advanced Window Analytic  Functions Senior SSIS SQL Server Data Migration Integration Engineer (Contract Role) Enable Networks Limited - Christchurch, Canterbury September 2018 to November 2018 • Developed Microsoft SSIS ETL scripts to automate the creation of purchase order lines from SQL Server and legacy applications into Microsoft Dynamics NAV. This involved creating XML API calls and handling API responses including errors and exceptions.  • Developed Microsoft SSIS ETL scripts to automate creation of new leads into Sales Rabbit for door to door sales. This involved creating JSON API calls and handling API responses including errors and exceptions.  • Created SQL Server queries, views, functions and procedures to import, cleanse and move data from multiple external systems into and out of Microsoft SQL Server. Linux Debian Python MySQL SQL SSIS Data Migration Data Science ETL Integration API Parts Quarry LLC - Wilmington, DE 2007 to 2018 • Developed a large data warehouse (Kimball Method) which connected to the US Defense Logistics Information System (DLIS). Developed a US Military supply parts business, conceptualizing the company from inception including managing a team of 10 data entry, 1 contract administrator and 1 warehouse operator all remotely.  • Developed Microsoft SQL Server / MySQL / MariaDb SQL Procedures for automating large complex data migration ETL processes. The information was refreshed and maintained automatically using Linux Bash scripts and SQL procedures.  • Extensive use of SQL Procedure writing including query tuning and optimization; index analysis and optimization; slow and general query log analysis.  • Scripted hot backups of MySQL and MariaDb.  • Scripted the creation of and maintained multiple Linux distributions including Debian, Centos, Ubuntu, and Linux Mint.  • Created Microsoft SQL Server / MySQL / MariaDb backup and restoration procedures.  • Developed and maintained remote server backup processes in Linux using rclone/rsync/ssh allowing for fast server to server transfer speeds (DropBox and Google Drive among others).  • Designed a mechanism using Linux Bash and SQL to import all open RFQs (Request for Quote) received by US Military for Foreign Military Sales within NATO.  • Maintained DNS records including DKIM and SPF1 records used for email security.  • Mapped the processes of the data entry department, optimized those processes and developed a PHP / MySQL web based data entry system used for an overseas data entry team which reduced data entry costs by +90% and errors by +99%.   • Created a quote requesting system that automatically faxed and emailed tailored RFQs to specific vendors tied directly to the data warehouse. This created the workload, for the overseas data entry team of 20,000 pages of quotes a month.  • Developed reports in Microsoft Visual Studio.  • Optimized profit and revenue by developing a Monte Carlo Simulation on historical quotation data. The resulting analysis led to the recreation of our pricing and sourcing policies which decreased work load by 80% and allowed the company to keep 95% of net profit compared to pre-optimization.  • Mapped the processes of the contract administrator, optimized those processes and developed a PHP / MySQL web based contract administration management system. This resulted in reducing contract administration costs by 60% and eliminated errors.   • Mapped the processes of the parts packaging department, optimized those processes and recreated the workflow. Developed all the software needed to encode the RFID shipping tags, automate the production of DD250 forms and all needed milspec packaging forms and all shipping labels. This software ensured the compliance with government contract regulations and guidelines.  • Automated invoice generation with the US Defense Department's Wide Area Workflow (WAWF) website using the DLA’s sftp flat file upload schema to Ogden Utah. Developed a batch file creator to import data into and synchronize contract administration system with QuickBooks and MySQL. Algorithmic Trading Systems Developer Python Interactive Brokers Self - Christchurch, Canterbury 1996 to 2018 Developed a personal server cluster for backtesting 50 years of historical Chicago Mercantile Exchange Futures data for automated algorithmic trading systems. This allowed for backtesting trading strategies on Live Cattle, Lean Hogs and certain spread trading on the Soybean Crush.  Developed and scripted the creation of 1000 Cloud Servers using Digital Ocean, AWS, and Google Cloud Compute. This allowed for the backtesting of 60 years of data to find every possible 1-5 day patterns that lead to large movements in the underlying instruments.   Developed automated trading systems that are currently trading in the Forex and Equities Markets using Python, Linux Bash, and MariaDb all in the cloud.   Developed automated trading systems using Interactive Brokers API and ibPy python wrapper.  Developed automated trading systems using Oanda's API. This included JSON calls and data extraction methods, stop market, stop limit orders. BI Developer / Consultant Information Technologies Group Inc. - Anchorage, AK 2002 to 2007 Mapped specific accounting processes for British Petroleum (BP) to facilitate the consolidation of Philips Petroleum accounting processes in Dallas Texas from Anchorage Alaska.  Developed and scripted the creation of 500 cloud servers using Amazon AWS, Google Cloud Compute and Digital Ocean used for an affiliate marketing sales company. These processes created 500 separate and unique web carts using X-Cart that worked with Google Shopping. The result scaled a single webcart and website to 500 webcarts and websites for a larger market profile.  Conducted needs and requirements analyses with clients for GCI in Anchorage Alaska. I defined areas of opportunities for efficiency gains and conceived and implemented innovative solutions to document key personnel processes for faster training of new personnel.  Oversaw the elimination of ongoing labor costs for Petroleum Equipment Services, creating automation routines.  Analyzed historical data for City of Anchorage's Worker Training Program.  Managed a team of data entry clerks that collated all data and created reports. EDI Mapping Oracle Project Migration ETL Developer Houston Contracting - Anchorage, AK 2000 to 2002 Developed all ETL processes and procedures along with a custom EDI invoicing system. This involved extensive procedure writing in SQL and extensive mapping from Houston's databases to ARCO's Cost Expenditure Invoicing System.  Worked directly with Oracle Projects developers in Seattle to map and integrate Houston's legacy database financial software into Oracle Projects using Oracle's API. This involved extensive use of SQL and data mapping as well.  Managed all project deliverables, as Project Lead, for the upgrade of the legacy systems to Oracle.  Created custom reports for SQL queries, MS Access Reports and Crystal Reports used by ARCO's financial auditors and Houston's Controller. Education Bachelor's in Finance / Business University of Alaska, Anchorage - Anchorage, AK 1996 to 2002 Partial in MBA University of Alaska, Anchorage - Anchorage, AK 1996 to 1997 Skills Linux, Debian, SQL, ETL, EDI, SSIS, Data Migration, API, JSON, XML, Python, Algorithmic Trading, Data Simulation, MySQL, SQL Server, Bash, PHP, SSRS, BI, Google Big Query (1 year) Links http://linkedin.com/in/b4b67364 Certifications/Licenses NACE - National Association of Corrosion Engineers Underground Corrosion Control Using Cathodic Protection Devices