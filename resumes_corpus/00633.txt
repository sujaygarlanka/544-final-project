Datawarehouse Architect/ Lead Datawarehouse Consultant Datawarehouse Architect/ Lead Datawarehouse Consultant Datawarehouse Architect/ Lead Datawarehouse Consultant - Waste Management Houston, TX Over 8 years of experience in all phases of Analysis, Design, Development, Implementation and support Relational Data Warehousing applications for business requirements, Architecture functions  • Experience in full life cycle implementation of Data Warehouses and Data Marting  • Over 7 years of experience in Netezza database in all phases of Analysis, Design, Development, Implementation, Architecting and Administration  • Worked for the projects in the areas of Informatica, Data stage, SSIS, UNIX shell scripting, Netezza appliance, SQL-Server, COGNOS reporting, SQL-Gen - API  • Expertise in outstanding performance tuning of NZSQL, T-SQL statements using Explain Plan, SQL trace for optimizing the cost and evaluating the performance  • Experience in translating business requirements into creation of database objects: tables, indexes, constraints, packages, stored procedures, functions, and triggers using Oracle PL/SQL tool and Netezza Tool Kit  • Extensive experience in analysis and design of database, database modeling, ER Diagrams, normalization and de-normalization  • Performing Analysis for complex problems and solving the problems  • Having good domain knowledge based applications such as Health Care, Retail, Marketing, Transactional and Financial industries  • Experience in using versioning tools- SVN and GIT  • Created scripts which will send notifications/warning emails which helped in smooth running of daily batch jobs  • Worked extensively with Visio and ER Studio, Power Designer for Relational, Dimensional Modeling and Data Migration projects  • Capturing responses API GET and PUT calls using UNIX shell scripts (wget command line utility) and save the successful response in FTP sites and automate them  • Parsing out Json files using UNIX Shell Script (jq command line utility)  • Proficient with the SQL development languages  • Knowledge on scheduling (Autosys, CORN, DAC and Control M)  • Knowledge on API calls and usage of the calls from clients and products Work Experience Datawarehouse Architect/ Lead Datawarehouse Consultant Carlson Wagonlit Travels - Minnetonka, MN November 2017 to Present Responsibilities:  • Working on gathering different travel related data feeds (Air, Rail, car) for both Commercial and Government  • Implementing the multi tenancy architecture in Data lake and data plane's using queue's to gather data from multiple national and international travel agencies and clients source systems  • Working in Agile methodology (with thin slicing environment - 1 and 2 week sprints)  • Creating Travel segment and Travel accountable Data Model components using Visio  • Create Users and assign permissions using Informatica Admin console and maintaining the repositories and domains  • Capturing responses API GET and PUT calls using UNIX shell scripts (wget command line utility) and save the successful response in FTP sites and automate them  • Parsing out Json files using Unix Shell Script (jq command line utility) and loaded the data in to Mart queues for ISOS Alerts  • Closely working relation with external and international clients, to gather travel data for requirement and data distribution level agreements  • Working and implementing Multi-tenancy (Commercial, Local, International and Government) architecture  • Very good Knowledge of Informatica Power center for ETL, Data transformation and Developer client  • Design, develop and provide unit test data loading and data transformation programs related to Oracle Data Warehouses, SQL-server and Pivotal GreenPlum Data Marts  • Performed unit, performance and scalability testing at data, tool and DB level  • Resolved clients data incidents and request incidents at data and transformation level  • Involved in Weekly, Bi-Weekly and Monthly release activities to lower and upper environment  • Knowledge on using API interface and retrieve data from different API calls  • Created wrapper scripts for Informatica in UNIX shell script for scheduling purposes  • Architecture and Working knowledge on scheduling (Control-M, Autosys, Crontab and TMUX)    Environment: Oracle Exadata, TOAD, Json, XML, SQL Developer, SQL-Server 2016, Aginity Work Bench for GreenPlum, Informatica 9.6.1/10.1, Control_M, Postman, API's, UNIX (SunOS) Jr. Datawarehouse Architect/ Lead Datawarehouse Engineer Morningstar - Chicago, IL April 2015 to November 2017 Responsibilities:  • Design and develop data loading programs to support real time data loading to Netezza databases and Sql-Server Marts  • Worked on ETL(Informatica) jobs from data collection to data integration, mainly from source documents to Netezza and Netezza to Sql-Server Marts  • Working in Agile methodology (Fast phased environment)  • Worked on mapping document, gathering requirements and process flow diagrams  • Create database objects in lower and upper level Environments  • Create Users and assign permissions based on the level of database access the user would need  • Hands on experience in Amazon Web Services (console) and products  • Designed and implemented Cloud solutions with AWS Virtual private cloud (VPC), Elastic Compute Cloud (EC2), S3, Auto scaling, RDS and other AWS services  • Worked closely with subject matter expertise to gather and understand the calculations and requirements  • Created database objects in lower and upper level Environments (Netezza and SQL-Server)  • Worked on the investment data and calculated the different breakdowns, statistics, Total - Daily - Price - Primary Return Index's at different universes for portfolio and securities  • Worked on Multi-tenancy architecture  • Very good Knowledge of Informatica ETL tool ( Administrator, Designer and Monitor)  • Design, develop and provide unit test data loading and data transformation programs related to Netezza Data Warehouses and SQL-server Data Marts  • Performed unit testing, integration testing, regression testing, system testing and scalability  • Resolved issues/bug fixes during testing phase  • Performance tuned the transformations using NZ Explain Plan  • Involved in Bi-Weekly and Monthly release activities to lower and upper environment  • Knowledge on using API interface and retrieve data from different API calls  • Working knowledge on scheduling (Control-M)    Environment: Netezza 7.0.2 and 7.1.1, SQL-Server 2012, NZsql, Nzload, NZmigrate, T-SQL, Aginity WorkBench, Informatica 9.6.1, Control_M, Sql-Server Management tool, Postman, Morningstar-Direct v2 and Morningstar-Direct-v3 Lead Datawarehouse Engineer (Sr. Consultant) Fossil. INC - Richardson, TX August 2014 to April 2015 Responsibilities:  • Design, develop and provide unit test data loading and data transformation programs related to Netezza Data Warehouses and Data Marts  • Worked on ETL(Datastage) jobs from Source to Target  • Worked on thin slicing in Agile methodology  • Create databases, users and groups for Lower and upper level Environments  • Knowledge of Datastage ( Administrator client, Designer client and Director client)  • Performed unit testing, integration testing, regression testing and system testing.  • Resolved issues during testing phase  • Performance tuned the transformations using ASYNC and Finalization  • Maintained system documentation of best database procedures, practices and standards  Environment: Netezza 7.1, UNIX Shell Scripting, NZsql, Nzload, NZmigrate Netezza, Aginity Work Bench, nzDIF 3.9, Datastage 9.1 Netezza Database Administrator (DBA) Saint Petersburg, FL February 2014 to August 2014 Responsibilities:  • Prepared strategic plans for data warehousing projects and related quality documentation  • Assisted in designing and implementation of detailed data warehouse models and mappings  • Provided technical expertise to technologies in fields of data warehouse and relevant analytics  • Performed establishment and maintenance of databases for assigned projects as per business requirements  • Implemented procedures for daily database monitoring, backup and configuration changes  • Developed queries, backups, and session management and log management records in detailed manner  • Executed processes for management of migration of tables, schemas, database as per defined and assigned locations.  • Working in Agile methodology  • Create databases, users and groups for Lower and upper level Environments  • Proficient with UNIX/LINUX commands  • Suggested enhancement methods, rollout and upgrades for maintenance of existing databases  • Maintained system documentation of best database procedures, practices and standards  • Resolved database problems and issues by troubleshooting and responding to service requests    Environment: Netezza 7.0.4.X, Netezza 6.0.X.X, UNIX Shell Scripting, NZsql, Nzload, NZmigrate Netezza TwinFin 24, Netezza Mustang, Netezza N2001, Aginity Work Bench, WINSQL Lead Datawarehouse Engineer/Datawarehouse Engineer PremierInc Healthcare Solutions - Charlotte, NC February 2011 to February 2014 Premier Connect Enterprise - Claims (Carolina Health Partners)  Responsibilities:  • Worked on mapping document and gathering requirements  • Design and develop data loading programs to support real time data loading to Netezza Data Marts  • Worked on ETL(Datastage) jobs from Source to Target  • Worked on thin slicing in Agile methodology  • Create databases, users and groups for Lower and upper level Environments  • Worked loading claims data from the source(Informed) to the Dimensional Data Marts  • Worked on Multi-tenancy architecture  • Extensive knowledge of Bash and Korn scripting language  • Knowledge of Datastage ( Administrator client, Designer client and Director client)  • Design, develop and provide unit test data loading and data transformation programs related to Netezza Data Warehouses and Data Marts  • Worked on creating reports in Cognos  • Extensive knowledge on scheduling (Autosys)    Environment: Netezza 7, UNIX Shell Scripting, NZSQL, SQL, Netezza TwinFin 24, Aginity Work Bench, Datastage 9.2, Cognos 10.1.    Premier Connect Enterprise - Clinical (Carolina Health Partners)  Responsibilities:  • Design, develop and provide unit test data loading and data transformation programs related to Netezza Data Warehouses and Data Marts  • Design Trickle feed data loading programs to support near real time data loading of Netezza Data Marts  • Designs data transformations and exceptions processing in addition to refresh and replay data loading capabilities  • Using SSIS, building high performance data integration solutions, including extraction, transformation, and load (ETL) packages for data bases and Scheduling the SSIS packages and Jobs  • Development coding and unit tests the data loading and transformation programs  • Codes orchestration programs that schedule the data loading and data transformations routines  • Unit tests all data loading and transformation programs  • Extensive knowledge of the nzDIF Data Integration Framework  • Extensive knowledge of Bash shell scripting language  • Extensive knowledge on scheduling (Autosys)    Environment: Netezza 6.x & 7, UNIX Shell Scripting, NZSQL, sql, Netezza TwinFin 24, Brightlight Framework 3.8 & 3.9, Aginity Work Bench, AQT, Silktest tool, SSIS - 2012, SQL-Server 2012, T-SQL    IPP/SYMPONY/SADVISOR  Responsibilities:  • Worked extensively on Brightlight Framework 3.5, 3.8.  • Used different Framework assets like nzf, app_init.  • Worked on Application asserts like setup_cfg.sh, run, log, ddl, meta, data, auto.  • Involved in admin workflows like db_create, db_init.  • Involved in dm workflows like ddl_convert, ora_ddl_convert and db_install.  • Involved in dev workflows like db_intake_config, db_xfr_templates, db_referential_config.  • Worked extensively on Aginity Work Bench and AQT.  • Written transformations using shell scripting mainly bash.  • Performed unit testing, integration testing, regression testing and system testing.  • Resolved issues during testing phase.  • Performance tuned the transformations using ASYNC and Finalization.  • Created list files, flow files and shell scripts (.sh).  • Created data files for test cases using Silktest tool and creating manifest files.  • Performed load test on transformations by using generated test data.  • Worked on Cognos views by using the table sql.  • Worked on the mapping documents.  • Assisted QA in testing the QA test cases and resolving the issues.  • Worked on updating the data model changes.  • Worked on the triggers and RDM joins.  • Wrote DIFF script for data model changes to find the difference between two scripts.  • Worked in Agile Environment.  Environment: Netezza 6.0.5, UNIX Shell Scripting, MS Excel, NZSQL, Netezza TwinFin 24, Brightlight Framework 3.5, 3.8, NZF, Aginity Work Bench, AQT, Silktest tool, triggers and RDM joins.    Physician Focus Project  Responsibilities:    • Worked extensively on SQL, PL/SQL, Perl and UNIX shell scripting  • Analyzed the source system of ETL Maps  • Developed the ETL routines  • Designed and developed the fact/dimension entities  • Involved in the Unit testing, Event & Thread testing and System testing of the individual  • Written Unix Shell Scripting based on requirements  • Worked in Agile environment  • Migration of data from Oracle to Netezza  • Worked on Brightlight Framework 3.0  • Worked extensively in NZSQL to migrate the data from Oracle to Netezza database  • Written Unix Shell Scripting based on requirements  • Performed unit testing of the shell scripts developed with various scenarios  • Performed integration testing and system testing  Environment: Netezza, UNIX Shell Scripting, MS Excel, NZSQL, Netezza TwinFin 12, Brightlight Framework 3.0 Education Master in Mechanical Engineering Sciences Lamar University - Beaumont, TX 2010 Bachelors in Mechanical Engineering B-Tech 2008 Jawaharlal Nehru Technological University - Hyderabad, Telangana Skills DATABASE (6 years), DATABASES (6 years), ETL (6 years), EXTRACT, TRANSFORM, AND LOAD (6 years), SCHEDULING (5 years) Additional Information Technical Skills  ETL Informatica 8.6/9.6.1/10.1, DATA STAGE 9.1  Scheduling Tool DAC (Data Warehousing Admin Console), Control_M, Autosys, Crontab and TMUX  Reporting Tool COGNOS 9.X, 10.X  Databases Netezza 4.5.1, 6.0.3, 6.0.5, 7.0.X, 7.1.X (NPS), Oracle 11g/10g/9i/8i, Oracle Exadata    Operating Systems Windows 95/98/2000/2003/7/10, UNIX and LINUX  Programming C, SQL, PL/SQL, XML, Json  Other Tools  SQL*Loader, Oracle SQL Developer, TOAD, Eclipse, MS Visio, MS Excel, Eclipse Platform, API-Postman, Aginity Workbench, SQL server studio    Scripting Languages UNIX Shell Scripting - bash, ksh, SQL, PL/SQL, Postgres and T-SQL    Methodologies E-R Modeling, Star Schema, Snowflake Schema, Hybrid Models - Data warehousing  Data Modeling Tool Microsoft Visio 2010/2016, ER Studio