{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sujay/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    DPRContextEncoder,\n",
    "    DPRQuestionEncoder,\n",
    "    DPRContextEncoderTokenizer,\n",
    "    DPRQuestionEncoderTokenizer,\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/test_data.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "comparison_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_correct = 0\n",
    "total_num_pos = 0\n",
    "labels = []\n",
    "accuracies = []\n",
    "\n",
    "for data in test_data:\n",
    "    label = data[\"label\"]\n",
    "    job_description = data[\"description\"]\n",
    "    pos = data[\"pos\"]\n",
    "    neg = data[\"neg\"]\n",
    "    all = pos + neg\n",
    "    random.shuffle(all) \n",
    "    tokenized_all = [doc.split(\" \") for doc in all]\n",
    "    bm25 = BM25Okapi(tokenized_all)\n",
    "\n",
    "    tokenized_query = job_description.split(\" \")\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "    _, indices = torch.topk(torch.from_numpy(scores), 5)\n",
    "    relevant_passages = np.array(all)[indices]\n",
    "    num_correct = 0\n",
    "    for p in relevant_passages:\n",
    "        if p in pos:\n",
    "            num_correct += 1\n",
    "    total_num_correct += num_correct\n",
    "    total_num_pos += 5\n",
    "    labels.append(label)\n",
    "    accuracies.append(num_correct)\n",
    "    print(f\"Accuracy ({label}): {num_correct}/{len(relevant_passages)}\")\n",
    "\n",
    "labels.append(\"Total\")\n",
    "accuracies.append(total_num_correct)\n",
    "comparison_data[\"Label\"] = labels\n",
    "comparison_data[\"BM25\"] = accuracies\n",
    "print(f\"Total accuracy: {total_num_correct}/{total_num_pos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\n",
    "    \"facebook/dpr-question_encoder-single-nq-base\"\n",
    ")\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\n",
    "    \"facebook/dpr-ctx_encoder-single-nq-base\"\n",
    ")\n",
    "\n",
    "def encode(tokenizer, encoder, text):\n",
    "    tokenized_output = tokenizer(\n",
    "        text, return_tensors=\"pt\", padding=\"max_length\", max_length=512, truncation=True\n",
    "    )\n",
    "    input_ids = tokenized_output[\"input_ids\"]\n",
    "    attention_mask = tokenized_output[\"attention_mask\"]\n",
    "\n",
    "    return encoder(input_ids.to(device), attention_mask.to(device)).pooler_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPR Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "question_encoder = DPRQuestionEncoder.from_pretrained(\n",
    "    \"facebook/dpr-question_encoder-single-nq-base\"\n",
    ").to(device)\n",
    "context_encoder = DPRContextEncoder.from_pretrained(\n",
    "    \"facebook/dpr-ctx_encoder-single-nq-base\"\n",
    ").to(device)\n",
    "\n",
    "total_num_correct = 0\n",
    "total_num_pos = 0\n",
    "accuracies = []\n",
    "\n",
    "for data in test_data:\n",
    "    label = data[\"label\"]\n",
    "    job_description = data[\"description\"]\n",
    "    pos = data[\"pos\"]\n",
    "    neg = data[\"neg\"]\n",
    "    all = pos + neg\n",
    "    random.shuffle(all)\n",
    "\n",
    "    # Encode the question and the context\n",
    "    question_output = encode(question_tokenizer, question_encoder, job_description)\n",
    "    context_output = encode(context_tokenizer, context_encoder, all)\n",
    "    scores = F.cosine_similarity(question_output, context_output)\n",
    "    _, indices = torch.topk(scores, 5)\n",
    "    relevant_passages = np.array(all)[indices.cpu().numpy()]\n",
    "    num_correct = 0\n",
    "    for p in relevant_passages:\n",
    "        if p in pos:\n",
    "            num_correct += 1\n",
    "    total_num_correct += num_correct\n",
    "    total_num_pos += 5\n",
    "\n",
    "    accuracies.append(num_correct)\n",
    "    print(f\"Accuracy ({label}): {num_correct}/{len(relevant_passages)}\")\n",
    "accuracies.append(total_num_correct)\n",
    "comparison_data[\"DPR No Finetuning\"] = accuracies\n",
    "print(f\"Total accuracy: {total_num_correct}/{total_num_pos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPR Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Security_Analyst): 3/5\n",
      "Accuracy (Systems_Administrator): 3/5\n",
      "Accuracy (Project_manager): 3/5\n",
      "Accuracy (Database_Administrator): 5/5\n",
      "Accuracy (Software_Developer): 4/5\n",
      "Accuracy (Front_End_Developer): 2/5\n",
      "Accuracy (Web_Developer): 2/5\n",
      "Accuracy (Java_Developer): 3/5\n",
      "Accuracy (Network_Administrator): 3/5\n",
      "Accuracy (Python_Developer): 2/5\n",
      "Accuracy (Security_Analyst): 3/5\n",
      "Accuracy (Systems_Administrator): 2/5\n",
      "Accuracy (Project_manager): 3/5\n",
      "Accuracy (Database_Administrator): 4/5\n",
      "Accuracy (Software_Developer): 3/5\n",
      "Accuracy (Front_End_Developer): 5/5\n",
      "Accuracy (Web_Developer): 3/5\n",
      "Accuracy (Java_Developer): 2/5\n",
      "Accuracy (Network_Administrator): 3/5\n",
      "Accuracy (Python_Developer): 4/5\n",
      "Accuracy (Security_Analyst): 4/5\n",
      "Accuracy (Systems_Administrator): 2/5\n",
      "Accuracy (Project_manager): 3/5\n",
      "Accuracy (Database_Administrator): 1/5\n",
      "Accuracy (Software_Developer): 4/5\n",
      "Accuracy (Front_End_Developer): 2/5\n",
      "Accuracy (Web_Developer): 3/5\n",
      "Accuracy (Java_Developer): 3/5\n",
      "Accuracy (Network_Administrator): 4/5\n",
      "Accuracy (Python_Developer): 4/5\n",
      "Accuracy (Security_Analyst): 5/5\n",
      "Accuracy (Systems_Administrator): 4/5\n",
      "Accuracy (Project_manager): 4/5\n",
      "Accuracy (Database_Administrator): 4/5\n",
      "Accuracy (Software_Developer): 4/5\n",
      "Accuracy (Front_End_Developer): 3/5\n",
      "Accuracy (Web_Developer): 2/5\n",
      "Accuracy (Java_Developer): 4/5\n",
      "Accuracy (Network_Administrator): 4/5\n",
      "Accuracy (Python_Developer): 3/5\n",
      "Accuracy (Security_Analyst): 2/5\n",
      "Accuracy (Systems_Administrator): 4/5\n",
      "Accuracy (Project_manager): 3/5\n",
      "Accuracy (Database_Administrator): 3/5\n",
      "Accuracy (Software_Developer): 4/5\n",
      "Accuracy (Front_End_Developer): 4/5\n",
      "Accuracy (Web_Developer): 3/5\n",
      "Accuracy (Java_Developer): 3/5\n",
      "Accuracy (Network_Administrator): 5/5\n",
      "Accuracy (Python_Developer): 4/5\n",
      "Accuracy (Security_Analyst): 3/5\n",
      "Accuracy (Systems_Administrator): 4/5\n",
      "Accuracy (Project_manager): 2/5\n",
      "Accuracy (Database_Administrator): 3/5\n",
      "Accuracy (Software_Developer): 3/5\n",
      "Accuracy (Front_End_Developer): 3/5\n",
      "Accuracy (Web_Developer): 4/5\n",
      "Accuracy (Java_Developer): 4/5\n",
      "Accuracy (Network_Administrator): 5/5\n",
      "Accuracy (Python_Developer): 4/5\n",
      "Accuracy (Security_Analyst): 4/5\n",
      "Accuracy (Systems_Administrator): 4/5\n",
      "Accuracy (Project_manager): 2/5\n",
      "Accuracy (Database_Administrator): 4/5\n",
      "Accuracy (Software_Developer): 3/5\n",
      "Accuracy (Front_End_Developer): 3/5\n",
      "Accuracy (Web_Developer): 3/5\n",
      "Accuracy (Java_Developer): 4/5\n",
      "Accuracy (Network_Administrator): 3/5\n",
      "Accuracy (Python_Developer): 2/5\n",
      "Accuracy (Security_Analyst): 4/5\n",
      "Accuracy (Systems_Administrator): 3/5\n",
      "Accuracy (Project_manager): 3/5\n",
      "Accuracy (Database_Administrator): 1/5\n",
      "Accuracy (Software_Developer): 4/5\n",
      "Accuracy (Front_End_Developer): 3/5\n",
      "Accuracy (Web_Developer): 4/5\n",
      "Accuracy (Java_Developer): 3/5\n",
      "Accuracy (Network_Administrator): 5/5\n",
      "Accuracy (Python_Developer): 4/5\n",
      "Accuracy (Security_Analyst): 2/5\n",
      "Accuracy (Systems_Administrator): 5/5\n",
      "Accuracy (Project_manager): 2/5\n",
      "Accuracy (Database_Administrator): 4/5\n",
      "Accuracy (Software_Developer): 3/5\n",
      "Accuracy (Front_End_Developer): 4/5\n",
      "Accuracy (Web_Developer): 4/5\n",
      "Accuracy (Java_Developer): 4/5\n",
      "Accuracy (Network_Administrator): 4/5\n",
      "Accuracy (Python_Developer): 5/5\n",
      "Accuracy (Security_Analyst): 3/5\n",
      "Accuracy (Systems_Administrator): 2/5\n",
      "Accuracy (Project_manager): 2/5\n",
      "Accuracy (Database_Administrator): 4/5\n",
      "Accuracy (Software_Developer): 3/5\n",
      "Accuracy (Front_End_Developer): 4/5\n",
      "Accuracy (Web_Developer): 2/5\n",
      "Accuracy (Java_Developer): 2/5\n",
      "Accuracy (Network_Administrator): 4/5\n",
      "Accuracy (Python_Developer): 2/5\n",
      "Total accuracy: 330/500\n"
     ]
    }
   ],
   "source": [
    "question_encoder = DPRQuestionEncoder.from_pretrained(\n",
    "    \"/home/sujay/Code/544-final-project/models/finetune_question_encoder\"\n",
    ").to(device)\n",
    "context_encoder = DPRContextEncoder.from_pretrained(\n",
    "    \"/home/sujay/Code/544-final-project/models/finetune_context_encoder\"\n",
    ").to(device)\n",
    "\n",
    "total_num_correct = 0\n",
    "total_num_pos = 0\n",
    "accuracies = []\n",
    "\n",
    "for data in test_data:\n",
    "    label = data[\"label\"]\n",
    "    job_description = data[\"description\"]\n",
    "    pos = data[\"pos\"]\n",
    "    neg = data[\"neg\"]\n",
    "    all = neg + pos\n",
    "    # random.shuffle(all)\n",
    "\n",
    "    question_output = encode(question_tokenizer, question_encoder, job_description)\n",
    "    context_output = encode(context_tokenizer, context_encoder, all)\n",
    "    scores = F.cosine_similarity(question_output, context_output)\n",
    "    _, indices = torch.topk(scores, 5)\n",
    "    relevant_passages = np.array(all)[indices.cpu().numpy()]\n",
    "    num_correct = 0\n",
    "    for p in relevant_passages:\n",
    "        if p in pos:\n",
    "            num_correct += 1\n",
    "    total_num_correct += num_correct\n",
    "    total_num_pos += 5\n",
    "    accuracies.append(num_correct)\n",
    "    print(f\"Accuracy ({label}): {num_correct}/{len(relevant_passages)}\")\n",
    "\n",
    "accuracies.append(total_num_correct)\n",
    "comparison_data[\"DPR Finetuning\"] = accuracies\n",
    "print(f\"Total accuracy: {total_num_correct}/{total_num_pos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "comparison_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clear gpu\n",
    "# def clear_memory():\n",
    "#     import gc\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "    \n",
    "# del question_tokenizer\n",
    "# del context_tokenizer\n",
    "# del question_encoder\n",
    "# del context_encoder\n",
    "# clear_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
