{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/sujay/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"resume\": []}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through a folder of text files and extract the text\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "# Define the directory\n",
    "dir_path = \"./resumes_corpus\"\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "files = os.listdir(dir_path)\n",
    "\n",
    "# Iterate over each file\n",
    "for file in files:\n",
    "    if file.endswith('.txt'):\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(dir_path, file)\n",
    "        # Open the file\n",
    "        with codecs.open(file_path, \"r\", encoding=\"utf8\", errors='ignore') as f:\n",
    "            # Read the file's contents\n",
    "            content = f.read()\n",
    "\n",
    "            new_row_df = pd.DataFrame(\n",
    "                {\"resume\": [content]}\n",
    "            )\n",
    "\n",
    "            # Add the new row to the DataFrame\n",
    "            df = pd.concat([df, new_row_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(row):\n",
    "    resume = row[\"resume\"]\n",
    "    # Lower case\n",
    "    resume = resume.lower()\n",
    "    # Remove html tags\n",
    "    resume = re.sub(r\"<.*?>\", \"\", resume)\n",
    "    # Remove urls\n",
    "    resume = re.sub(r\"http[s]?://\\S+\", \"\", resume)\n",
    "    # Remove non alphanumeric characters\n",
    "    resume = re.sub(r\"[^a-zA-Z\\s]\", \"\", resume)\n",
    "    # Remove extra whitespaces\n",
    "    resume = re.sub(r\"[\\s]+\", \" \", resume)\n",
    "    # Remove contractions\n",
    "    resume = contractions.fix(resume)\n",
    "\n",
    "    return resume\n",
    "\n",
    "cleaned_resume_df = df.copy(deep=True)\n",
    "cleaned_resume_df[\"resume\"] = cleaned_resume_df.apply(cleaning, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim import utils, matutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(w1, w2):\n",
    "    return np.dot(matutils.unitvec(w1), matutils.unitvec(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        for r in cleaned_resume_df[\"resume\"]:\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield r.split()\n",
    "\n",
    "\n",
    "resumes = MyCorpus()\n",
    "resume_wv_model = gensim.models.Word2Vec(\n",
    "    sentences=resumes, min_count=10, vector_size=300, window=11\n",
    ")\n",
    "resume_wv = resume_wv_model.wv\n",
    "google_wv = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pythons', 0.66883784532547), ('Burmese_python', 0.6680365800857544), ('snake', 0.6606292724609375), ('crocodile', 0.6591362953186035), ('boa_constrictor', 0.6443520188331604)]\n",
      "[('pythondjango', 0.49119943380355835), ('pandas', 0.48099473118782043), ('usingpython', 0.43581053614616394), ('pythons', 0.43535345792770386), ('jinja', 0.418903112411499)]\n"
     ]
    }
   ],
   "source": [
    "print(google_wv.most_similar(positive=[\"python\"], topn=5))\n",
    "print(resume_wv.most_similar(positive=[\"python\"], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33437246\n",
      "0.32137614\n"
     ]
    }
   ],
   "source": [
    "print(similarity_score(google_wv[\"javascript\"], google_wv[\"java\"]))\n",
    "print(similarity_score(resume_wv[\"javascript\"], resume_wv[\"java\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "544-final-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
