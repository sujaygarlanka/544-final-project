Hadoop Developer Hadoop <span class="hl">Developer</span> Hadoop Developer - Dow Chemical's Pasadena, CA • Eight years of experience with emphasis on Big Data Technologies, Design and Development of Java based enterprise applications.  • Three years of experience in Hadoop Development and five years of java application development.  • Experience in installation, configuration, supporting and managing Hadoop clusters.  • Up to date on evaluating new analytical tools and projects coming up big data space like Apache Spark and Apache Shark, Datameer, Platfora etc.  • Implemented in setting up standards and processes for Hadoop based application design and implementation.  • Responsible for writing MapReduce programs.  • Logical Implementation and interaction with HBase.  • Developed MapReduce jobs to automate transfer of data from HBase.  • Perform data analysis using Hive and Pig.  • Load log data into HDFS using Flume.  • Gained good knowledge on creating strategies on risky transactions.  • Assist with the addition of Hadoop processing to the IT infrastructure.  • Support development, testing, and operations teams during new system deployments.  • Evaluate and propose new tools and technologies to meet the needs of the organization.  • Experience in using Scoop, ZooKeeper and Cloudera Manager.  • Good Knowledge on Hadoop Cluster architecture and monitoring the cluster.  • 24/7 operational support to production servers and related infrastructure clusters.  • Experience in Administering, Performance Monitoring and Fine-tuning of Linux Redhat.  • Worked on debugging tools such as Dtrace, Struss and Top. Expert in setting up SSH, SCP, SFTP connectivity between UNIX hosts. Authorized to work in the US for any employer Work Experience Hadoop Developer Dow Chemical's - Philadelphia, PA May 2018 to Present Philadelphia is the home for Dow's Advanced Materials Division, which is the umbrella for some of the company's most important high-growth specialty businesses. Five businesses make up the division - Coatings, Building and Construction, Paper and Textiles, Specialty Packaging and Films and Separations Technologies, and Electronic Materials - that serves the fastest-growing end markets such as water, electronics, food, pharmaceuticals, health care, paints and more.    Responsibilities:  • Involved in review of functional and non-functional requirements.  • Facilitated knowledge transfer sessions.  • Importing and exporting data into HDFS and Hive using Sqoop.  • Experienced in defining job flows.  • Experienced in managing and reviewing Hadoop log files.  • Extracted files from CouchDB through Sqoop and placed in HDFS and processed.  • Experienced in running Hadoop streaming jobs to process terabytes of xml format data.  • Load and transform large sets of structured, semi structured and unstructured data.  • Responsible to manage data coming from different sources.  • Got good experience with NOSQL database.  • Supported Map Reduce Programs those are running on the cluster.  • Involved in loading data from UNIX file system to HDFS.  • Installed and configured Hive and also written Hive UDFs.  • Involved in creating Hive tables, loading with data and writing hive queries which will run internally in map reduce way.  • Gained very good business knowledge on health insurance, claim processing, fraud suspect identification, appeals process etc.  • Developed a custom File System plug in for Hadoop so it can access files on Data Platform.  • This plugin allows Hadoop MapReduce programs, HBase, Pig and Hive to work unmodified and access files directly.  • Designed and implemented Mapreduce-based large-scale parallel relation-learning system  • Extracted feeds form social media sites such as Facebook, Twitter using Python scripts.  • Setup and benchmarked Hadoop/HBase clusters for internal use  • Setup Hadoop cluster on Amazon EC2 using whirr for POC.    Environment: Java, Eclipse, Oracle, Sub Version, Hadoop, Hive, HBase, Linux, MapReduce, HDFS, Hive, Java, Hadoop Distribution of HortonWorks, MapReduce, DataStax, IBM DataStage 8.1, Oracle, PL/SQL, SQL*PLUS, Toad, Windows NT, UNIX Shell Scripting. Hadoop Developer Kraft Foods - Chicago, IL November 2016 to April 2018 Kraft Foods, Inc. has approximately 140, 000 diverse employees around the world and is mainly involved in creating food products all around United States    Responsibilities:  • Worked on analyzing Hadoop cluster using different big data analytic tools including Pig, Hive, and MapReduce  • Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis  • Implemented nine nodes CDH3 Hadoop cluster on Red hat LINUX.  • Involved in loading data from LINUX file system to HDFS.  • Created HBase tables to store variable data formats of PII data coming from different portfolios.  • Implemented a script to transmit sysprin information from Oracle to Hbase using Sqoop.  • Implemented best income logic using Pig scripts and UDFs.  • Worked on debugging, performance tuning of Hive & Pig Jobs  • Created Hbase tables to store various data formats of PII data coming from different portfolios  • Implemented test scripts to support test driven development and continuous integration  • Worked on tuning the performance Pig queries  • Involved in loading data from LINUX file system to HDFS  • Importing and exporting data into HDFS and Hive using Sqoop  • Experience working on processing unstructured data using Pig and Hive  • Supported MapReduce Programs those are running on the cluster  • Gained experience in managing and reviewing Hadoop log files  • Involved in scheduling Oozie workflow engine to run multiple Hive and pig jobs  • Created and maintained Technical documentation for launching HADOOP Clusters and for executing Hive queries and Pig Scripts    Environment: Hadoop, HDFS, Pig, Hive, MapReduce, Sqoop, Oozie, Cloudera, LINUX, and Big Data Hadoop and Java Developer Sheffield Financial - Salem, Tamil Nadu February 2015 to October 2016 Sheffield Financial offers the simplest, quickest, and easiest retail finance program. Sheffield Financial finances major brand equipment like Honda and Nissan. Sheffield is headquartered in North Carolina and has an extensive independent dealer base throughout the U.S. Dealer suite is a web application that helps dealers to register and enter their selling products. Borrowers can enter credit applications and equipment's to which they want to take the finance    Responsibilities:  • Worked with several clients with day to day requests and responsibilities.  • Installed/Configured/Maintained Apache Hadoop clusters for application development and Hadoop tools like Hive, Pig, HBase, Zookeeper and Sqoop.  • Involved in analyzing system failures, identifying root causes and recommended course of actions.  • Worked on Hive for exposing data for further analysis and for generating transforming files from different analytical formats to text files.  • Wrote the shell scripts to monitor the health check of Hadoop daemon services and respond accordingly to any warning or failure conditions.  • Managing and scheduling Jobs on a Hadoop cluster.  • Implemented and maintained various projects in Java.  • Utilized Java and MySQL from day to day to debug and fix issues with client processes.  • Developed, tested, and implemented financial-services application to bring multiple clients into standard database format.  • Assisted in designing, building, and maintaining database to analyze life cycle of checking and debit transactions.  • Excellent JAVA, J2EE application development skills with strong experience in Object Oriented Analysis, Extensively involved throughout Software Development Life Cycle (SDLC)  • Strong experience of J2SE, XML, Web Services, WSDL, SOAP, UDDI, TCP, IP.  • Strong experience of software and system development using JSP, Servlet, Java Server Face, EJB, JDBC, JNDI, Struts, Maven, Trac, Subversion, JUnit, SQL language.  • Rich experience of database design and hands-on experience of large database systems: Oracle 8i and Oracle 9i, DB2, PL, SQL.  • Hands-on experience of Sun One Application Server, Web logic Application Server, Web Sphere Application Server, Web Sphere Portal Server, and J2EE application deployment technology.  Environment: Hive, Pig, HBase, Zookeeper, Sqoop, Cloudera, Java, JDBC, JNDI, Struts, Maven, Trac, Subversion, JUnit, SQL language, spring, Hibernate, Junit, Oracle, XML, Altova XmlSpy, Putty and Eclipse. Java/JEE Architect/ developer Pfizer Global Research & Development - New York, NY January 2013 to January 2015 Pfizer, Inc. is an American multinational pharmaceutical corporation headquartered in New York City and with its research headquarters in Groton, Connecticut, United States. It is one of the world's largest pharmaceutical company by revenues.Pfizer develops and produces medicines and vaccines for a wide range of conditions including in the areas of immunology and inflammation, oncology, cardiovascular and metabolic diseases, neuroscience and pain.    Responsibilities:  • Architected a JSF, Web sphere, Oracle, spring, and Hibernate based 24x7 Web application.  • Built an end to end vertical slice for a JEE based billing application using popular frameworks like Spring, Hibernate, JSF, Facelets, XHTML, Maven2, and Ajax by applying OO design concepts, JEE & GoF design patterns, and best practices.  • Integrated other sub-systems like loans application, equity markets online application system, and documentation system with the structured products application through JMS, Websphere MQ, SOAP based Web services, and XML.  • Designed the logical and physical data model, generated DDL scripts, and wrote DML scripts for Oracle 9i database.  • Tuned SQL statements, Hibernate mapping, and Websphere application server to improve performance, and consequently met the SLAs.  • Gathered business requirements and wrote functional specifications and detailed design documents.  • Improved the build process by migrating it from Ant to Maven2.  • Built and deployed Java applications into multiple Unix based environments and produced both unit and functional test results along with release notes.    Environment: Java 1.5, JSF Sun RI, Facelets, Ajax4JSF, Richfaces, Spring, XML, XSL, XSD, XHTML, Hibernate, Oracle 9i, PL/SQL, MINA, Spring-ws, SOAP Web service, Websphere, Oracle, JMX, ANT, Maven2, Continuum, JUnit, SVN, TDD, and XP. Java developer Apollo Healthcare - Hyderabad, Telangana September 2011 to December 2012 The Apollo PathLinks system is a patient-centered, process-oriented, Web-host or self-host information system that allows you to streamline your business processes by choosing from integrated modules that facilitate clinical, financial and enterprise management. You can start with a few applications and add others as requirement or implement the entire system for a total solution.  Responsibilities:  • Developed Admission & Census module, which monitors a wide range of detailed information for each resident upon pre-admission or admission to your facility.  • Involved in development of Care Plans module, which provides a comprehensive library of problems, goals and approaches. You have the option of tailoring (adding, deleting, or editing problems, goals and approaches) these libraries and the disciplines you will use for your care plans.  • Involved in development of General Ledger module, which streamlines analysis, reporting and recording of accounting information. General Ledger automatically integrates with a powerful spreadsheet solution for budgeting, comparative analysis and tracking facility information for flexible reporting.  • Developed UI using HTML, JavaScript, and JSP, and developed Business Logic and Interfacing components using Business Objects, XML, and JDBC.  • Designed user-interface and checking validations using JavaScript.  • Managed connectivity using JDBC for querying/inserting & data management including triggers and stored procedures.  • Developed various EJBs for handling business logic and data manipulations from database.  • Involved in design of JSP's and Servlets for navigation among the modules.  • Designed cascading style sheets and XML part of Order entry Module & Product Search Module and did client side validations with java script.    Environment: J2EE, Java/JDK, JDBC, JSP, Servlets, JavaScript, EJB, JNDI, JavaBeans, XML, XSLT, Oracle 9i, Eclipse, HTML/ DHTML, SVN. Education Bachelor's Skills ORACLE (7 years), JAVA (6 years), XML (6 years), SQL (5 years), APACHE HADOOP SQOOP (4 years) Additional Information Technical Skills:    • Programming Languages: Java, C++, SQL, PIG, PL /SQL.  • Java Technologies: Java, JAXP, AJAX, JFC Swing, Log4j, Java Help API  • J2EE Technologies: JSP Servlets, JDBC, JNDI, XML, JAXP, Java Beans    • Methodologies: Agile, UML, Design Patterns (Core & J2EE)  • Frame Works: Jakarta Struts, JUnit and JTest, LDAP.  • Databases: Oracle, NO SQL (HBase), MY SQL, MS SQL server.  • Application Server: Apache Tomcat, Jboss  • IDE's & Utilities: Eclipse and JCreator, NetBeans.  • Web Dev. Technologies: HTML, Java Script, XML, DTD, XSL, XSLT, XPath, DOM, XQuery.  • Protocols: TCP/IP, HTTP and HTTPS.    • Operating Systems: Linux, MacOS, WINDOWS.  • Hadoop ecosystem: Hadoop and MapReduce, Spark, Sqoop, Hive, PIG, HBASE, HDFS,  Flume, Hue, Zookeeper, Lucene, Sun Grid Engine Administration.