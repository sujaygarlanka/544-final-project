Hadoop Admin Hadoop Admin Hadoop Administrator - NCR Corp • Over 7 years of professional IT experience which includes experience in Big Data ecosystem related technologies.  • 3+ years of experience in Hadoop Administration.  • Expertise in Big Data technologies and Hadoop ecosystem: HDFS, Job Tracker, Task Tracker, NameNode, Data Node andMapReduceprogramming paradigm.  • Hands on experience in installing, configuring, and using Hadoop ecosystem components like Hadoop MapReduce, HDFS, HBase, Oozie, Hive, Sqoop, Pig, Zookeeper and Flume.  • Hands on experience with Apache Hadoop Map Reduce programming, PIG Scripting and Distribute Application and HDFS.  • Hands on experience in installation, configuration, management and deployment of Big Data solutions and the underlying infrastructure of Hadoop Cluster using Cloudera, Horton works distributions and Map.  • Excellent understanding of Hadoop Cluster architecture and monitoring the cluster.  • Experience in managing and reviewing Hadoop log files.  • Experience in NoSQLdatabase Monod and Cassandra.  • Hands on experience importing and exporting data using Sqoop from HDFS to Relational Database Systems.  • Over two years of experience in design, development, and maintenance and support of Big Data Analytics using Hadoop Ecosystem components like HDFS, Hive, Pig, HBase, Sqoop, Flume, Zookeeper, MapReduce, and Oozie.  • Managing the health of Cluster, resolving performance related issues, coordinating with various parties for Infrastructure support.  • Update Hadoop 1.2.1 to 2.5.2  • Experience in Talend Big Data Studio 6.0.  • Push data as delimited files into HDFS using Talend Big Data studio.  • Expertise in Commissioning and decommissioning the nodes in Hadoop Cluster using Cloudera Manager Enterprise.  • Experience on Oracle OBIEE.  • Setting up HDFS Quotas to enforce the fair share of computing resources.  • Experience in Rebalance an HDFS Cluster.  • Hadoop Cluster capacity planning, performance tuning, cluster Monitoring, Troubleshooting.  • Hands on experience in analyzing Log files for Hadoop and eco system services and finding root cause.  • Expertise in benchmarking, performing backup and disaster recovery of Name Node metadata and important and sensitive data residing on cluster.  • Rack aware configuration for quick availability and processing of data.  • Experience in designing and implementing of secure Hadoop cluster using Kerberos.  • Successfully loaded files to Hive and HDFS from Oracle , SQL Server, MySQL, and Teradata using Sqoop.  • Loaded streaming log data from various web servers into HDFS using Flume.  • Created Hive internal and external tables defined with appropriate static and dynamic partitions.  • Experience in Creating and managing HBase clusters dynamically using Slider and Start & Stop HBase clusters running on Slider.  • Strong Knowledge on Spark concepts like RDD Operations, Caching and Persistence.  • Experience in Upgrading Apache Ambary, CDH and HDP Cluster.  • Extensive knowledge in using job scheduling by Oozie and Centralized Service Zookeeper.  • Expertise in Collaborating across Multiple technology groups and getting things done.  • Worked on both traditional Waterfall model and Agile methodology, Sound knowledge of Data warehousing concepts.  • Worked on Oracle , Teradata and Vertica database systems with Good experience in UNIX Shell scripting.  • Experience in modeling with both OLTP/OLAP systems and Kimball and Inmon Data Warehousingenvironments.  • Experience in extracting data from both Relational systems and Flat Files.  • Analysis and development of mappings using the transformations in Informatica.  • Handsome experience in Linux admin activities.  • Worked in a 24x7 on-call Production Support Environment. Work Experience Hadoop Admin Tracfone Wireless - Miami, FL May 2011 to August 2012 Description: TracFone Wireless is known for its coverage in 99.6% of the cellular populations. They are contracted with the major carriers to bring you the same level of coverage by providing connectivity through the same cellular towers.    Responsibilities:  • Installed and configured Flume, Hive, Pig, Sqoop and Oozie on the Hadoop cluster.  • Launching and Setup of HADOOP/ HBASE Cluster, which includes configuring different components of HADOOP and HBASE Cluster.  • Experienced in loading data from the UNIX file system to HDFS.  • Created HBase tables to load large sets of structured, semi-structured and unstructured data coming from UNIX, NoSQL and a variety of portfolios.  • Worked on writing transformer/mapping Map-Reduce pipelines using Java.  • Involved in creating Hive tables, loading them with data and writing Hive queries that will run internally in Map Reduce way.  • Migrated ETL jobs to Pig scripts do Transformations, even joins and some pre-aggregations before storing the data onto HDFS  • Worked on different file formats like Sequence files, XML files and Map files using Map Reduce Programs.  • Experienced in loading and transforming of large sets of structured, semi structured and unstructured data.  • Involved in creating Oozie workflow and Coordinator jobs to kick off the jobs on time for data availability.  • Used Flume to collect, aggregate, and store the web log data from different sources like web servers, network devices and pushed to HDFS.  • Scripting to deploy monitors, checks and critical sysadmin functions automation.  • Managing and scheduling Jobs on a Hadoop cluster.  • Performing tuning and troubleshooting of MapReduce jobs by analyzing and reviewing Hadoop log files.  • Involved in defining job flows, managing and reviewing log files.  Environment: Map Reduce, HDFS, Hive, Java, SQL, Cloudera Manager, Pig, Sqoop, Oozie Database Administrator Convergys - Cincinnati, OH January 2010 to March 2011 Description: Convergys Corporation is a corporation based in Cincinnati, Ohio, that sells customer management and information management products, primarily to large corporations. Information management provides convergent billing and business support system (BSS) products and services including revenue management, product and order management, and customer care management to telecom, utilities, and cable/satellite/broadband service providers.  Responsibilities:  • Installed, optimized and configured new servers and application upgrades in existing network environment as per requirements.  • Extensively working on performance tuning, working with developers every day and support production readiness testing in performance lab database.  • Identified poorly written SQL statements and suggested code change, implemented SQL profiles and SQL plan baselines.  • Managed and administered Windows/Linux systems - Performed periodic maintenance, provided technical support and executed systems engineering.  • Wrote PL/SQL procedures to do the database jobs and other monthly, weekly maintenance tasks.  • Performed schema refreshes from Production to QA and other test environments for testing.  • Develop new views and triggers for new software release.  • Provide on call support for various database issues like Oracle errors, slow performance and system maintenance issues.  • Perform SQL server Databases Migration from One server to another Server during Maintenance window activities.  • Scheduled Cron jobs for day-to-day database jobs and other monitoring tasks at database and UNIX level.  • Having High level of experience with different Interface systems supporting our applications and working on various environments based on requirements.  • Involved in analyzing the real time data and doing performance tuning.  • Supported Production, A and development database servers.  • Written SQL Scripts for generating reports.  • Provided User training and support.  • Worked on identifying and troubleshooting the bugs.  Environment: Oracle , PL/SQL, Linux, UNIX, Perl, Shell, Java, Nagios, Infants Rating & Billing Linux System Administrator Ediko Systems Inc - Hyderabad, Andhra Pradesh June 2008 to December 2009 Description:Ediko Systems Integrators Pvt Ltd, an IBM Premier Business Partner, is a specialist company delivering world class business solutions leveraging IBM Technologies. EDIKO ensures the delivery of high quality business integration solutions through the application of sound software architecture principles and using the latest IBM technologies together with agile project management techniques.  Responsibilities:  • Administration, package installation, configuration of Oracle Enterprise Linux 5.x.  • Administration of RHEL, which includes installation, testing, tuning, upgrading and loading patches, troubleshooting both physical and virtual server issues.  • Creating, cloning Linux Virtual Machines.  • Installing RedHat Linux using kick start and applying security polices for hardening the server based on the company policies.  • RPM and YUM package installations, patch and other server management.  • Managing systems routine backup, scheduling jobs like disabling and enabling cron jobs, enabling system logging, network logging of servers for maintenance, performance tuning, testing.  • Tech and non-tech refresh of Linux servers, which includes new hardware, OS, upgrade, application installation, testing.  • Set up user and group login ID's, printing parameters, network configuration, password, resolving permissions issues, and user and group quota.  • Creating physical volumes, volume groups, and logical volumes.  • Gathering requirements from customers and business partners and design, implement and provide solutions in building the environment.  • Installing and configuring Apache and supporting them on Linux production servers.    Environment: Oracle , Shell, Perl, PL/SQL, DNS, TCP/IP, Apache Tomcat, XML, HTML and UNIX/Linux Education Bachelor of Technology in Computer Science Osmania University - Hyderabad, Andhra Pradesh Additional Information Technical Skills:  Hadoop/Big Data HDFS,MapReduce,HBase,Pig,Hive,Sqoop,Flume,MongoDB, Cassandra, Power pivot, Puppet, Oozie, Zookeeper  Java & J2EE Technologies Core Java, Servlets, JSP, JDBC, JNDI, Java Beans  IDE's Eclipse, Net beans  Big Data Analytics Defamer 2.0.5  Frameworks MVC, Struts, Hibernate, Spring  Programming languages C,C++, Java, Python, Ant scripts, Linux shell scripts  Databases Oracle 11g/10g/9i, MySQL, DB2, MS-SQL Server  Web Servers Web Logic, Web Sphere, Apache Tomcat  Web Technologies HTML, XML, JavaScript, AJAX, SOAP, WSDL  Network Protocols TCP/IP, UDP, HTTP, DNS, DHCP  ETL Tools Informatica, Pentaho  Testing Win Runner, Load Runner ,QTP