Job Seeker Edison, NJ Work Experience Wells Fargo September 2018 to Present S & P Global Ratings (Sep-2017 - Feb-2018) Spark- Python Developer and Data Modeler Wells Fargo Wholesale Data - Summit, NJ September 2018 to Present Location: Summit, NJ.  Role: Spark- Python Developer and Data Modeler  Project: Design holistic view with conceptual and logical data model for CRM 360 for the consolidated view of 20 different lines of business (LOB) within the wholesale region. New initiative in order to bring various LOB's in wholesale under one roof and regulate data through surveillance process for entire wholesale region.  ? Developing Spark programs using Python API's to compare the performance of Spark with Hive and Oracle.  ? Designed and created Hive external tables using shared meta-store instead of derby with partitioning, dynamic partitioning and buckets.  ? Used PySpark-SQL to load JSON data and create schema RDD, Data Frames and loaded it into Hive Tables and handled Structured data using Spark-SQL.  ? Imported required tables from RDBMS to HDFS using Sqoop and used PySpark RDDs to get real time streaming of data into HBase.  ? Analyzed the SQL scripts and designed solutions to implement using PySpark and implemented process using Python and utilizing data frames and temporary table SQL for faster processing of data.  ? Performing Business Area Analysis and logical and physical data modeling for Data Warehouse / Data Mart applications as well as Operational applications enhancements and new development.  ? Develop Technical Metadata and Business Glossary for all wholesale LOBs business systems by partnering with the IT and business systems teams.  ? The data is cleaned in Oracle and loaded into a new table which is moved into HDFS using sqoop.  ? Working with Data Stewards to establish metadata registry responsibilities.  Tools: Oracle 12C database, Teradata, TOAD, Power Designer, ERStudio, SPARX Enterprise architect, Pyspark, Spark SQL,Hive,Sqoop. Senior PL-SQL developer and Data Architect New York, NY September 2017 to February 2018 Location: (New York, NY)  Role: Senior PL-SQL developer and Data Architect  Project: The Surveillance Optimization is project for restructuring existing rating application along with additional surveillance optimization rule enhancement. This platform has been introduced in order to accommodate new analytical solutions provided by business.  ? Created new DB objects like Tables, Procedures, Functions, Triggers and View using Oracle PL-SQL.  ? Taking care of DB Performance issues by tuning SQL queries and stored procedures by using SQL Profiler and Oracle execution plan tables.  ? Involved in writing Company's Metadata standards and practices, including naming standards, modeling guidelines and Data Warehouse Strategy. Identified capabilities and supporting applications imbalances resulting in revisions to corporate data models and reporting capabilities.  ? Analyze data across multiple sources to design and document logical and physical data models and maintain the Enterprise Data Warehouse Data Model. Gather and define data requirements based on specific application business requirements.  ? Prepare normalized models and dimensional models. Define conceptual, logical and physical data models  ? Understand crucial areas in existing process and new process for rating surveillance and design the data model which will maintain and support all surveillance processes including new and existing one  ? Communicate all designing requirement to all implementation partners which includes application, database and ETL team. ? Data Warehouse architecture, data design and implementation  ? Responsible for the Change Control and Release of Enterprise Data Models for Data Warehouse Subject Area Data Marts for all rating applications along with Logical and Physical design for Dimensional data warehouse designs using following tools: CA/ERWIN; Quest/TOAD; Oracle RDBMS; Ralph Kimball methodology.  Tools: Oracle 12C database, PL/SQL Developer, TOAD, Oracle SQL Developer, Erwin, ERStudio, Perl. Deutsche Bank April 2016 to August 2017 Spark- Python Developer and Data Modeler-Analyst Deutsche Bank - New York, NY April 2016 to August 2017 Location: (New York, NY)  Role: Spark- Python Developer and Data Modeler-Analyst  Projects: The Control Room Re-Engineering program has been commissioned to deliver strategic technical solution which will support the functions the Global Control Room perform such as conflict and research clearance ,restricted list, watch list management and information barrier maintenance as well .LUCID is compliance hub which is designed to source data for Actimize trade surveillance data for different products like FX, Equities and FX. Along with trade surveillance there is pricing and valuation for the different product of the repository.  ? Involved in creating Hive tables, loading with data and writing hive queries that will run internally in mapReduce way.  ? Importing and exporting data in HDFS and Hive using Sqoop.  ? Used Hive to do transformations, event joins and some pre-aggregations before storing the data onto HDFS  ? Used Sqoop efficiently transfer data between databases and HDFS and used PySpark to stream the log data from servers.  ? Primary business focus is in the area of trade surveillance for different IB products, Pricing and Valuation, compliance and control room application understanding along with complete data security and sensitivity for Control Room data.  ? Position reporting data analysis for reference security master data alignment and 13F holding fillings.  ? Developed Spark/MapReduce jobs to parse the JSON files for Oracle data.  ? Data Analysis with the help of Python Scripting for AML alert data files and few process implementations to store AML Alert data in DB in Python.  ? Data analysis for Watch List/Restricted List for existing Control Room.  ? Source data mapping for transaction data for different asset classes in order utilize this data for Actimize trade surveillance modules.  ? Data Analyst/Data Modeler performing Business Area Analysis and logical and physical data modeling for Data Warehouse / Data Mart applications as well as Operational applications enhancements and new development. Used the Ralph Kimball Methodology for Data Warehouse/Data Mart designs.  ? Used Spark (RDD) and python for processing and transformation of data and integration with popular NoSQL, Oracle database for huge volumes of data.  ? ER data modelling and logical database designs. Metadata and data taxonomy management.  ? Ensuring integrity of backend designs and reporting data marts. Initiating data design and reviews of high-level design requirements.  Tools: Oracle 10g, 11g database, PL/SQL Developer, TOAD, Oracle SQL Developer, Power Designer 16.5,Apache Hadoop2.3 ,Sqoop,Spark,PySpark, Python, Vice President April 2015 to April 2016 Lead Data Modeler and Datawarehouse Developer TD Bank - Mount Laurel, NJ April 2015 to April 2016 Location: (Mount Laurel, NJ)  Role: Lead Data Modeler and Datawarehouse Developer  Projects: Collects detailed data on bank holding Companies (BHCs) for various assets classes, capital components, small loans, corporate loan and pre-provision net revenue (PPNR) on monthly/quarterly/yearly basis. This data will be reported to Federal Government in the form of FR Y-14, FR Q-14 or FR M-14 reporting format.  ? Working with solution architect & business analysts to define implementation design and coding of the assigned modules/responsibilities with highest quality (bug free) with the help of Teradata FSLDM.  ? Involved in performance tuning of code using execution plan and SQL Profiler.  ? Oracle DB implementation and ETL process development.  ? Involved in Migration activity for few modules from IBM DB2 to Oracle.  ? Implemented Python Scripts to validate source system data.  ? Participating with key management resources in the strategic analysis and planning requirements for Data Warehouse/Data Mart reporting and data mining solutions.  ? Manage newly built Enterprise Data Warehouse, Analytics Data Mart and the Customer Data Platform.  ? Data Quality Management and Data Architecture standardization.  ? Managed the meta-data for the Subject Area models for both Operational & Data Warehouse/Data Mart applications.  ? Loaded data from Oracle into HDFS using sqoop batch jobs for post trade transaction for different reporting applications.  ? Developed SPARK programs using Python to load data from Oracle to HDFS and Hive external tables.  ? Loaded JSON and XML file data using Spark-SQL and created RDD schema to load same data into HIVE tables.  ? Implemented performance effective solutions for long running SQL processes by using PySpark Data Frame processing.  ? Implemented CSV file load processes in Oracle in PySpark by using Panda libraries.    Tools: Oracle 10g, 11g database, IBM DB2, PL/SQL Developer, TOAD, Oracle SQL Developer, Power designer 16.5, ERStudio,Erwin. Python,SPARK. Datawarehouse Architect Cognizant Technology Solutions October 2005 to April 2015 Spark- Python Developer and Data Modeler Credit Suisse Singapore March 2011 to March 2013 from Mar-2011 - Mar-2013 and Credit Suisse New York from Apr 2013 - Apr-2015)  Location: (Singapore and NY)  Role: Spark- Python Developer and Data Modeler  Projects: Sales Reporting & Securities Intelligence Processing is state of the Reporting and Sales intelligence Applications which cater the enterprise wide Reporting and Real time data needs. It includes equity trade surveillance along with sale credit calculations for equity trades.  ? Data Modeling (logical, dimensional and physical) for various projects, work with application teams to analyze data requirements, review data model and address the functional and nonfunctional requirements (performance, geographical separation, auditing, archiving).  ? Core member of the solution team responsible for proposing architecture, high level design for Investor Tax platform for multiple geographies (US, UK, Singapore and India)  ? FPML data processing implementation for FX products.  ? Led data model team on Data Marts (Star Schema) for Volcker reporting Data model set up and implementation.  ? Define best practices for data modelling and database development.  ? Built and tuned complex and large data load from various sources to ODS (Operational Data System), Financial Star Schema and Campaign Analysis Mart ETL (Informatica Power Center 1.7/Oracle Analytical SQL)  Tools: Oracle 10g, 11g database, PL/SQL Developer, TOAD, Oracle SQL Developer, Oracle Data Modeler, Informatica, Erwin, Apache Hadoop 2.2, Sqoop,Spark,PySpark, Python. Senior Database Developer Wellington Management Company - Pune, Maharashtra March 2009 to February 2011 Location: (Pune, India)  Role: Senior Database Developer  ? Assess, Troubleshoot, analyze performance issues during 9i to 11g migration, nightly batch processes and advise solutions.  ? SQL tuning, 10053 trace analysis, responsible for improving PLSQL refactoring, improving performance from hours to minutes, optimize distributed processing, reduce database resource usage - logical IOs, temporary table space usages, CPU consumption latch contention etc. Played crucial role in go-live of the 11g Upgrade project.  ? Design database objects including tables, indexes, views, materialized views, sequences and referential integrity for a reporting data warehouse. Develop and maintain database programs including packages, procedures, functions and triggers.  Tools: Oracle 10g, 11g database, PL/SQL Developer, TOAD, Oracle SQL Developer, Erwin. Database Developer and Data Modeler AOL - Pune, Maharashtra September 2005 to February 2009 Sep-2005 to Feb 2007 for ABN AMRO and from Mar 2007 - Feb- 2009 for RBS)  Location: (Pune, India)  Role: Database Developer and Data Modeler  Projects: This is online corporate reporting application developed in Java, Oracle10g which is used to process ABN-AMRO corporate client payment data.  ? Develop and maintain database programs including packages, procedures, functions and triggers using PL/SQL.  ? Develop and support the Oracle PL/SQL code that performs the calculations for the all fulfillment marketing programs.  ? As requested, support/fulfill requests for information and troubleshoot code  Tools: Oracle 11g database, PL/SQL Developer, TOAD, Oracle SQL Developer. Data Architect Credit Suisse - Boston, MA May 2005 to September 2005 Boston, US  ? ABN AMRO Bank (AOL-Access Reporting)  • Persistent Systems Ltd. (AMTS). (May-2005 - Sep-2005) Lead Database Developer and data Modeler Home Builder Association - Pune, Maharashtra May 2005 to September 2005 Location: (Pune, India)  Role: Lead Database Developer and data Modeler  Projects: This is online corporate reporting application developed in .NET, Oracle 9i for local builders. Bay Systems Pvt. Ltd December 2004 to April 2005 Senior Database Developer MBT-Bay system - Pune, Maharashtra December 2004 to April 2005 Projects: This is online credit card application developed in .NET, Oracle 9i for UAE based financial institution. DSK Systems Pvt. Ltd May 2002 to December 2004 Database Developer DSK Systems Pvt. Ltd - Pune, Maharashtra May 2002 to December 2004 Projects: This is online builder application, which is used for DSK builder's internal all site details. The application developed in .NET, Oracle 9i for UAE based financial institution. Education DAC in Advance Computing PUNE University January 2002 Bachelor of Computer Science in B.SC-Comp Dr. Babasaheb Ambedkar Marathwada University - Aurangabad, Maharashtra May 2001 Skills database (10+ years), Erwin (5 years), Oracle. (10+ years), Pl-sql (10+ years), Sql (10+ years)