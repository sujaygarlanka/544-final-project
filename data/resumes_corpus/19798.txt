Hadoop Developer Hadoop <span class="hl">Developer</span> Hadoop Developer - Vaizva Inc Alpharetta, GA Around 4 years of IT experience in Big Data Analysis, Design, Development, Implementation and Testing of various stand-alone and client-server architecture based enterprise application software in Python on various domains.  • Proven organizational, time management and multi-tasking skills and ability to work independently and quickly learn new technology and adopt to new environment  • Expert knowledge & experience in Object Oriented Design and Programming concepts.  • Strong experience in using Python's application development frameworks such as Django.  • Experience in real-time configuration and management of MySQL, MongoDB and PostgreSQL databases on a large scale.  • Experience on several python packages like Numpy, Beautiful Soup, Pickle, PySide, Scipy, and PyTables.  • Experience in developing Web Services with Python programming language.  • Familiar with JSON based REST Web services and Amazon Web services.  • Good experience in writing SQL queries and implementing stored procedures, functions, packages, tables, views, cursors, triggers.  • Significant application development experience in Ubuntu, RedHat and Windows environments.  • Extensive experience on developmental tools such as Eclipse, PyCharm and Sublime Text, version control tools like Git and Apache SVN.  • Experience in testing and debugging applications using PyUnit, PyTest and JUnit frameworks.  • Experience in working with AWS: Amazon S3, Amazon EC2, and Relational Database Services.  • Have experience in Object-oriented programming, multi-threading, algorithms, data structures and system programming.  • Proficient in Python, experience building, and product ionizing end-to-end systems  • Knowledge of Information Extraction, NLP algorithms coupled with Deep Learning  • Experience with file systems, server architectures, databases, SQL, and data movement (ETL).  • Excellent understanding of Hadoop architecture and various components such as HDFS, Job Tracker, Task Tracker, Name Node, Data Node, YARN, Spark and MapReduce programming paradigm.  • Experience in importing and exporting data using Sqoop from Relational Database to HDFS and from HDFS to Relational Database.  • Experience in continuous build and version control systems like Jira, Git and SVN.  • Setting up python REST API Frame work using Django.  • Hands on experience in data quality, data organization, metadata, and data profiling and large Data set sizes.  • Ability to move data between production systems and work on cross/multiple platforms.  • Experience in writing Sub Queries, Stored Procedures, Triggers, Cursors, and Functions on MySQL and PostgreSQL database.  • Good experience in handling errors/exceptions and debugging the issues in large scale applications.  • Good Analytical and Problem-Solving skills and ability to work on own besides being a valuable and contributing team player.  • Exceptional problem solving and decision-making capabilities, recognized by associates for quality of data in alternative solutions.  • Well-versed with Agile Development process tools like Jira.  • Practical experience with working on Multiple-Environments like Development, Testing, Production.  • Excellent Interpersonal and Communication skills, Efficient Time Management and Organization Skills, ability to handle Multiple-Tasks and work well in a Team-Environment. Work Experience Hadoop Developer Vaizva Inc - Alpharetta, GA January 2018 to Present PTT serves at the one-stop solution for logistics business. It is a coherently designed integrated solution with planning, scheduling, real-time tracking, and key performance indicators with rich, built-in analytics. And, as there is no middleware software involved users can significantly reduce their IT costs.    Responsibilities:  • Experience in writing Sqoop scripts to import and export data from RDBMS into HDFS, HIVE and handled incremental loading on the customer and transaction information data dynamically.  • Involved in creating Hive tables, loading with data and writing hive queries that will run internally in map reduce way.  • Developed Spark code using Scala and Spark -SQL/Streaming for faster testing and processing of data.  • Skilled experience in installing, configuring and using Apache Hadoop ecosystems such as Pig and Spark.  • Estimated the Software & Hardware requirements for the Name Node and Data Node in the cluster.  • Extracted the needed data from the server into HDFS and Bulk Loaded the cleaned data into HBase using MapReduce.  • Develop HIVE queries for the analysts.  • Created an e-mail notification service upon completion of job for the particular team which requested for the data.  • Migrate on in-house database to AWS Cloud and also designed, built, and deployed a multitude of applications utilizing the AWS stack (Including EC2, RDS) by focusing on high-availability and auto-scaling.  • Involved in support for Amazon AWS and RDS to host static/media files and the database into Amazon Cloud.  • Microservice architecture development using Python and Docker on an Ubuntu Linux platform using HTTP/REST interfaces with deployment into a multi-node Kubernetes environment.  • Performed Test Driven Development (TDD) and continuous integration to keep in line with Agile Software Methodology.  • Worked in development of applications especially in LINUX environment and familiar with the commands.  • All projects adhered to agile principles to deliver commitments, meet acceptance criteria and ensure fast releases.    Environment: Python, Hortonworks, Spark, Oozie, Hive, HBase, Linux, MapReduce, Scala, Sqoop, GitHub, AWS, Linux, Shell Scripting. RPA Developer The Vanguard Group - Charlotte, NC August 2017 to December 2017 Retails Systems focuses on automating the manual process of creating Report by extracting data from Web, Database and Main Frame Application and preparing the report and sending a mail with attaching these reports and exceptions. At VANGUARD, the focus was on creating Robots are designed to identify existing huge time taking, repetitive, manual processes and provide innovative automated solutions.    Responsibilities:  • Developed Blue Prism frame work (Visual Business Objects, Process studio) to update the Provider directory from excel, XML, Website data through screen scraping and using OCR (Optical character recognition) from printed material for some of the Financial Advisor and Client facing Applications.  • Worked on Integrating applications like (Web, Services, Mainframe, MS Office, GUI, Outlook etc.) using workflow, automation tools (Blue Prism).  • Worked with BA (Business Analysts) in developing and creating the estimations for Processes.  • Extensively used Blue Prism In-built objects, functions in Expression Builder for String Operations, Collection Manipulation to Compare and eliminate duplicates from the data  • Interacting with different based applications and buildings various action stages within the business object and interacted all the objects in process studio.  • Implemented Blue Prism User authentication by defining user roles, creating users and setting password policies.  • Efficiently handled monitoring and troubleshooting the Blue Prism environment through Control room.  • Created access control interfaces via the Application Modeler, within Object Studio.  • Experience in using Blue Prism's Credential Manager for maintaining, securing and retrieving the user credentials.  • Experience using Excel VBO, XML VBO object to perform operations while taking data as inputs.  • Efficient use of stages, blocks, data types, session and environmental variables for processing of Business Process diagrams and Process flow charts using RPA tools.  • Creating and documenting test procedures and scenarios for Pre-UAT phases supporting the operational teams during UAT and rollout phases.  • Actively involved in vigorous unit testing, end-to-end testing and Production support to accelerate Business processes.  • Provided Exception handling at every possible scenario in order to diffuse an exception during Process development.  • Managed a team of developers in an Agile environment.  • Extensively used different action stages to perform logical operations in Blue Prism.  • Responsible to develop services using .NET and Web API technology.  • Performed DEV, SAT and POAT testing before sending to production. Worked closely with QA (Quality Analyst) and involved in creating Elevation checklists and Elevating processes.  • Proven organizational, time management and multi-tasking skills and ability to work independently and quickly learn new technology and adopt to new environment  • Professional communication skills, coupled with very positive user interaction & team spirit.    Environment: Blue Prism V5.0, V4.2, OCR, C#.NET, VBA, HTML, XML, MS Excel, SQL Server, Visual Studio. Hadoop Developer Vaizva Inc - Alpharetta, GA February 2017 to August 2017 VaizTrack intelligent software along with a physical device installed in vehicles can allow organizations to surveil and track their vehicles round the clock, monitor battery status and remotely disable battery in the event of theft. It can also help manage, maintain and retrieve vehicle and vehicle parts information efficiently and manage inventories with automated alerts.  Responsibilities:    • Development of Map Reduce jobs in cascading for data cleansing and data processing of flat files.  • Responsible for importing the flat files from external environments to ACC in Hadoop.  • Design, developed and implemented main flow component for end to end data flow process within the platform.  • Responsible for creating the SOAP clients for consuming the web service Requests.  • Involved in Analysis and design for setting up edge node as per the client requirement.  • Created Pig Latin scripts to sort, group, join and filter the enterprise wise data.  • Expertise in writing the hive scripts for large data sets comparison.  • Expertise in performance optimization and memory tuning of map-reduce applications.  • Experience with MySQL for utilizing it for auditing purposes on the cluster.  • Responsible for MapR upgrade in both production and non-production environment.  • Written shell scripts for data extraction and data cleansing for performing member specific analytics.  • Co-ordinate with Administrator team to analyze Map Reduce Jobs performance for resolving any cluster related issues.  • Expertise in platform related Hadoop Production support tasks by analyzing the job logs.  • Transferred data from external sources from MySQL to Hadoop using Sqoop.  • Co-ordinate with different teams to determine the root cause and taking steps to resolve them.  • Responsible for continuous Integration with Jenkins and deploying the applications into production using XL Deploy.  • Performed POC in installation & configuration of Apache Hadoop on Amazon AWS (EC2) system.  • Managed and reviewed Hadoop log files to identify issues when job fails and finding out the root cause.  • Utilizing service now to provide application support for the existing clients.    Environment: Python, Hortonworks, Hadoop YARN, MapReduce, Hive, Pig, Shell Scripting, SOAP web service, MySQL, Sqoop, MapR, Git, Jenkins. Python Developer Sutherland Global Services - Chennai, Tamil Nadu November 2013 to May 2015 Responsibilities:  • Wrote application views which triggered by URLs and open respected templates  • Developed different REST APIs in Jinja and flask framework with using python scripting.  • Used MySQL as backend database and MySQL dB of Python as database connector to interact with MySQL server.  • Used Restful APIs to access data from different suppliers.  • Support the scripts configuration, testing, execution, deployment and run monitoring and metering.  • Used Restful API's to gather network traffic data from Servers.  • Developed and executed User Acceptance Testing portion of test plan.  • Involved in complete SDLC - Requirement Analysis, Development, System and Integration Testing.  • Followed MVC Structure to develop Application.  • Responsible for search engine optimization to improve the visibility of the website.  • Developed Merge jobs in Python to extract and load data into MySQL database.  • Performed Unit and system testing.  • Handled cross browser/platform compatibility issues (IE, Firefox, and Chrome) on both Windows.  • Managed application state using server and client-based State Management options.    Environment: Python 2.7, Django, MySQL, HTML, XHTML, CSS, JavaScript, Apache Web Server, Git, Linux. Education Master's Skills HTML (1 year), Linux (1 year), MySQL (2 years), Python (2 years), Scripting. (2 years) Additional Information Technical skills:    Languages Python, C# .NET, JAVA,HTML, CSS, Shell Scripting.  Tools BluePrism V4.2, V5.0, UiPath, Selenium web driver, Mainframes, SAP Ui5.  Platforms Linux/Unix (Redhat 5,6, 7), Ubuntu (12.04,14.04), CentOS, Windows NT/2003/2008, Windows 10.  Databases MySQL, Oracle 10/11g, PostgreSQL, SQL Developer, SQL Server.  Hadoop Tools Hortonworks,HDFS, Hive, PIG, Sqoop, Flume, MapReduce,Oozie,Spark, Ambari, HDP, Hortonworks Data Flow (HDF), NiFi.  Other tools SAP Support Portal,HPSM, JIRA, Service Now,MS Office applications, Outlook ,Visual Studio