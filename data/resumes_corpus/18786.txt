Job Seeker I have been working as a hands on solution architect for Microsoft at one of their client site, a multi-billion dollar company in New York with Petabytes of data to resolve their architectural, technical and security issues. This is the 4th contract for me at Microsoft as a high level Multi-cloud Azure and AWS expert. On this project.This is a Multi-Cloud project (Azure AWS) with cloud Migration, streaming including, massive amount of data in Petabytes with real time streaming using Kappa architecture. This project among others includes Data Pipeline, Data Lakes (Landing Zone, Processing Zone and Consumption zone), high level of domain Join security, Big Data Visualizations, Machine Learning, Deep Learning (CNN RNN). The architecture that we came up with is similar to Uber Gen-4 architecture with Hudi capable of processing tens of petabytes of streaming data. Please note that due to confidentiality and sensitivity I cannot and will not reveal specific technical details. Scala, SBT, Eclipse, <span class="hl">Python</span>, Pyspark, R, Zeppeline, Jupyter, Hadoop, Spark, Kakfka, Oozie, Sqoop, NiFi, Pig, Hive, Hbase etc. Senior lead, Senior Solution Architect Consultant, senior developer, Big Data, Data Engineer, Data Scientist, Data Warehousing, Spark, HDFS, Kafka, BI, Kubernetes, IBM Cloud and IBM Cloud Private (ICP), advanced Machine Learning Deep Learning and Convolutional Neural Networks CNN. I am one of the main Architects of GAIA (Ericsson Global Artificial Intelligence Accelerator) a $75,000,000 / year new department intended to use Artificial Intelligence ML, DL , CNNs to revolutionize cellular communication especially for the G5. Working as senior Solution Architect Consultant, senior developer, senior data engineer, senior data scientist. Implemented 2000 node 16,000 CPUs Kubernetes cluster and implemented variety of Machine Learning, Deep Learning and Convolutional Neural Network CNN. The Kubernetes Cluster that I created had Data Pipeline, Data Lake (Raw Data Landing Zone, Processing Zone, Consumption Zone) with the capacity of Peta Bytes 150TB /Day Data for Machine Learning, Convolutional Neural Network CNN, Spark, HDFS, Kafka, Elasticsearch Logstash Kibana ELK, NiFi, Hive, HBase, Jupyter Zeppelin with Scala and Python. Also implemented advanced Machine Learning, Deep Learning, and Convolutional Neural Networks CNN and deployed massive POCs on Azure Cloud, GCP, AWS and OpenStack with impressive results. Newport Beach, CA SUMMARY  I have almost 20 years of experience in software development, firmware, hardware design, wireless and satellite communications engineering. I have worked for some of the most prestigious companies in Silicon Valley and other locations such as Lockheed Martin, Space Systems Loral, Microsoft, Hewlett Packard, Texas Instruments, Optum, Acxiom, Citibank, Adaptec Broad Logic, United Healthcare, ABB Atom, Ericsson, Dell, Argonaut Technologies, Bausch & Lomb, DARPA, Department of Defense (DOD), etc.  I have been responsible for the development of many systems and applications from the designing board to the commercial release.    OBJECTIVE  Primarily looking for consulting and contract work but also open to permanent positions.   Senior hands on developer, application and solution architect, big data architect, mobile app developer and architect, project manager.    Senior Big Data and Cloud Architect Consultant / BI Solutions Architect / Data Management and Cloud Lead/ DevOps    SKILLS  • 18 years of solid working experience with a PhD from University of California UCLA/Davis   • 17 years of experience as Hands-On Solution Architect, Team Lead, Specialist, Developer, doing Senior Big Data and Cloud Architect Consultant / BI Solutions Architect / Data Management and Cloud Lead  • Cloud Architecture, Big Data Engineering, Machine Learning, Deep Learning, Data Scientist, Business Intelligence (BI), Data Warehousing System Engineering multi-tiered applications.  • 9+ years of experience in Big Data, Hadoop, Spark, MapR, Cloudera, Hortonworks, Storm, KafKa, Hive, Impala, Flume, Sqoop, MapReduce, Pig, HBase, NiFi, oozie, Tableau Power BI, Cloudera visualization, QlikView Scala SBT (My Preference), Java Maven etc.  • Expert in Big Data: HDFS, Hive, Spark, Scala, Java, Python, Hadoop, Sqoop, Map/Reduce, Hortonworks, Cloudera, MapR, NoSQL HBase, Cassandra, Kafka, Storm, Spark Streaming, Zeppelin, Kibana, Spark MLib, Mahout, R, NiFi, Falcon, Oozie, Kylin, Atlas, Drill, Solr, ElasticSearch, Ambari,Ranger, Flume, Impala, Pig, HDInsight, SBT etc.  • 7 years of Multi-Cloud including Azure Extensive full cycle Cloud Azure experience with full Big Data, Machine Learning Deep Learning, Azure Machine Learning Studio, Azure Power BI, Azure Search, and Elasticsearch on Azure development and deployment. Comprehensive deployments of massive Azure infrastructures from inside the Visual Studio 2017 using ARM Templates. Deployment of many Azure projects for different companies and here are some of practical!! Hands on!! Component that I have personally used, Azure Spark Hive HBase Hadoop Kafka Storm ML Services (R Server) HDInsight clusters, Domain-joined HDInsight clusters, Azure Zeppelin notebooks, Azure Jupyter notebooks, Azure SQL Data Warehouses, Azure Databricks, Azure Data Lake, Azure Data Lake Factory, Azure Data Lake Storage, Azure Data Lake Analytics, Azure Data Links, Azure Integration Runtime (IR), Azure Data Gateway, Azure Kubernetes Services, Azure Storage Blobs, Azure Active Directory, Azure Service Principals, Azure Security Center, Azure Key Vaults, Azure Virtual Network vnet, Azure Log Analytics, Azure Network Interfaces, Azure Cosmos DB, Azure Cortana Intelligence Suite, Infrastructure as a Service IaaS, Platform as a Service PaaS, Microsoft R Server, NLB, Key phrase extraction Azure search, Unstructured text analytics, Event hub, Streaming, Poly Base etc.  • 8 years of AWS Cloud Extensive full cycle Cloud AWS Redshift, RDS, EMR, Kinesis, S3, Glue, DMS, Athena, EC2, Lambda, experience with full Big Data, Amazon Elastic MapReduce (EMR), Hadoop, Spark, Hive, Pig, Kafka, MSK, AWS Management Console, AWS CLI, Amazon EMR File System (EMRFS), collaborative notebooks Apache Zeppelin, Jupyter, deep learning frameworks like Apache MXNet, Elasticsearch and SOLR, Machine Learning and Deep Learning development and deployment. AWS Compute E2C, FarGate, Lambda, VMware, AWS Developer Tools, AWS Management Tools, Amazon Machine Learning, AWS DeepLens, Amazon Deep Learning AIMs, Amazon TensorFlow on AWS and other components etc.  • 7 years of Machine Learning, Deep Learning and Artificial Intelligence MLlib, TensorFlow, Keras, Weka Mahout, Multilayer perceptron classifier (MLPC), the feedforward artificial neural network, Convolutional Neural Network CNN, scikit-learn, Pandas, Deeplearning4j, H2o, Sparkling Water ML, Caffe2, MxNet etc. Different algorithms K-Means, Random Forest, Gradient Boosting algorithms (GBM, XGBoost and CatBoost) etc.  • 9 years of search engines ELK Stack Elasticsearch, Logstash, Kibana, Filebeat, SOLR, Lucene, Rsync, Tika. Also been involved with migration from SOLR to Elasticsearch for at least three companiesetc.  • Expert in DevOps using Azure DevOps, VSTS, AWS-CodePipeline, Terraforms, Jenkins, Ansible, Git, Maven, Cloudera Navigator, Data Lineage, Kubernetes, Docker.   • 5 years of Solid Kubernetes, Orchestration and Micro Services experience Distributed Container based Architecture, Docker, Docker CLI, Kubernetes, Kubernetes CLI, kubeflow, Kube-scheduler, Pods, Pods deployments. Service Deployments, Ingress, Helm Charts, Helm Charts CLI, YAML etc. Successfully installed and deployed for several companies huge Kubernetes Cluster (the latest with more than 2000 nodes 16,000 CPUs deployed in 20 minutes) with both Data Pipeline, Data Lake (Raw Data Landing Zone, Processing Zone, Consumption Zone) for Machine Learning, Convolutional Neural Network CNN, Spark on Kubernetes, HDFS on Kubernetes, Kafka on Kubernetes, Elasticsearch Logstash Kibana ELK on Kubernetes, NiFi on Kubernetes, Hive HBase Jupyter Zeppelin with Scala and Python on Kubernetes. Used Vagrant Terraform HashiCorp for deployment with 2000 nodes, 16,000 CPUs with Peta Bytes and 150TB / day capacity etc.  • 3 years of IBM Cloud and IBM Cloud Private (ICP) Distributed Container based Architecture, Docker, Docker CLI, Kubernetes, Kubernetes CLI, Pods, Pods deployments. Service Deployments, Ingress, Helm Charts, Helm Charts CLI. Successfully installed and deployed an entire IBM Cloud Private ICP Cluster then implemented and deployed ELK Elasticsearch, Logstash, Kibana, Filebeat, Kafka, Zookeeper, Cassandra, Curator on ICP IBM Private Cloud, Kubernetes, Pods using Helm Charts, Scala SBT.  • 16 years of hands on .NET development, architecture and management experience in application, real time, instrumentation, web, front end, back end, full stack, multiple products out there multiple awards  • 16 years of hands on Java development, architecture, front end, back end, full stack  • 6 years of Android mobile development and architecture with multiple apps in the app store  • 16 years of experience in SQL 7-2016, MySQL, Oracle, and other databases T-SQL, SSIS, SSRS, SSAS, OLTP, OLAP, Multidimensional Cube, MDX, PowerPivot, Tabular Model, SharePoint, PerformancePoint.  • Demonstrated experience and understanding of the best practices in all aspects of data warehousing (Inmon/Kimball approach). Solid experience in Data Warehouse  • Strong knowledge and proven results in Data Warehouse and Data Mart design including Dimensional Modeling (Star & Snowflake Schemas), ER Modeling, 3 Normal Forms, Normalization and Demoralization, Logical Model and Physical Model, Fact/Dimension/Hierarchy identifications.  • From Business Case to Data Visualization, I have designed and developed solutions by combining Business Process with Information Technology.  • Firmware embedded programming, ARM, PIC, DSP, FPGA, RTOS Linux etc.  • Significant management experience including 4 years as the VP of engineering Authorized to work in the US for any employer Work Experience I have been working as a hands on solution architect for Microsoft at one of their client site, a multi-billion dollar company in New York with Petabytes of data to resolve their architectural, technical and security issues. This is the 4th contract for me at Microsoft as a high level Multi-cloud Azure and AWS expert. On this project.This is a Multi-Cloud project (Azure AWS) with cloud Migration, streaming including, massive amount of data in Petabytes with real time streaming using Kappa architecture. This project among others includes Data Pipeline, Data Lakes (Landing Zone, Processing Zone and Consumption zone), high level of domain Join security, Big Data Visualizations, Machine Learning, Deep Learning (CNN RNN). The architecture that we came up with is similar to Uber Gen-4 architecture with Hudi capable of processing tens of petabytes of streaming data. Please note that due to confidentiality and sensitivity I cannot and will not reveal specific technical details. Scala, SBT, Eclipse, Python, Pyspark, R, Zeppeline, Jupyter, Hadoop, Spark, Kakfka, Oozie, Sqoop, NiFi, Pig, Hive, Hbase etc. Microsoft - Newport Beach, CA February 2019 to Present This is the 4th contract for me at Microsoft as a high level Multi-cloud Azure and AWS expert. On this project I have been working as a hands on solution architect for Microsoft at one of their client site, a multibillion dollar company in New York with Petabytes of data to resolve their architectural, technical and security issues.    This is a Multi-Cloud project (Azure AWS) with cloud Migration, streaming including, massive amount of data in Petabytes with real time streaming using Kappa architecture. This project among others includes Data Pipeline, Data Lakes (Landing Zone, Processing Zone and Consumption zone), high level of domain Join security, Big Data Visualizations, Machine Learning, Deep Learning (CNN RNN). The architecture that we came up with is similar to Uber Gen-4 architecture with Hudi capable of processing tens of petabytes of streaming data. Please note that due to confidentiality and sensitivity I cannot and will not reveal specific technical details.    ·       Azure: Working with ADF Azure Data Factory gen2, ADLS Azure Data Lake gen2, Databricks, Kubernetes, HDInsight 4.0, ESP. AD. AAD-DS. ESP, MFA, IR Integration Runtime Gateway, Domain join, Ranger, Ambari, Advanced 4 pillar of security for HDInsight (perimeter VNET, Kerberos AD ESP authentication, Hive policies Ranger and data encryption), Advanced ADLS data lake structures, advance Machine learning and CNN, ARM templates, notebooks Zeppelin, Jupyter,, Hadoop, Spark, Kakfka, Oozie, Sqoop, NiFi, Pig, Hive, Hbase etc.    ·       AWS: Redshift, RDS, EMR, Kinesis, S3, Glue, DMS, Athena, EC2, Lambda projects with full Big Data, Amazon Elastic MapReduce (EMR), CloudFormation CFN,Spark, Kakfka, Oozie, Sqoop, NiFi, Pig, Hive, Hbase, MSK, VPC, Subnet, Gateway, AWS Management, Kubernetes, Console, AWS CLI, Amazon EMR File System (EMRFS), collaborative notebooks Zeppelin, Jupyter, deep learning frameworks like Apache MXNet, Elasticsearch and SOLR, Machine Learning and Deep Learning development and deployment. AWS Compute E2C, FarGate, Lambda, VMware, AWS Developer Tools, AWS Management Tools, Amazon Machine Learning, AWS DeepLens, Amazon Deep Learning AIMs, Amazon TensorFlow on AWS and other components. etc.    ·       Development in, Scala, SBT, Eclipse, Python, Pyspark, R, Zeppeline, Jupyter Senior lead, Senior Solution Architect Consultant, senior developer, Big Data, Data Engineer, Data Scientist, Data Warehousing, Spark, HDFS, Kafka, BI, Kubernetes, IBM Cloud and IBM Cloud Private (ICP), advanced Machine Learning Deep Learning and Convolutional Neural Networks CNN. I am one of the main Architects of GAIA (Ericsson Global Artificial Intelligence Accelerator) a $75,000,000 / year new department intended to use Artificial Intelligence ML, DL , CNNs to revolutionize cellular communication especially for the G5. Working as senior Solution Architect Consultant, senior developer, senior data engineer, senior data scientist. Implemented 2000 node 16,000 CPUs Kubernetes cluster and implemented variety of Machine Learning, Deep Learning and Convolutional Neural Network CNN. The Kubernetes Cluster that I created had Data Pipeline, Data Lake (Raw Data Landing Zone, Processing Zone, Consumption Zone) with the capacity of Peta Bytes 150TB /Day Data for Machine Learning, Convolutional Neural Network CNN, Spark, HDFS, Kafka, Elasticsearch Logstash Kibana ELK, NiFi, Hive, HBase, Jupyter Zeppelin with Scala and Python. Also implemented advanced Machine Learning, Deep Learning, and Convolutional Neural Networks CNN and deployed massive POCs on Azure Cloud, GCP, AWS and OpenStack with impressive results. Ericsson - Santa Clara, CA August 2018 to February 2019 Designed and developed data engineering solutions as a senior Hands-On Solution Architect Consultant, Big Data, Data Engineer, Data Scientist, senior developer, Data Warehousing, Spark, HDFS, Kafka, BI, Kubernetes, IBM Cloud and IBM Cloud Private (ICP), advanced Machine Learning Deep Learning and Convolutional Neural Networks CNN. I am one of the main Architects of GAIA (Ericsson Global Artificial Intelligence Accelerator) a $75,000,000 / year new department intended to use Artificial Intelligence ML, DL, CNNs to revolutionize cellular communication especially for the G5. Working as senior Solution Architect Consultant, senior developer, senior data engineer, senior data scientist. Implemented 2000 node 16,000 CPUs Kubernetes cluster and implemented variety of Machine Learning, Deep Learning and Convolutional Neural Network CNN. The Kubernetes Cluster that I created had Data Pipeline, Data Lake (Raw Data Landing Zone, Processing Zone, Consumption Zone) with the capacity of Peta Bytes 150TB /Day Data for Machine Learning, Convolutional Neural Network CNN, Spark, HDFS, Kafka, Elasticsearch Logstash Kibana ELK, NiFi, Hive, HBase, Jupyter Zeppelin with Scala and Python. Also implemented advanced Machine Learning, Deep Learning, and Convolutional Neural Networks CNN and deployed massive POCs on Azure Cloud, GCP, AWS and OpenStack with impressive results.    ·       Migrated and architected a Multi-Cloud solution using AWS and Azure using resources such as Redshift, RDS, EMR, Kinesis, S3, Glue, DMS, Athena, EC2, Lambda projects with full Big Data, Amazon Elastic MapReduce (EMR), Hadoop, Spark, Hive, Pig, Kafka, AWS Management Console, AWS CLI, Amazon EMR File System (EMRFS), collaborative notebooks Apache Zeppelin, Jupyter, deep learning frameworks like Apache MXNet, Elasticsearch and SOLR, Machine Learning and Deep Learning development and deployment. AWS Compute E2C, FarGate, Lambda, VMware, AWS Developer Tools, AWS Management Tools, Amazon Machine Learning, AWS DeepLens, Amazon Deep Learning AIMs, Amazon TensorFlow on AWS and other components. etc.    In a course of less than a year, I have architected and lead some of the most sophisticated Big Data, Deep learning, Machine Learning, and Convolutional Neural Network CNN in the nation for Ericsson Artificial Intelligence Accelerator (Ericsson GAIA) in Santa Clara California. For the first time the technologies were right to create a comprehensive 2000 nodes 16,000 CPUs cluster on Kubernetes with all necessary micro services with automatic orchestration with dynamic deployment for a petabytes Artificial Intelligence system. Some of the results were are impressive. Examples:    ·       Machine Learning, Deep Learning, Convolutional Neural Network CNN, Anomaly detection on massive amount of real time streaming 5G wireless data with 150TB /day live with high accuracy using the 2000 nodes 16,000 CPUs Kubernetes cluster. This would have taken years of processing in the past and is now feasible in minutes on live data.  ·       High accuracy at detecting hack attack, security breach and data breach on live data using CNNs  ·       High accuracy at predicting system availability and reliability and predicting anomalies.    ***Please note that due to the sensitivity and proprietary nature of these projects and since I am one of the main architect I cannot and will not reveal and will not go into too much details!!!    ·       Created massive Kubernetes clusters on OpenStak, Azure, GCP, AWS with micro service Successfully installed and deployed clusters with more than 2000 nodes 16,000 CPUs deployed in 20 minutes with both Data Pipeline, Data Lake (Raw Data Landing Zone, Processing Zone, Consumption Zone) for Machine Learning, Convolutional Neural Network CNN, Spark on Kubernetes, HDFS on Kubernetes, Kafka on Kubernetes, Elasticsearch Logstash Kibana ELK on Kubernetes, NiFi on Kubernetes, Hive, HBase, kubeflow, Kube-scheduler, Jupyter Zeppelin with Scala and Python on Kubernetes. Used Vagrant Terraform HashiCorp for deployment with 2000 nodes, 16,000 CPUs with Peta Bytes and 150TB / day capacity.  ·       I lead, architected and helped developing deep learning, machine learning and Convolutional Neural Networks (CNN) systems for 5G wireless data anomaly detection, system availability, Hack attack, Security breach, data breach detection and protection. For peak performance a distributed architecture were created using 16,000 CPUs with amazing results using Spark, HDFS, Scala, SBT, MLlib, TensorFlow, Keras. Some of the development were based on Multilayer perceptron classifier (MLPC) which is a classifier based on the feedforward artificial neural network. Also created other prototypes using Python, R, PySpark and Scala libraries like scikit-learn, Pandas, Deeplearning4j, Sparkling Water ML, Caffe2, MxNet etc. Different algorithms were used like K-Means, Random Forest, Gradient Boosting algorithms (GBM, XGBoost, XGBoost and CatBoost). Also used GPUs especially with Convolutional Neural Networks CNNs with TensorFlow, Keras.  ·       I used TensorFlow, Keras and create similar to YOLO type Convolutional Neural Network on GPUs for detection of different type of anomalies on wireless data. I used the conversion of data to pictures and run it through the Neutral network with amazing results.  ·       I used Acumos AI platform for certain type of ML and CNN projects with cascading Convolutional Neural Networks. I attempted to create an Artificial Intelligence AI environment to facilitate cascading Convolutional Neural Network using.  ·       Created Data Pipeline with real time, intermediate and permanent repositories on Kubernetes. By using huge Kafka clusters with huge partitions inside Kubernetes, the data was gathered in a Round-robin fashion from variety of sources including real hardware, routers, radios, etc. and was brought into the real time repositories. Using Scala SBT Python Jupyter Zeppelin, Spark, Kafka, NiFi, ELK, Kubeflow, Kube-scheduler etc. the capacity of the Data Pipeline was 150TB /Day but could be easily extended by simple Kubernetes Orchestration scripts.  ·       Created a Petabytes Data Lake with Raw Data Landing Zone, Processing Zone and Consumption Zone. The Data Lake was specifically designed for ease of use for Artificial Intelligence and therefore the Data Scientist were able to directly access different type of data from real time to intermediate to permanent Data from the Data Lake’s Consumption Zone for different type of anomaly detection. The processing in the Data Lake was using Delta and Flip to ensure that the data is accessible at any time even during the processing.   ·       Azure Cloud, I architected, led and did actual implementation of massive Azure Cloud project with extensive full cycle Cloud Azure experience covering the Data Ingestion, Data Transformation and Data Consumption with Machine Learning Deep Learning. This was a massive project on a large scale on Azure which covered a full range of areas including but not limited to Big Data, Machine Learning Deep Learning, Azure Machine Learning Studio, Azure Power BI, Azure Search, and Elasticsearch on Azure development and deployment. Comprehensive deployments of massive Azure infrastructures from inside the Visual Studio 2017 using ARM Templates. Deployment of many Azure projects for different companies and here are some of practical!! Hands on!! Component that I have personally used, Azure Spark Hive HBase Hadoop Kafka Storm ML Services (R Server) HDInsight clusters, Domain-joined HDInsight clusters,, Azure Zeppelin notebooks, Azure Jupyter notebooks, Azure SQL Data Warehouses, Azure Databricks, Azure Data Lake, Azure Data Lake Factory, Azure Data Lake Storage, Azure Data Lake Analytics, Azure Data Links, Azure Integration Runtime (IR), Azure Data Gateway, Azure Kubernetes Services, Azure Storage Blobs, Azure Active Directory, Azure Service Principals, Azure Security Center, Azure Key Vaults, Azure Virtual Network vnet, Azure Log Analytics, Azure Network Interfaces, Azure Cosmos DB, Azure Cortana Intelligence Suite, Infrastructure as a Service IaaS, Platform as a Service PaaS, Microsoft R Server, NLB, Key phrase extraction Azure search, Unstructured text analytics, Event hub, Streaming, Poly Base etc..  ·       The Big Data part used Hadoop, Spark on mainly MapR but also Cloudera and Hortonworks, Storm, KafKa, Hive, Pig, Impala, Flume, Sqoop, MapReduce, Pig, HBase, NiFi, oozie, Tableau Power BI and Cloudera visualization, QlikView etc.  ·       Development languages, Extensive Scala SBT (My Preference), Java Maven, Eclipse Intellij (my preference), Python, R, Jupyter, PySpark, Ruby and even some, Linux Shell Script, Shell Scripts Senior Solution Architect Consultant, senior developer, Azure Big Data, ETL, various databases, Data Warehousing, Spark, Databricks, HDInsight, BI, advanced Machine Learning Deep Learning. I implemented, architected and a massive Azure Cloud infrastructure with big data, machine learning, deep learning, and Artificial Intelligence Neural Network Microsoft - New York, NY May 2018 to August 2018 Senior Solution Architect Consultant, senior developer, Azure Big Data, ETL, various databases, Data Warehousing, Spark, Databricks, HDInsight, BI, advanced Machine Learning Deep Learning. I implemented, architected and a massive Azure Cloud infrastructure with big data, machine learning, deep learning, and Artificial Intelligence Neural Network     This was a project with Microsoft and I worked as Microsoft expert on Azure in New York, other companies involved were Pragmatic Works and Selective in New York. I was involved from the designing board all the way to complete implementation and production release. Azure despite simplicity has enormous amount of details. Many engineers that I interviewed and some of whom that I worked with, may have known bits and pieces but actually creating clusters and implementing systems on Azure require substantial experience and know how, I have that.  ·       Azure Cloud, I architected, led and did actual implementation of massive Azure Cloud project with extensive full cycle Cloud Azure experience covering the Data Ingestion, Data Transformation and Data Consumption with Machine Learning Deep Learning. This was a massive project on a large scale on Azure which covered a full range of areas including but not limited to Big Data, Machine Learning Deep Learning, Azure Machine Learning Studio, Azure Power BI, Azure Search, and Elasticsearch on Azure development and deployment. Comprehensive deployments of massive Azure infrastructures from inside the Visual Studio 2017 using ARM Templates. Deployment of many Azure projects for different companies and here are some of practical!! Hands on!! Component that I have personally used, Azure Spark Hive HBase Hadoop Kafka Storm ML Services (R Server) HDInsight clusters, Domain-joined HDInsight clusters, Azure Zeppelin notebooks, Azure Jupyter notebooks, Azure SQL Data Warehouses, Azure Databricks, Azure Data Lake, Azure Data Lake Factory, Azure Data Lake Storage, Azure Data Lake Analytics, Azure Data Links, Azure Integration Runtime (IR), Azure Data Gateway, Azure Kubernetes Services, Azure Storage Blobs, Azure Active Directory, Azure Service Principals, Azure Security Center, Azure Key Vaults, Azure Virtual Network vnet, Azure Log Analytics, Azure Network Interfaces, Azure Cosmos DB, Azure Cortana Intelligence Suite, Infrastructure as a Service IaaS, Platform as a Service PaaS, Microsoft R Server, NLB, Key phrase extraction Azure search, Unstructured text analytics, Event hub, Streaming, Poly Base etc. Senior lead, Senior Solution Architect Consultant, senior developer, Big Data, Data Engineer, Data Scientist, Data Warehousing, Spark, HDFS, BI, Kubernetes, IBM Cloud and IBM Cloud Private (ICP), advanced Machine Learning Deep Learning on the cutting edge of the Genome, Berkeley Amplab Adam Genomics, GATK. Elasticsearch SOLR. Also implemented and deployed a massive Azure Cloud infrastructure with big data, machine learning, deep learning, and artificial intelligence convolutional neural network CNN. Optum United Healthcare - Santa Ana, CA August 2016 to March 2018 In a course of one year, I have architected and lead some of the most sophisticated Big Data, Deep learning and Machine Learning, Azure Cloud projects in the nation for Optum in Minneapolis and California. I have had access and utilized thousands of servers, 12,000 CPUs, enormous amount of memories and the results has been astonishing beyond even my own and everybody’s expectations. Examples:    ·       Genomic analysis for prediction of various cancers on 3000 known samples from our genomic bank, 12,000 CPUs, enormous amount of memories the processing time were reduced from 46.7 years to 22 minutes 47 seconds! with 99% prediction accuracy, this was recently presented at a conference.  ·       4 billion records with 53 pre and post processing queries, reduced from days to under a minute!    Please note that due to the highly proprietary and sensitive nature of these projects, I will not be able nor will I disclose the technical details.    ·       I lead, architected and helped developing deep learning and machine learning systems for genetic analysis and prediction system for occurring of different type of cancers with more than 99% accuracy. For peak performance a distributed architecture were architected using 12000 CPUs with amazing results. Spark, HDFS, Scala, SBT, MLlib, TensorFlow, Keras. Some of the development were based on Multilayer perceptron classifier (MLPC) which is a classifier based on the feedforward artificial neural network. Also created other prototypes using Python, R, PySpark and Scala libraries like scikit-learn, Pandas, Deeplearning4j, Sparkling Water ML, Caffe2, MxNet etc. Different algorithms were used like K-Means, Random Forest, Gradient Boosting algorithms (GBM, XGBoost, XGBoost and CatBoost)  ·       For hardware processing of the Big Data, the Deep Learning and Machine Learning different architectures were tested on IBM Neteeza, Teradata and distributed architecture with 12000 CPUs and memory (Spark). The result were absolutely clear, there is no comparison the distributed architecture is far more superior and is the future!  ·       The Big Data part used Hadoop, Spark on mainly MapR but also Cloudera and Hortonworks, Storm, KafKa, Hive, Pig, Impala, Flume, Sqoop, MapReduce, Pig, HBase, NiFi, oozie, Tableau Power BI and Cloudera visualization, QlikView etc.  ·       I lead, architected and helped developing a hybrid system of Elasticsearch and HDFS using Logstash, Rsync and Kafka. The Elasticsearch was sharded over 25 nodes but later deployed on AWS. Three type of data were indexed and inputted into the Elasticsearch (Kibana):  o  The hardware and system logs for real time (Kafka) hardware and system monitoring  o  The patient and claim data from HDFS, Hive and HBase for search and quick BI visualization in Kibana.  o  Data Export files from MarkLogic   o  For basic search used by different system via an API on top of the Elasticsearch  ·       I lead, architected and helped developing a massive data ingest system from different providers with a permanent and real time pipeline (Kafka) using Delta and Flip methods for an ongoing uninterrupted data ingest. The source data were RDBMS, Hive, HBase, MarkLogic, flat data and log files etc.  ·       Azure Cloud, I architected, led and did actual implementation of massive Azure Cloud project with extensive full cycle Cloud Azure experience covering the Data Ingestion, Data Transformation and Data Consumption with Machine Learning Deep Learning. This was a massive project on a large scale on Azure which covered a full range of areas including but not limited to Big Data, Machine Learning Deep Learning, Azure Machine Learning Studio, Azure Power BI, Azure Search, and Elasticsearch on Azure development and deployment. Comprehensive deployments of massive Azure infrastructures from inside the Visual Studio 2017 using ARM Templates. Deployment of many Azure projects for different companies and here are some of practical!! Hands on!! Component that I have personally used, Azure Spark Hive HBase Hadoop Kafka Storm ML Services (R Server) HDInsight clusters, Domain-joined HDInsight clusters,, Azure Zeppelin notebooks, Azure Jupyter notebooks, Azure SQL Data Warehouses, Azure Databricks, Azure Data Lake, Azure Data Lake Factory, Azure Data Lake Storage, Azure Data Lake Analytics, Azure Data Links, Azure Integration Runtime (IR), Azure Data Gateway, Azure Kubernetes Services, Azure Storage Blobs, Azure Active Directory, Azure Service Principals, Azure Security Center, Azure Key Vaults, Azure Virtual Network vnet, Azure Log Analytics, Azure Network Interfaces, Azure Cosmos DB, Azure Cortana Intelligence Suite, Infrastructure as a Service IaaS, Platform as a Service PaaS, Microsoft R Server, NLB, Key phrase extraction Azure search, Unstructured text analytics, Event hub, Streaming, Poly Base etc..  ·       Genome Analysis Toolkit 4 (GATK4) from Broad Institute, ADAM Genomics Berkeley AMPLab, BAM, SAM, VCF genome variant. Worked also with mango, gnocchi, deca, avocado, quinine, cannoli etc  ·       Using SOLR / Elasticsearch created a detail analytical graphical dashboard in Kibana for Patient Data, Claim Data and Provider Data.  • I architected and led multiple AWS projects, Redshift, RDS, EMR, Kinesis, S3, Glue, DMS, Athena, EC2, Lambda, with full Big Data, Amazon Elastic MapReduce (EMR), Hadoop, Spark, Hive, Pig, Kafka, AWS Management Console, AWS CLI, Amazon EMR File System (EMRFS), collaborative notebooks Apache Zeppelin, Jupyter, deep learning frameworks like Apache MXNet, Elasticsearch and SOLR, Machine Learning and Deep Learning development and deployment. AWS Compute E2C, FarGate, Lambda, VMware, AWS Developer Tools, AWS Management Tools, Amazon Machine Learning, AWS DeepLens, Amazon Deep Learning AIMs, Amazon TensorFlow on AWS and other components. etc.  • Development languages, Extensive Scala SBT (My Preference), Java Maven, Eclipse Intellij (my preference), Python, R, Jupyter, PySpark, Ruby and even some, Linux Shell Script, Shell Scripts  ·       Successfully installed and deployed an entire IBM Cloud Private ICP Cluster then implemented and deployed ELK Elasticsearch, Logstash, Kibana, Filebeat, Kafka, Zookeeper, Cassandra, Curator on ICP IBM Private Cloud, Kubernetes, Pods using Helm Charts, Scala SBT. IBM Cloud and IBM Cloud Private (ICP) is a Distributed Container based Architecture, Docker, Docker CLI, Kubernetes, Kubernetes CLI, Pods, Pods deployments. Service Deployments, Ingress, Helm Charts, Helm Charts CLI.  Senior lead, Senior Solution Architect Consultant OneStop - El Segundo, CA February 2016 to August 2016 Senior lead, Senior Solution Architect Consultant, senior developer, Big Data, Data Warehousing, BI, SOLR, Lucene, Elasticsearch, Mahout, Weka Machine Learning Lead.  I was initially hired at OneStop because of similar experience I had from Dell and Microsoft in Big Data, SOLR Elasticsearch and Machine Learning, taxonomy etc.  • I lead, architected and helped developing a Big Data system with 256 nodes using Hadoop, Spark on Hortonworks and later migrated to Cloudera, Storm, KafKa, Hive, Pig, Impala, Flume, Sqoop, MapReduce, Pig, HBase, oozie, Tableau Power BI and Cloudera visualization, QlikView  • Development languages, extensive Scala SBT (my preference), Java Maven, Eclipse Intellij, Python, R, PySpark, Ruby, C#, Unix Shell Scripts, Linux Shell Scripts  • I lead, architected and help developing the SOLR/Lucene Search system which was later migrated to Elasticsearch with 15 nodes sharding. The system was initially developed and maintained in house but later deployed to AWS cloud and prototyped on Azure. The development and test of the 15 node done on VirtualBox machines, physical machines before deployment to the Cloud. The SOLR development was done in two different phases, initially we did indexing directly on top of the metadata extracted from various files with Apache Tika, Apache Flume and scoop. I wrote a scheduler in Java that run delta indexing periodically every few hours. We had customized faceting and then the API would grab the top N results from the XML. The search worked better than expected, the indexing was slow but the search was extremely fast in fraction of a second. On the second phase we stored all the raw documents in HDFS and create indexing and then use HBase to store the index files in HDFS. Also an API was developed in JAVA with a .NET wrapper with SOLR search calls into the SOLR engine.  • I lead, architected and help developing an advanced Machine Learning system initially in Spark MLlib then Weka and ultimately a Mahout Machine learning and recommendation system using both ItemSimilarity and UserNeighborhood. I personally favored and created porotypes using Spark MLlib, TensorFlow, Keras, Python libraries like scikit-learn, Pandas but in this case Mahout worked very well. The .NET API would record every time a product was clicked or purchased. The data was recorded in the database and then the metadata was created and the mahout would create a scoring table (0-10) for product and region. The .NET API would select top N highest score and would present it as recommendations.  • I lead, architected and helped developing a gigantic amount of data extraction, data warehousing, Big Data. The data was gathered in access of tens of terabytes from more than 40 top of the lines brands Ecommerce sites partnered and operated by OneStop like FRYE, Juicy couture, NYDJ, PAIGE, Splendid, Coffee Beans, Jones New York, Hudson and many more. Used SSIS ETL for SQL to port data to the Data Warehouse and then used Sqoop for extracting from RDBMS to the HDFS, used Flume for extracting from logs files, FTP NAS files to the HDFS, used Apache Tika and Java for extracting metadata from various files into the HDFS, used Nutch for web crawling and for extraction metadata into the HDFS, used SAPI, CMU Sphinx, Kaldi for customer service voice to text conversion into the HDFS  • I architected and implemented a real time and streaming component for the Cloudera visualization using Apache Strom and Apache Kafka.  • I lead, architected and help developing an elaborate real time visualization using Tableau and Cloudera visualization for the big data portion.  • The Big data prototype was deployed both on AWS and Azure. For a number reasons the final decision for the cloud deployment was made for deployment into the AWS not Azure.  • I lead and oversaw the conversion of part of the SOLR search project to Elasticsearch and benchmarked the performance. Although I liked working with JASON for various reason SOLR was preferred initially.  • I lead, architected and help developing Kibana 4.5 visualization on top of both SOLR and Elasticsearch.  • Wrote and oversaw a development of combination of batch files, python and Ruby scripts for SOLR/Lucene and Big Data deployment and configurations. I have to add that I started the conversion of batch file to Python but there were simply not enough time.  • Did extensive prototyping and benchmarking and helped evaluating the performance of the big data on Massively Parallel Processing (MPP) and other Data Warehouse Appliances such as IBM Netezza, Teradata, APS (PDW), Oracle Exadata Senior Big Data, DW and BI Lead Solution Architect Consultant Canadian Tire, California - Toronto, ON August 2015 to February 2016 • Led multiple large scale Big Data, Enterprise Data Warehouse EDW and Business Intelligence BI projects on Teradata utilizing Spark, Hadoop, Hortonworks, Cloudera, Hive, Impala, Flume, Sqoop, Map Reduce, Pig, HDInsight, HBase, oozie, and facilitating the real-time data analysis by the data scientist.  • Led multiple EDW projects, prototyped and evaluated their performance on the Massively Parallel Processing (MPP), other Data Warehouse Appliances such as IBM Netezza, Teradata, APS (PDW), Oracle Exadata  • Development languages, extensive Scala SBT (my preference), Java Maven, Eclipse Intellij, Python, R, PySpark, Ruby, C#, Unix Shell Script, Linux Shell Scripts  • Leading the team, I designed architected and implemented the migrating from legacy information warehouse to a modern high performance Big Data and Data Warehouse running on multiple DW appliances. Drafted a BI/DW prioritized implementation roadmap working with the business and finance department.  • Leading the team we migrated and deployed 5 projects to Azure Cloud. I was personally involved in the full cycle of vendor selection, requirement gathering, design, development and the deployment of these projects. The migration included different aspects of the projects from front, backend, and integration. We went through thorough research before selecting the Azure cloud for this project and also utilized cutting edge utilities to perform the migration and deployment.  • Drafted a BI/DW prioritized implementation roadmap while taking input from internal divisional service plans, business and IT strategy documentation, as well as corporate BI Strategy and the Financial Planning and Reporting System  • Designed Enterprise Information Management (EIM) solutions for retail operation. Led technical teams and designed various BI solutions including loyalty programs, card management, POS data management, customer behavioral analysis, store dashboards, finance, ecommerce, cyber security analytic.  • Defined the data governance strategy, designed security patterns, implemented data standards and procedures across the enterprise; drafted business specific methodology to establish business stakeholder-driven data stewardship through MDM  • Conducted BI maturity assessment of the organization. Architected DW&BI Program Structure, defined the role of DW&BI Program Steering Committee, it's mission, objectives, roles and responsibilities, monitored regular improvements to help manage risks, evaluate trends, and develop capacity and capability to achieve the Program mission Senior Big Data, DW and BI Lead Solution Architect Consultant, Java Android NovaWurks/DARPA - Los Alamitos, CA November 2014 to August 2015 consultant  Worked as senior Big Data Solution Architect, team leader and core developer on PHOENIX project, an advanced satellite system for DARPA (Defense Advanced Research Projects Agency), a network of small satellites due to launch to orbit in 2015. Due to the sensitivity cannot go into too much details!  • Led several Big Data projects on massive amount of transmitted and logged data from the satellite network to the ground station. These projects were developed utilizing Cloudera, Hadoop, Spark, Hive, Impala, Flume, Sqoop, Storm, Pig, HDInsight, HBase, oozie. Due to the real time nature of the project Apache Storm and Apache Kafka was used for handling of the streaming and the real time data feed.  • I led the team, designed, architected and implemented an elaborate Data Warehouse and Data Mart using Dimensional Modelling Star Schema for satellite data aggregation, data storage, data log, real time operation status data and other needs.  • Utilized the Cloudera Visualizations, Dashboards, and Reports to monitor the operation of the satellites and any warning issues due to any errors, miss functions or failures. Other visualization tools were also created using Java and Android.  • Led the team developed multiple real time Android Apps and middleware using Android Studio and Eclipse, Android SDK and Java, RESTful APIs, Retrofit, GSON, JSON, Regex, JGroups IP Multicast, Apache Thrift, Python. Also used the following technologies and systems, Xilinx FPGA, Verilog, TI DSP, ARM® Cortex®-A9 Cores: i.MX 6 Series Multicore Processors etc. Senior Big Data, DW and BI Lead Solution Architect, .NET Architect Consultant Paramit - Morgan Hill, CA July 2013 to November 2014 Led multiple large scale Big Data, Enterprise Data Warehouse EDW and Business Intelligence BI projects utilizing, Hadoop, Cloudera, Hive, Impala, Flume, Sqoop, Map Reduce, Pig, HDInsight, HBase, oozie, and facilitating real-time data analysis by data scientist.  • Leading the team, I designed architected and implemented the migration from legacy normalized SQL, FoxPro, medical device manufacturing, ERP, MRP, CRM, sales, finance and other information warehouses to a consolidated modern high performance Big Data Warehouses running on multiple DW appliances.  • Leading the team we migrated and deployed multiple projects to Azure Cloud. I was personally involved in the full cycle of vendor selection, requirement gathering, design, development and the deployment of these projects. The migration included different aspects of the projects from front, backend, and integration. We went through thorough research before selecting the Azure cloud for this project and also utilized cutting edge utilities to perform the migration and deployment.  • Using a combination of WPF C# application GUI and the Cloudera Visualizations, Dashboards, and Reports created advanced data visualization and data entry tools for ERP, MRP, CRM, sales, finance and other departments.  • I lead, architected and help developing a SOLR/Lucene Search for the huge amount of ERP, MRP and CRM. The SOLR project was later converted to Elasticsearch. The Elasticsearch /Lucene system was architected with 5 nodes sharding. It was developed and tested on 5 node VirtualBox machines and then deployed to AWS cloud. Created an API in C# .NET for calls to the search engine. Also a GUI was developed in C# .NET for search calls to the Elasticsearch.  • Developed a customized SOLR indexing scheduler in C# which would run periodically to do the delta indexing.  • Drafted a BI/DW prioritized implementation roadmap while taking input from internal divisional service plans, business and IT strategy documentation, as well as corporate BI Strategy and the Financial Planning and Reporting System  • Designed Enterprise Information Management (EIM) solutions for the manufacturing process, customer support and retail operation. Led technical teams and designed various BI solutions including medical device manufacturing tracking process, component reliability analysis, vendor analysis, customer behavioral analysis, finance, ecommerce, cyber security analytic.  • Conducted BI maturity assessment of the organization. Architected DW&BI Program Structure, defined the role of DW&BI Program Steering Committee, it's mission, objectives, roles and responsibilities, monitored regular improvements to help manage risks, evaluate trends, and develop capacity and capability to achieve the Program mission  • Led the team and developed multiple applications including medical device, ERP, MRP applications with big data architecture. Used NET 4.5, C#, WPF, WCF, WF, MVVM Light, Telrik, MVC 4 Razor Entity Framework 6.0 TFS, SQL 2012. Senior Big Data, DW and BI Lead Solution Architect, .NET Architect Consultant Microsoft - Redmond, WA February 2013 to July 2013 Led multiple Azure Cloud Big Data, NoSQL Riak, MongoDB SIP Trunk VOIP projects doing analysis on massive amount of voice to text converted data utilizing Hadoop/HDInsight, PDW, Map/Reduce jobs, Hive, and Sqoop.  • Created real time multithreaded C# code using C++ Dubango Library, SIP, TCP, UDP, RTP the VOIP telephony voice was recorded and using SAPI converted to text. The text was then stored into key value and document tables using Riak and MangoDB. The voice data gathered from Cisco/IPCC telephone systems. Integrated with Cisco Verint for VOIP call recording, quality monitoring (QM), and speech analytics.  • Microsoft SQL Server Parallel Data Warehouse (SQL Server PDW) was chosen as the main appliance for the Big Data processing due to its Massively Parallel Processing (MPP) architecture designed for Big Data Processing.  • Microsoft Power BI in conjunction with a .NET application is used for data visualization.  • Led the design and development of the Workforce Management (WFM) data warehouse and BI solution to optimize adherence and attendance in the contact center. The predictive analytic component accurately forecasts the number of CSRs needed in the call center to fulfill the services.  • Led the design and development of an efficient BI auditing framework that collects the data from packages being executed and used in data flows, row counters, versioning, and error handling. The framework is crucial for monitoring, timing, troubleshooting, and auditing. Also, developed Stored Procedures, Views, and Functions for the framework to automate logging the information and error handling in the packages.  • Led the design and development of ETL processes and data mapping using SQL server, Master Data Services (MDS), SSIS to extract data from Lagan ECM and division data sources including SQL server and oracle databases, flat files, and excel sheets. The data, then, is transformed and loaded into a data warehouse for reporting.  • Led the design and development of data quality ETL packages to correct and cleanse the data and enhance the quality of consolidated data. Wrote hundreds lines of .NET C# code, embedded in the packages, to create a rules engine that loads business rules and apply them to the data efficiently. In addition, the data quality issues are mapped for reporting purposes.  • Led the design and development of a SQL Server SSAS Analysis cube utilizing star schema with complex MDX calculated measures, named sets and KPIs to present an analytical view for the data and data quality with multiple dimensions.  • Led the design and development of map application and report using ASP.NET/C# web application. The application loads the data from the data warehouse, combines it with geographical information, and displays the data on a map. The application communicates through restful mapping services and uses client side scripts (JavaScript and AJAX) to improve performance and user experience. Senior Big Data, DW and BI Lead Solution Architect, .NET Architect Consultant Dell - Austin, TX October 2012 to February 2013 Led a Big Data project on gigantic amount of taxonomy data and customer portfolio using Hadoop, Cloudera, Hive, Map Reduce, Pig, HDInsight, and facilitating real-time data which was both analyzed and also in real time restructured the Dell website on the demographic portfolio of the customers.  • I architected, worked and help developing the SOLR/Lucene Search deployed to Azure. The indexing was done directly on top of the metadata extracted from various files with customized Java code and Apache Tika. Used customized faceting to overwrite the default search criteria.  • Developed a customized SOLR indexing scheduler in C# which would run periodically to do delta indexing.  • Wrote variation of batch files, python for SOLR/Lucene deployment and configurations  • Leading the team, we designed architected and implemented the migrating from legacy normalized SQL taxonomy data, customer portfolio data and other data to a modern high performance Big Data Warehouses running on multiple DW appliances.  • Defined the data governance strategy, designed security patterns, implemented data standards and procedures across the enterprise; drafted business specific methodology to establish business stakeholder-driven data stewardship through MDM  • Led multiple EDW projects, prototyped and evaluated the performance on Azure cloud, AWS Amazon Cloud, Massively Parallel Processing (MPP) Data Warehouse Appliance  • I wrote complicated taxonomy algorithm in C# to load, sort the taxonomy data into huge multidimensional trees on the memory which made the data processing supper fast.  • Created Taxonomy data visualization using the Cloudera Visualizations, Dashboards, and Reports to monitor customer profile, demography and other useful data. Other visualization tools were also created using C#.  • Created data quality ETL packages to correct and cleanse the taxonomy data and enhance the quality of consolidated data. The consolidated taxonomy data then were segmented using Hadoop and Cloudera.  • Led the design and development of a SQL Server SSAS Analysis cube utilizing star schema with complex MDX calculated measures, named sets and KPIs to present an analytical view for the data and data quality with multiple dimensions.  • Leading the team we migrated and deployed multiple projects to Azure Cloud. I was involved in the full cycle of vendor selection, requirement gathering, design, development and the deployment of these projects. The migration included different aspects of the projects from front, backend, and integration.  • In conjunction with the Big Data I was involved in multiple projects using variety of technologies including MVC 4 Razor, WPF, WF, WCF, TPL, LINQ, SQL 2012, jQuery, Android, Java, J2EE, JRE, Ajax, AngularJS, ExtJS, Entity Framework 5.0,.NET 4.5 Team Leader, .NET Architect, Hands on Developer Consultant Laguna Niguel, CA June 2012 to October 2012 Worked as an architect, team leader, and core developer on multiple advanced projects in Silverlight/ WPF C# ASP .NET, MVVM, Java, J2EE, J2SE, Ajax, AngularJS, LINQ, WCF RIA SQL, SSRS, Hibernate, Telerik. Team Leader, .NET Architect, Hands on Developer Consultant BEW / General Electric - San Ramon, CA June 2011 to June 2012 Worked as a system architect, core developer on a sophisticated control system for generators and wind turbines lead the software (WPF), hardware (Xilinx FPGA & TI DSP 6000) and firmware (C++ Verilog/VHDL) teams.  • The high level software controlled a network of generators via TCP/IP. The WPF C# project was architected using MVVM light, Entity Framework, LINQ, WCF Services SQL etc. The Silverlight ASP .NET project was architected using MVVM light, Entity Framework, LINQ, WCF RIA Services Domain Service/Context. Developed equivalent Android application for reading the generator's parameters like RPM, temperature, sensor Voltages etc. Used Java programming and the Android Software Development Kit, Eclipse using the Android Development Tools (ADT) Plugin. Also worked on the firmware and FPGA DSP TMS320C6713 TMS320F28335 EMIF, I2C, MCBS, GPIO, RTC UART, Anybus CANbus, DM9000, second level bootloader, EEPROM, code composer 3.3 etc FPGA Xilinx Spartan 6, Xilinx ISE Design Suite 13.2, Verilog and VHDL. Team Leader, .NET Architect, Hands on Developer Consultant Texas Instruments, Dallas Texas - Dallas, TX January 2011 to June 2011 Worked as the main architect, team leader, and core developer on a scientific highly multithreaded WPF C# application for emulation and design of advanced communication chips using scientific algorithms. I also worked on an Android application for the PLL, Java programming using the Android Software Development Kit, Eclipse using the Android Development Tools (ADT) Plugin. The WPF application was architected using propriety MVVM architecture. Utilizing advanced 3D objects the application was similar to OrCad and AutoCad. A smaller prototype version was also developed in Silverlight. Team Leader, Embedded Architect, Hands on Developer Consultant Puresense - Oakland, CA November 2010 to February 2011 Firmware/Hardware wireless &Satellite Communication modules. Developed an Android application for irrigation, Java programming using the Android Software Development Kit, Eclipse using the Android Development Tools (ADT) Plugin. Linux Ubuntu, Freescale i.MX31, C++ with multithreading. Enfora GSM 1308, Legacy system OWL2pe, Basic Stamp 2pe, Alan Bradley, Siemens PLC. Team Leader, .NET Architect, Hands on Developer Consultant Direct Response Medicine (DRM) - Temecula, CA March 2009 to September 2010 Worked as the main architect, team leader, core developer on a major medical device - FDA project HW & SW Shown in CDS Chicago with huge success. SW: C#, NET, WPF, WCF, WF, MVVM, Silverlight, NetTcp etc. Team Leader, Architect, Hands on Developer Consultant Multibeam Corporation - Santa Clara, CA July 2008 to March 2009 Worked as the project lead, helped designing, architecting, and implementing a revolutionary complex electron beam based instrument for the next generations of semiconductor fabs. Advanced analog digital boards, Embedded Linux, Xilinx & Altera FPGA, Quartus, NIOS, ARM9, ARM11, C, C# .NET, WPF (MVVM), WCF etc. DSP TMS320C67x GPIO, RTC UART, Modbus, DM9000, second level bootloader, EEPROM. Altera FPGA, Stratix, Cyclone Series, Quartus II Nios II. Team Leader, Architect, Hands on Developer Consultant Department of Defense Contract (DOD) - Washington, DC August 2007 to July 2008 Architected, developed and led a highly sophisticated hardware/firmware/software system. Due to the classified nature of the project, I can only provide the following generic information: The project involved advanced radio scanners, signal generators using GPS, WCDMA, CDMA, GSM and other systems and protocols. The software application controlling the instruments was a multi-tiered application written in C#, .NET, Visual C++, MFC, CLR, Embedded Linux. It utilized a very advanced multi-threading architecture with sophisticated synchronization, message handling, logging system, serialization etc. Specialized algorithms were devised to speed up the real-time performance of hardware/software. Again because of the defense-related nature of this project I cannot reveal any more details. Team Leader, Architect, Hands on Developer Consultant SpectraSensors Inc - Rancho Cucamonga, CA March 2007 to August 2007 Led and developed an advanced Hardware and Firmware project using two boards with HC12, PC/104, ARM9 connected by RS232. One board Embedded Linux, the other board C++ Round Robin. The instrument was successfully shown at the show in Oklahoma oil and gas show. A Modbus protocol was also implemented. The instruments was uses Tunable Diode Lasers (TDL) in conjunction with Absorption Spectroscopy. Vice President Patton Design - Irvine, CA May 2005 to March 2007 Worked as the vice president of software and hardware. I led and developed the software/hardware for a $140,000 instrument medical device - FDA. Please check the website of Patton Design and Busch & Lomb to see this award winning instrument for cataract surgery. I designed, architected, led the team and developed the software and also directed the hardware and firmware developments. The software included a sophisticated multithreading architecture, RS232 and TCP/IP communications, managed wrapper for firmware calls, video streaming, voice recognition, database hierarchy encryption etc. In addition to leading the team and acting as the vice president, I personally wrote the complex core components in C# .NET. Due to the large scale of the medical device - FDA projects with hundreds of screens many of the .NET C# libraries and objects had to be used. We also used legacy unmanaged code inside the managed code (wrappers). DirecX, DirectShow, Windows Communication foundation WCF, Windows Presentation Foundation WPF, WF, Silverlight, WCSF, SCSF, Enterprise Library, animation, video, audio etc were also used.  In addition to the main control application I wrote and oversaw the firmware in C++ Embedded Linux, C++ Round Robin, CodeWarrior. I also oversaw and participated in the hardware development using OrCad 10.    * Patton Design / Cameron Health: Developed the software and participated in the hardware design of the heart pacemaker medical device - FDA and the controller called Q-TECH(TM) Programmer. medical device - FDA The heart pacemaker is transplanted in the heart and controlled by the wireless controller via Bluetooth. Due to the FDA regulations I could not use the .NET framework but had to use Embedded Visual C++ 4.0 and MFC for windows CE. More than 140 screens! Very sophisticated programming involving memory managements, DirectX, DirectShow etc.    * Patton Design / GoVideo: Worked as the Vice President/architect/team leader on a joint project between, GoVideo, Patton Design, Daewoo and MTK in Taiwan. I led the Patton Design team developed a TiVo style DVD/VCR combo with hard drive recording capability. I was the vice president and the team coordinator between the 4 companies overseeing hardware, software and Firmware (Embedded Linux), several patents were filed. The System was presented at the CES show in Las Vegas in 2007 and received tremendous positive recognitions.    * Contract with usCalibration Inc.: Architected, developed and led a sophisticated web based application using C# .NET and Visual Studio 2005, SQL Server 2005 and SSRS. I wrote the core part of the application. The application was successfully launched in 2006 for Calibration systems with advanced security systems. Tens of thousands of lines of code with advanced navigation systems with several pending patents. Team Leader, Architect, Hands on Developer Consultant First American Capital Management - Newport Beach, CA December 2004 to May 2005 Developed a financial application in VB .NET, SQL, Crystal Report for automation of investment portfolio and account managements. Sophisticated GUI, interactive query management system, reports etc. Team Leader, Architect, Hands on Developer Consultant John - Muir, MT June 2003 to December 2004 Developed and architected an advance medical billing and tracking system for hospital, doctors and patients. The system was a state of the art automated system at the time capable of generating hundreds of reports and financial analysis. Visual Basic .NET, SQL Server, MS Access Crystal Reports. Team Leader, Architect, Hands on Developer Consultant American SkySat, California November 2000 to June 2003 Developed a client application in C# .NET and several ATL (COM+) components running under Windows (IE browser) and capable of talking to a UNIX Java Server.  * Developed an e-commerce application using Visual Studio .NET, ASP .NET, C# (C Sharp) .NET, XML, XSL and SQL. The SQL database contains 45 tables and 50 stored procedures, etc. The application is a large ecommerce application with sophisticated Shopping Cart, Wish List, Product Catalog, Discounting, Coupons, Order Tracking, Fulfillment, Affiliates, Communities, etc. It also uses advanced CMP Metadata and Queuing (MSMQ) and 12 DLLs. Demo available!  *Developed Shopping Cart, Order Tracking and other ecommerce components in VB .NET Conversion of old COM based Visual Basic to VB .NET.  *Developed a commercial smart antenna system in hardware for LEO satellite systems. It was also simulated, using advanced simulation techniques in MatLab and Visual C++ 6.0 and implemented in hardware using DSP.  *Developed an application in C# (C Sharp .NET) Using System.Web.Mail, POP3, DNS etc Team Leader, Architect, Hands on Developer Consultant Inserv e-Customer solutions July 2000 to November 2000 Developed a CRM application on the web. Used a three-tier e-commerce architecture using visual C++, COM (ATL), DCOM, ASP, Visual J++, Visual InterDev, XML and SQL. Worked as a senior developer and architect. Team Leader, Architect, Hands on Developer Consultant Hewlett Packard (HP) - Cupertino, CA April 2000 to July 2000 Worked as senior developer/ technical lead on an advanced server client based communication system for server diagnostics. The system was designed using TCP/IP and SNMP protocols for monitoring hardware sensors like thermocouples, voltage and current monitoring sensors and other hardware sensors installed on HP servers. By reading these sensors, HP was able to remotely do detailed hardware/software diagnostics of the HP servers around the globe.  Worked on hardware, software and the overall system architecture. The software had a server and client component and was written in visual C++, COM (ATL), DCOM, ASP, Visual J++, XML, SNMP, MIB, SQL and InstallShield. Team Leader, Architect, Hands on Developer Consultant Broad Logic - Milpitas, CA July 1999 to April 2000 I was brought to BroadLogic, Inc. by Paul Rudnick because of my expertise in satellite communication systems and my experience from Space Systems Loral and CyberStar. Prior to this, I had worked closely with Adaptec and Broadlogic on the development of the satellite receiver hardware while still a senior manager at Space Systems Loral.  I worked on the design and implementation of the next generation of two way satellite Express PC transceiver cards, a high speed two way satellite communication system. I designed, simulated, researched, architected and led the project for the development of an advanced two way satellite communication system (satellite Express PC transceiver cards). Audio, video transfer and high speed internet access over satellite. Using, frequency, time, phase multiplexing. TDMA, CDMA, GMSK, Conditional Access. TCP/IP, UDP, DVB, SNMP, MIB and proprietary protocols. Using OQPSK modulation implementation on the Texas Instrument DSP Chip. I have written several documents related to this system. Team Leader, Architect, Hands on Developer Consultant Hewlett Packard (HP) March 1999 to July 1999 Worked as the senior architect, technical lead and senior developer on the HP Ecommerce site which later became the foundation of the HP website for PC and servers. The web application was developed in Visual InterDev 6.0 using Active Server Pages (ASP), Microsoft E-Commerce, SQL 7.0, XML, Visual C++ 6.0 and Visual Basic 6.0, Visual J++ 6.0, COM (ATL), DCOM, JavaScript and VB Script. The web server was Microsoft Internet Information Server (IIS), Microsoft site server 3.0, with Microsoft E-Commerce edition 3.0 and FrontPage extension running under the NT Server I have written several documents related to this application. Team Leader, Architect, Hands on Developer Consultant Hewlett Packard (HP) - Cupertino, CA May 1998 to March 1999 Worked as the senior architect, technical lead and senior developer on the HP servers configuration software which later became a major component and the foundation of the HP website for PC and servers configuration. Stand alone and the web application in was developed in Visual InterDev 6.0 using Active Server Pages (ASP), Microsoft E-Commerce, SQL 6.5, Visual C++ 6.0 and Visual Basic 6.0, COM (ATL), DCOM, JavaScript and VB Script. The web server was Microsoft Internet Information Server (IIS), Microsoft site server and FrontPage extension running under the NT Server. I have written several documents related to this application. Team Leader, Architect, Hands on Developer Consultant Space Systems Loral - Mountain View, CA November 1996 to May 1998 I was brought into Space Systems Loral from Lockheed Martin by Bob Lapin to help starting the CyberStar division at the Space Systems Loral. By the time I left the CyberStar in 1998 to finish my PhD in Satellite Communication, the CyberStar division had grown to more than 100 employees. I personally interviewed majority of those people.  I was one of the main architects of the CyberStar project and oversaw the design, development and implementation of different aspects of hardware, software, firmware and the satellite communication at CyberStar.  I first established a complete satellite communication link both uplink and downlink, using 3rd party modulators, demodulators, encoders, decoders, cryptography modules, conditional access, transmitter, receivers, amplifiers, dampers, data aggregator, data parsers etc. Very soon we were able to transmit and receive from and to the satellite. We were primarily using MIB and DVB protocols initially but I was one the first who managed in 1998 to implement TCP/IP and high speed internet access over satellite using an ACK table!! (patents)  To develop the integrated transceiver hardware we started working with Adaptec and I personally was directly involved in the design and implementation of the satellite receiver card hardware using OrCad. This later led to the creation of BroadLogic from Adaptec. I was later hired by BroadLogic to continue the improvement of the two way satellite receiver / transmitter.    This project was personally very important to me and made me understand and experience the satellite communication in a very comprehensive way both theoretically and practically. It helped me to get a PhD in Low Earth Orbit Satellite Communication from the University of California, one of very few who did. I travelled extensively in both US and in Europe and came in contact with some amazing people from NASA, Lockheed Martin, BroadLogic, European Space Agency etc I wrote many documents in satellite communication during this period for Space Systems Loral. Team Leader, Architect, Hands on Developer Consultant Lockheed Martin - Milpitas, CA June 1996 to November 1996 Lockheed Martin at the time in 1996 had the most sophisticated high resolution CCDs (Farichild) in the world which were in use in a number of sensitive military applications, advanced high resolution digital satellite imaging, and few civilian applications.  Due to the classified nature of some of these projects I cannot in detail describe what I did. However I was involved in the design and development of some of these advanced and sensitive projects. I worked as a senior engineer, designing and developing, systems, hardware, firmware and software.  * Hardware: We used OrCad for designing analog and digital circuits, filters, amplifiers, Data collectors from CCDs, interfaces etc.  *Firmware was written in C++ , flat file Round Robin, on Freescale HC and ARM family CPUs.  *Software: developed 32-bits, real time applications in Visual C++ 4.2 using MFC and SDKs under Windows 95 for control and testing of an advanced digital camera with high resolution CCD. The GUI software is designed for driving the special digital camera through parallel communication and testing of IPS, ADP, CCD and different part of the system. The tests included advanced image processing and image quality tests. The project involved both 16-bits and 32-bits DLLs and VXDs (device drivers), Thunking and also conversion from and between 16-bits and 32-bits. Team Leader, Architect, Hands on Developer Argonaut Technologies, Inc - San Carlos, CA April 1995 to June 1996 At the time Argonaut Technologies was attempting to develop the first true Organic Synthesizer medical device - FDA in the US. A highly complex and expensive machine (more than $200,000.00) with hundreds of valves, thermocouples, pressure transducers, heaters, agitators and a network of control boards with multiple CPUs. I was brought in as an expert and senior manager in charge of designing, architecting, developing and managing the software, firmware interfaces, and communication modules. The software was developed in Visual C++ 4.0 (32-bits) using MFC, SDK's and OCX's under Windows 95 and NT (one prototype in VC++ 16 bit). Due to the complexity of the hardware we had to implement a very sophisticated multithreading architecture. We used a combination of RS232 and our own proprietary Master-Slave communication protocol with an advanced Error Checking mechanism. I wrote several documents and gave speeches at different locations related to this project. I successfully developed the prototype and Argonaut's first commercial release. I left Argonaut because I got a great offer from Lockheed Martin.    OTHER PROJECTS  FHP, Concord, California  Team Leader, Architect, Hands on Developer Consultant.  Development of an advanced, large database application Visual Basic and Access and MS-SQL    City Building, Inc.(CBI), San Francisco  Team Leader, Architect, Hands on Developer Consultant.  Development of an advanced, large database application in Visual Basic, Access and MS-SQL.    Ericsson (Ellemtel), Stockholm, Sweden  Team Leader, Architect, Hands on Developer.  Developed and designed hardware and a control system for the new generation of AXE telephone systems, based on the FUTUREBUS+ bus technology, Using the VHDL programming language. I wrote the VHDL program on the SUN platform (SUN OS version 3.0).  I documented the application in a detailed technical white paper entitled "Verification Methods for Hardware Construction". This paper was released to all programmers and hardware engineers at Ericsson and KTH. A copy is available for your review.    ABB Atom AB, Vasteras, Sweden  Team Leader, Architect, Hands on Developer Consultant.  Electrical and Computer Lab--section SLC3:  Developed a series of utility programs / application in Quick BASIC (version 5.0) used for calibration of computer operated measurement equipment in the nuclear power plant reactors. Programs were run on the HP 9000/300, and Intel 286 platforms.    TECHNOLOGY  • Data Warehouse, Data Mart, OLAP, OLTP Databases, Teradata, Netezza, Oracle, Parallel Data Warehouse (PDW), SQL Server, MDM, MDS, Data Quality (DQ), Spark, Hadoop, Hortonworks, Cloudera, Apache KafKa, Hive, Impala, Flume, Sqoop, Map/Reduce, Pig, HDInsight, HBase, Storm, oozie, Python, Scala, HDFS, StreamInsight, PolyBase, Microsoft SSIS, SSAS, SSRS, ETL, BI, MDX, PL/SQL, TSQL, ERwin, Enterprise Architecture (EA), SQL Servere 2000/2005/2008/2012/2014, Power Query, Power Map, PowerPivot, Power View, IBM Cognos, SPSS, InfoSphere DataStage, Informatica PowerCenter, SAP BusinessObjects (BO), SAP HANA, Crystal Reports, Hyperion, MicroStrategy, SharePoint 2007/2010/2013, Nintex, SharePoint Social, Collaboration, Record Management, Search, Web forms, InfoPath, Branding, CSOM, JSOM, PerformancePoint, Clustering, Failover, Web Analytics, Google Visualization, .NET 1.1 to 4.5, C#, WCF, Restful Services, WPF/Silverlight, WF, VB .NET, ASP .NET, ADO.NET, LINQ, MVC, MVVM, MVP, AJAX, Visual Studio, Dashboard Designer, SharePoint Designer, Visio, TFS, Cloud, Azure, PaaS, SaaS, IaaS, HTML 5.0, DHTML, XML, XSL, WSDL, XSD, JSON, COM, DCOM, MFC, C, Visual C++, Visual Basic, PowerShell scripts, and SDKs, DocXpress, BI Documentation, Nintex, SharePoint Social, Collaboration, Record Management, Search, Web forms, Branding. Education PhD in Electrical and computer engineering (in Smart Antenna for Low Earth Orbit (LEO) Satellites) UCLA/University of California Davis - Davis, CA June 2003 M.S. Degree in Telecommunications Engineering. The Royal Institute Of Technology - Stockholm, SE B.S. Degree in Electrical Engineering The Royal Institute Of Technology - Stockholm, SE B.S. Degree in Computer Science. The Royal Institute Of Technology - Stockholm, SE Skills Solution Architect, Team Lead, Specialist, Developer, project manager, Vice President, doing multi-tiered applications, Big Data, Business Intelligence (BI), Data Warehousing Machine Learning, Deep Learning, System Engineering (10+ years), Big Data, Hadoop, Spark, MapR, Cloudera, Hortonworks, Storm, KafKa, Hive, Impala, Flume, Sqoop, MapReduce, Pig, HBase, NiFi, oozie, Tableau Power BI, Cloudera visualization, QlikView Scala SBT (My Preference), Java Maven (7 years), Azure Cloud Extensive full cycle Cloud Azure experience with full Big Data, Elasticsearch and SOLR development and deployment. HDInsight, Data Lake, Data Factory, Data Gateway, Machine Learning Studio, Power BI, Azure Cosmos DB, Cortana Intelligence Suite, Infrastructure as a Service IaaS, Platform as a Service PaaS, Microsoft R Server, NLB, Key phrase extraction Azure search, Unstructured text analytics, Event hub, Streaming, Poly Base (6 years), AWS Cloud Extensive full cycle Cloud AWS experience with full Big Data, Elasticsearch and SOLR development and deployment. AWS Compute E2C, FarGate, Lambda, VMware, AWS Developer Tools, AWS Management Tools, Amazon Machine Learning, AWS DeepLens, Amazon Deep Learning AIMs, Amazon TensorFlow on AWS and other components. (6 years), search engines Elasticsearch, SOLR, Lucene, Kibana. Logstash, Rsync, Tika. Also been involved with migration from SOLR to Elasticsearch for at least three companies (7 years), Machine Learning, Deep Learning and Artificial Intelligence MLlib, TensorFlow, Keras, Weka Mahout, Multilayer perceptron classifier (MLPC), the feedforward artificial neural network, scikit-learn, Pandas, Deeplearning4j, H2o, Sparkling Water ML, Caffe2, MxNet etc. Different algorithms K-Means, Random Forest, Gradient Boosting algorithms (GBM, XGBoost, XGBoost and CatBoost) etc. (7 years), .NET development, architecture and management experience in application, real time, instrumentation, web, front end, back end, full stack, multiple products out there multiple awards (10+ years), Java development, architecture, front end, back end, full stack (10+ years), Android mobile development and architecture with multiple apps in the app store (7 years), SQL 7-2016, MySQL, Oracle, and other databases T-SQL, SSIS, SSRS, SSAS, OLTP, OLAP, Multidimensional Cube, MDX, PowerPivot, Tabular Model, SharePoint, PerformancePoint. (10+ years), • Demonstrated experience and understanding of the best practices in all aspects of data warehousing (Inmon/Kimball approach). Solid experience in Data Warehouse (10+ years), • Strong knowledge and proven results in Data Warehouse and Data Mart design including Dimensional Modeling (Star & Snowflake Schemas), ER Modeling, 3 Normal Forms, Normalization and Demoralization, Logical Model and Physical Model, Fact/Dimension/Hierarchy identifications. (10+ years), • Firmware embedded programming, ARM, PIC, DSP, FPGA, RTOS Linux (10+ years), • Significant management experience including 4 years as the VP of engineering (10+ years), IBM Cloud and IBM Cloud Private (ICP) Distributed Container based Architecture, Docker, Docker CLI, Kubernetes, Kubernetes CLI, Pods, Pods deployments. Service Deployments, Ingress, Helm Charts, Helm Charts CLI. Successfully installed and deployed an entire IBM Cloud Private ICP Cluster then implemented and deployed ELK Elasticsearch, Logstash, Kibana, Filebeat, Kafka, Zookeeper, Cassandra, Curator on ICP IBM Private Cloud, Kubernetes, Pods using Helm Charts, Scala SBT. (2 years), Azure (6 years), Virtual Reality, JEE, Solution Architect (10+ years), Big Data (10+ years), Hadoop (10+ years), Presales, Java, Ldap, J2Ee, Kubernetes, • Created massive Kubernetes clusters on OpenStak, Azure, GCP, AWS with micro service Successfully installed and deployed clusters with more than 2000 nodes 16,000 CPUs deployed in 20 minutes with both Data Pipeline, Data Lake (Raw Data Landing Zone, Processing Zone, Consumption Zone) for Machine Learning, Convolutional Neural Network CNN, Spark on Kubernetes, HDFS on Kubernetes, Kafka on Kubernetes, Elasticsearch Logstash Kibana ELK on Kubernetes, NiFi on Kubernetes, Hive, HBase, kubeflow, Kube-scheduler, Jupyter Zeppelin with Scala and Python on Kubernetes. Used Vagrant Terraform HashiCorp for deployment with 2000 nodes, 16,000 CPUs with Peta Bytes and 150TB / day capacity. (4 years), WAN Additional Information SKILLS  • 18 years of solid working experience with a PhD from University of California UCLA/Davis  • 15 years of experience as Solution Architect, Team Lead, Specialist, Developer, project manager, Vice President, doing multi-tiered applications, Big Data, Business Intelligence (BI), Data Warehousing Machine Learning, Deep Learning, System Engineering etc.  • 7+ years of experience in Big Data, Hadoop, Spark, Cloudera, Hortonworks, MapR, Storm, KafKa, Hive, Impala, Flume, Sqoop, MapReduce, Pig, HBase, oozie, Tableau Power BI, Cloudera visualization, QlikView  • 6 years of Cloud Azure, AWS, , HDInsight, Cortana Intelligence Suite, Data Factory, Data Gateway, Infrastructure as a Service IaaS, Platform as a Service PaaS, Microsoft R Server, NLB, Key phrase extraction Azure search, Unstructured text analytics, Event hub, Streaming, Poly Base  • 7 years of search engines Elasticsearch, SOLR, Lucene, Kibana. Logstash, Rsync, Tika  • 7 years of Machine Learning, Deep Learning and Artificial Intelligence MLlib, TensorFlow, Keras, Weka Mahout, Multilayer perceptron classifier (MLPC), the feedforward artificial neural network, scikit-learn, Pandas, Deeplearning4j, Sparkling Water ML, Caffe2, MxNet etc. Different algorithms K-Means, Random Forest, Gradient Boosting algorithms (GBM, XGBoost, XGBoost and CatBoost) etc.  • 16 years of hands on .NET development, architecture and management experience in application, real time, instrumentation, web, front end, back end, full stack, multiple products out there multiple awards  • 16 years of hands on Java development, architecture, front end, back end, full stack  • 6 years of Android mobile development and architecture with multiple apps in the app store  • Extensive full cycle Cloud Azure experience including Big Data projects in could with multiple projects architected, developed and deployed, also prototyped in AWS and GCP  • 16 years of experience in SQL 7-2016, MySQL, Oracle, and other databases T-SQL, SSIS, SSRS, SSAS, OLTP, OLAP, Multidimensional Cube, MDX, PowerPivot, Tabular Model, SharePoint, PerformancePoint.  • Demonstrated experience and understanding of the best practices in all aspects of data warehousing (Inmon/Kimball approach). Solid experience in Data Warehouse  • Strong knowledge and proven results in Data Warehouse and Data Mart design including Dimensional Modeling (Star & Snowflake Schemas), ER Modeling, 3 Normal Forms, Normalization and Demoralization, Logical Model and Physical Model, Fact/Dimension/Hierarchy identifications.  • From Business Case to Data Visualization, I have designed and developed solutions by combining Business Process with Information Technology.  • Firmware embedded programming, ARM, PIC, DSP, FPGA, RTOS Linux  • Significant management experience including 4 years as the VP of engineering    SKILLS  • Big Data, Hadoop, Spark, Cloudera, Hortonworks, Storm, KafKa, Hive, Impala, Flume, Sqoop, MapReduce, Pig, HDInsight, HBase, oozie, Tableau Power BI and Cloudera visualization  • Cloud Azure, AWS, , HDInsight, Cortana Intelligence Suite, Data Factory, Data Gateway, Infrastructure as a Service IaaS, Platform as a Service PaaS, Microsoft R Server, NLB, Key phrase extraction Azure search, Unstructured text analytics, Event hub, Streaming, Poly Base  • Search engines Elasticsearch, SOLR, Lucene, Kibana. Logstash, Rsync, Tika  • Machine Learning and recommendation engines MLlib, TensorFlow, Keras, Weka Mahout, Multilayer perceptron classifier (MLPC), the feedforward artificial neural network, scikit-learn, Pandas, Deeplearning4j, Sparkling Water ML, Caffe2, MxNet etc. Different algorithms K-Means, Random Forest, Gradient Boosting algorithms (GBM, XGBoost, XGBoost and CatBoost)  • BI Framework: Strategy and Implementation Plans, Enterprise Metrics, Integration Points, Gap Analysis, BI Portfolio, Performance Management (PM), Analytic and PM Technologies, Defining Business and Decision Process, Building Metadata and Services Centers, Establishing Enterprise Information Management (EIM) Committees, Defining The Role of DW and BI Program Steering Committee, It's Mission, Objectives, Roles and Responsibilities, DAMA DMBOK  • Architecture and Data Modeling: Initial Conceptual Solution, Solution Blueprints, Technology Impact Analysis (TIA), Gap Analysis, Technology Roadmap, Dimensional modelling, ER Modelling, Start Schema, Snowflake, Fact, Dimension, Hierarchy, Inmon/ Kimball/ Imhoff, Data Marts, EDW, ERWin 9.5/8.0/7.x, DeZign, Microsoft Visio, Enterprise Architecture (EA), Service Oriented Architecture (SOA), UML, Zachman, TOGAF, Star & Snowflake Schemas, 3 Normal Forms, Normalization and Demoralization, Logical Model and Physical Model, Fact/Dimension/Hierarchy identifications, Data Warehouse Development Lifecycle, Data Mapping, Data Dictionaries  • Data Governance: IBM InfoShere MDM, Informatica MDM, MDS, DQS, Profisee Maestro, SAS MDM  • Integration and ETL: SSIS/SSRS/SSAS, SQL Server 2014/2012/2008R2/2008/2005, Informatica PowerCenter, DataStage, Cognos, ETL Mapping design, Data Profiling, Data Validation, Data Migration, Data Cleansing, Data Structure, Data Quality Services (DQS), BIDS, SQL Data Tools (SSDT), Auditing Framework, Execution Plans, ETL Parallel Processing, Error Handling, Custom Scripting, IBM Cognos, InfoSphere DataStage, Informatica PowerCenter, SAP BusinessObjects (BO)  • Data Warehousing and Analysis: OLAP/Cube/MDX/DAX, Dimensional Modelling, Tabular Modelling, KPIs, KPPIs, Data Analysis, SPSS, Predictive Analysis, Data Mining, Machine Learning, SAP HANA, Statistical Analysis, SAS, SAS VA (Visual Analytics), R, XLSTAT, Sentiment analysis, Speech analytics, Teradata. Netezza, Cloudera, PDW, Aginity, Master Data Services (MDS), Master Data Management (MDM), Data Quality (DQ), Analysis of Change (AOC), Metric Engine.  • Reporting: Predefined Reports, Ad-hoc Reporting, Analytical Reports, Custom Reporting with .NET/ Report Viewer, SQL Server Reporting Services (SSRS), SharePoint 2013/2010/2007/2003, PerformancePoint, PowerPivot, Power View, Crystal Reports, Hyperion, MicroStrategy, Cognos Report Studio, Framework, Workspace Advanced, DMR, TM1  • Data Visualization: Power Map, PowerPivot, Power View, SharePoint, Liferay, PerformancePoint, Google Visualization, Esri's GIS (geographic information systems) , mapping, SAP Lumira, QlikView, Tableau, Data Mapping  • Database: MS-SQL, Oracle, Oracle SQL Developer, TSQL, MDX, DMX, PL/SQL, Stored Procedure, View, Function, Erwin Data Modeler, DB2, PowerDesigner, MongoDB, Access, Excel, FoxPro, Informix, NoSQL, Big-data, Hadoop, Spark, HBase, HDInsight, PDW, PolyBase, Hive, HQL, Map/Reduce, HFS, Alert  • Programing Languages: SQL, T-SQL, PL/SQL, C#, WCF, Restful Services, WPF/Silverlight, WF, VB .NET, ASP .NET, ADO.NET, LINQ, MVC, MVVM, MVP, AJAX, HTML 5.0, DHTML, XML, XSL, WSDL, XSD, JSON, Java Script, PowerShell, COM, DCOM, VB Script, UNIX Shell Scripting  • Others: Agile, Extreme Programing, RUP, Use Cases, SDLC, TCP/IP, CVS, Microsoft Team Foundation Server (TFS), Tortoise SVN, SQL*Plus, TOAD, WinSQL, SilverLight, LightSwitch, Kerberos, Single Sign-On, Datazen, One-Key.  • Architecture and Design: Enterprise Architecture (EA), Service Oriented Architecture (SOA), Enterprise Service Bus (ESB), Top-Down/ Bottom-Up Design, Structured Design, Object Oriented Design, Multi-tiered and Multi-threaded architecture, Rational Rose, UModel, Patterns: Model/View/ViewModel (MVVM), MVC, MVP, Visio, UML, Zachman, TOGAF, Federal Enterprise Architecture, Gartner Methodology  • Business Optimization: Asset management, Information Technology Infrastructure Library (ITIL), customer satisfaction, call center management, service request enhancement, AODA compliance, fraud detection, CRM and ERP optimization, improving marketing effectiveness, portfolio optimization, governance, risk management, compliance, healthcare patient records management, electronic medical records (EMR), optimizing routes and schedules for logistics planning, insurance risk assessment, optimizing manufacturing production.  • Integration: Windows API, Biztalk, SOA, WCF, SSIS  • Data Access: ADO.NET, LINQ, Entity Framework, Microsoft Enterprise Library, OLE DB, Oracle Data Provider, MS OLAP, SQL Master Data Services (MDS), StreamInsight  • Software Development    • Methodologies: Test driven programming, Agile software development, Extreme Programming (XP) Microsoft .NET Framework (from 1.0 to 4.0), C#, Visual Basic .NET, VB .NET, ADO .NET, WinFX including Windows communication foundation (WCF), windows workflow (WF), windows presentation foundation (WPF), XAML, XML, HTML, HTML5, Java J2EE, Spring Framework, JavaScript, AJAX, RESTful services, Payment Card Industry (PCI), Image Processing  • Visual C++ (MFC, SDKs, COM, DCOM, ATL ActiveXs), VB, C++, Perl, VHDL, Verilog, Shell, Skill, Ocean, SystemC  • Scala SBT, Java Maven  • Version control tools: Source Safe, Team Foundation Version Control, (TFVC), Subversion Tortoise SVN  • Code metrics: Simian, RSM  • Type/ industry: financial, banking, biomedical, pharmaceutical, engineering, telecommunication, semiconductor, logistics, health, scientific, e-commerce, instrumental  • Internet Development: ASP .NET, MVC, Sliverlight, HTML, DHTML, Web services for marketing and financial applications, AJAX, ASP, JavaScript and VB Script, XML, Microsoft Internet Information Server (IIS), Microsoft E-Commerce, PHP, Webload  • Cloud Computing  • Windows Azure, Amazon AWS EC2  • SharePoint: SharePoint 2013/2010/2007/2003, Multi-machine SharePoint Farm Architecture, Setup, Configuration, Load Balancing, Clustering, Backup Plans, Web Part and module development, Collaboration, Social, Search, Web Content Management, Enterprise Content Management, App Management, PerformancePoint and PowerPivot, PowerView, Application Federation, Secure Store Application, Business Connectivity, Usage Reports, SharePoint Designer, Dashboard Designer, PerformancePoint, dashboard, charts, KPI, Scorecards, reports, filters, Excel Services, PowerPivot Services, Web Analytics, Static Analysis, Hit Counters, Custom Development, PowerShell, SharePoint API, Object Model, web parts web services, workflows, Content Management, site collections/structure  • Mobile Development  • Android, iOS, Windows  • Operating System Used: Windows, UNIX, Windows Azure, Linux, Android, iOS, Windows Mobile, MS-DOS  • Hardware and Simulation: Matlab, Cadence Spectre, Spice, Eldo, ANSYS  • Algorithms: Genetic algorithm, simulated annealing based algorithms, heuristic search, binary search, quick sort  • Automation and Scripting: VB, Perl, Unix Shell