Data Analyst Data Analyst Data Analyst - EPATH USA, IOWA Richmond, VA • Overall 6+years of experience in Data Analysis, Data Conversion, Data Validation, Data Profiling, UAT Testing and Report Creation and working experience in Tableau, Teradata, AWS Redshift, AWS S3, Python, Unix and Oracle.  • Experience in Teradata, SQL and Utilities like Tpump, Multiload and Fast load.  • Good experience in Developing Teradata SQL queries and using Utilities such As BTEQ  • Strong experience in using Excel and MS Access to dump the data and analyze based on business needs.  • Experience in Creating Teradata SQL scripts using OLAP functions like rank and rank () Over to improve the query performance while pulling the data from large tables.  • Experience in data analysis using Python (Pandas, NumPy)  • Worked on standard Python libraries include boto3 to connect to AWS.  • Experience in moving data to cloud platform like AWS (S3) and manipulate data using Redshift.  • Worked on performance tuning and optimization to improve the efficiency in script executions.  • Good working experience loading Data Files in AWS Environment and Performed SQL Testing on AWS redshift databases.  • Exceptional ability to research, analyze and convey complex technical information to diverse end-users at all levels. Solutions-driven strategist who consistently improves efficiency, productivity and the bottom line.  • Recognized for partnering with business leaders and technical teams to plan, integrate, document and execute complex project plans on time and on budget. Work Experience Data Analyst EPATH USA, IOWA April 2017 to Present Responsibilities:  • Involved in analysis, design and documenting business reports such as Executive summaries, Scorecards and drilldown reports.  • Interacted with Business analysts to understand data requirements to ensure high quality data is provided to the customers.  • Developed scripts using Teradata advanced techniques like Row Number and Rank Functions.  • Worked on performance tuning and Query Optimization for increasing the efficiency of the scripts.  • Extracted data from existing data source and performed ad-hoc queries by using SQL.  • Using advanced Excel features like Pivot tables and Charts for generating Graphs.  • Worked on completing the metadata documentation for the projects.  • Created monthly and quarterly business monitoring reports.  • Developed BTEQ scripts in UNIX using Putty and used cron-tab to automate the batch scripts and execute scheduled jobs in UNIX.  • Developed Python programs for manipulating the data reading from various Teradata Tables and convert them as one CSV Files, update the Content in the database tables.  • Used Python modules like Pandas and NumPy and date time to perform extensive data analysis.  • Used python to save and retrieve data files from Amazon S3 buckets.  • Performed transformations on loaded datasets using Python over the spark engine using both batch and streaming data.  • Leverage Python development environment for data analysis and report building  • Moved data from AWS S3 buckets to AWS Redshift cluster by using CLI commands.  • Performed verification and validation for accuracy of data in the monthly/quarterly reports.  • Good knowledge on Json format data and performed the source, target validations using aggregations and null validity functions.  • Created multi-set tables and volatile tables using existing tables and collected statistics on table to improve the performance.  • Supported the users of application with using multiple SQL and PL/SQL techniques.  • Designed stunning visualizations using tableau software and publishing and presenting dashboards on web and desktop platforms.  • Performed data visualization and developed presentation material using Tableau.  • Identified and fixed cause(s) of the reported issues by checking batch loading and python scripts scheduled as cron jobs.  • Implemented point of view security to Tableau dashboards to facilitate visibility across various levels of the Organization.  • Drew upon full range of Tableau platform technologies to design and implement proof of concept solutions and create advanced BI visualizations.  • Created Tableau scorecards, dashboards using stack bars, bar graphs, scattered plots, geographical maps and Gantt charts.  • Developed and reviewed SQL queries with use of joins clauses (inner, left, right) in Tableau Desktop to validate static and dynamic data for data validation.    Technical Skills: Teradata SQL Assistant, Teradata Loading utilities (BTEQ, Fast Load, MultiLoad), Spark, PL/SQL, Python, AWS Redshift, AWS S3, Tableau, MS Excel, MS Power Point. Python Developer/Data Specialist PPDI, NC January 2016 to March 2017 Responsibilities:  • Responsible for gathering requirements from business analysts and operational analysts, Identified the data sources required for the reports needed to the customers.  • Used Python programs automated the process of combining the large datasets and data files and then converting as Teradata tables for Data Analysis.  • Created an Automated Python Programs to Archive the database tables which large in size and not in use into Mainframes folders.  • Responsible for running scripts in SAS Enterprise Guide and generate Daily, Weekly and Monthly reports  • Developed programs with manipulate arrays using libraries like Numpy and Python.  • Did performance tuning and optimization for increasing the efficiency of the scripts by creating indexes, adding constrains and query optimization.  • Analyzed data from AWS Redshift database in Python using Pandas and NumPy module.  • Experience analyzing huge datasets in AWS Redshift.  • Generated graphs using MS Excel Pivot tables and creating presentations using Power Point.  • Replace VBA macros with python using PyXll, a python add-in for Microsoft Excel.  • Communicated with business users and analysts on business requirements. Gathered and documented technical and business Meta data about the data.  • Created numerous processes and flow charts to meet the business needs and interacted with business users to understand their data needs.  • Experience in Teradata, SQL and Utilities like Tpump, Multiload and Fastload.  • Experience in writing korn shell scripts for automating the jobs. Automated reports by connecting Teradata from MS Excel using ODBC.  • Flat Files brought it through Transformation and model it into human-readable form using the visualization Software tools like Tableau.  • Designed rich data visualizations to communicate complex ideas to customers or company leaders using Tableau Software.  • Performed numerous data pulling requests, manipulated and prepared data for business analyst using SQL, Unix, PL/SQL, Oracle, Excel and Access  • Documented scripts, specifications, other processes and preparation of Technical Design Documents.  Technical Skills: Teradata, Teradata utilities (SQL Assistant, BTEQ, Fast Load), PL/SQL, SAS, AWS, Python, Tableau, Agile, Excel Macros. Data Analyst AT&T - Atlanta, GA September 2013 to December 2015 Responsibilities:  • Analysis of functional and non-functional categorized data elements for data profiling and mapping from source to target data environment. Developed working documents to support findings and assign specific tasks.  • Created data masking mappings to mask the sensitive data between production and test environment.  • Worked on claims data and extracted data from various sources such as flat files, Oracle and Mainframes.  • Worked with data investigation, discovery and mapping tools to scan every single data record from many sources to ensure data quality.  • Performed data analysis and data profiling using complex SQL on various sources systems including Oracle and Teradata.  • Written several shell scripts using UNIX Korn shell for file transfers, error logging, data archiving, checking the log files and cleanup process.  • Created Webi reports and modified existing Universes in SAP Business Objects.  • Performing data management projects and fulfilling ad-hoc requests according to user specifications by utilizing data management software programs and tools like Perl, Toad, MS Access, Excel and SQL.  • Written SQL scripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to Test Scripts to ensure any Change Control in requirements leads to test case update.  • Worked on loading data from flat files to Teradata tables using SAS Proc Import and Fast Load Techniques.  • Designed scripts in SAS to be compatible with Teradata to load and access data from the Teradata tables.  • Connected to Mainframe and Teradata from SAS using Macros.  • Used Proc SQL to connect to Teradata.  • Loaded files from SAS to Teradata and Created files from Teradata tables Using SAS  • Developed web, mail and email campaigns for marketing to impress the customer with new business features provided.  • Involved in extensive DATA validation by writing several complex SQL queries and Involved in back-end testing and worked with data quality issues.  • Assisted in defining business requirements for the IT team and created BRD and functional specifications documents along with mapping documents to assist the developers in their coding.  • Designed and developed database models for the operational data store, data warehouse, and federated databases to support client enterprise Information Management Strategy.  • Was flexible to work late hours to coordinate with offshore team.    Technical Skills: MS SQL Server 2008, Oracle 10g, MS office, SAS, Clear Quest, Clear Case, Teradata R13 Data Analyst Inforica - Hyderabad, Telangana October 2008 to June 2010 Roles and Responsiblities:  • Generated a streamline process to understand the various steps in the Digital web design lifecycle and desired functionality of the new system by interacting with users, consultants, stakeholders and subject matter experts  • Organized the Initial Kick-Off meeting with the involved Business Stakeholders to discuss the initiatives and the business intent, understand the stake holders needs and expectations in that scope using various Elicitation Techniques.  • Identified, researched, investigated, analyzed, defined opportunities for business process improvement, documented business processes and initiated efforts to make improvements  • Conducted and participated in JAD sessions with stakeholders and system users to collect the system Requirement specifications(SRS)analyzed the feasibility of their needs by coordinating with the project manager and technical lead  • Worked in Agile environment playing an active role in Iteration planning, Sprint Review, Lesson Learned and resolving Impediments  • Used Agile methodology to analyze and translate business requirements into system specifications  • Every request is well documented with the complete approval and stored in the Knowledge Link, a data storage repository location for the future reference.  • Assisted the Project Manager in setting realistic project expectations and in evaluating the impact of changes on the organization and plans accordingly and conducted project related presentations  • Manage Scope and change throughout the life cycle of the product  • Transferring file across various servers using secured FTP and uploading flat files into Teradata database tables using SAS and FastLoad utilities.  • Responsible for creating requirements, data profiling and data mapping of customer one attributes across multiple LOBs  • Coordinated with different LOBs in identifying the data and logic as part of the requirements gathering efforts  • Identify the project dependencies and escalate impediments  • Data analysis of customer data for creating custom reports in support of the LOBs    Technical Skills: Agile(Enhancement), Rational Requisite Pro, MS Excel, MS WORD, UML, Flow Charts, Activity/ State Diagram, MS Visio,Unix, Terra Data, SAS, HTML, Java Script Skills Data modeling, Ms access, Ms sql server, Sql server, Oracle, Pl/sql, Sql, Teradata, Tableau, Shell scripting, Unix, Unix shell, Gui, Python, Reporting tools, Scripting, Pivot tables, Rdbms, Erwin, Rational Additional Information Skill Set  • GUI & Reporting Tools: Business Objects12, Tableau  • Data Modeling: Star-Schema Modeling, Snowflake-Schema Modeling, FACT and dimension tables, Pivot Tables, Erwin  • Testing Tools: Mercury Quality Center, Rational Clear Quest  • RDBMS: Oracle 11g/10g/9i/8i/7.x, MS SQL Server, Teradata V2R6/R12/R13, R14, MS Access 7.0  • Programming: SQL, PL/SQL, UNIX Shell Scripting, VB Script, Python  • Environment: Windows (95, 98, 2000, NT, XP, 7), UNIX  • Other Tools: TOAD, AWS, Spark, MS-Office suite (Word, Excel, Project and Outlook), BTEQ, Teradata V2R6/R12/R13 SQL Assistant, SQL Workbench