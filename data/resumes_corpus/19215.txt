Java/ Python Hadoop Developer Java/<span class="hl">Python</span> Hadoop <span class="hl">Developer</span> Senior Java Hadoop Developer - Citi Bank New York, NY • Has almost 11.5 years (including 6 years in US) of extensive hands-on experience in latest Java and Python and its related technologies along with 3 years of experience in Hadoop related technologies – Hive, Spark , Map/reduce and Oozie using Cloudera distribution of Hadoop Framework.  • Has been working with a market leading Human Resources Company and previously worked with Retail Risk, Investment, and Auto & Loan banking clients.  • A self-starter who takes ownership of tasks at hand, with experience in architecting, designing, developing and testing complex custom applications in Java/J2EE, Spring Framework and development of Big Data technologies in highly scalable end-to-end Hadoop Infrastructure  • Capable and proactive team player with excellent analytical, problem solving, communication and interpersonal skills with ability to interact with individuals. Authorized to work in the US for any employer Work Experience Java/ Python Hadoop Developer ADP - Roseland, NJ November 2017 to Present Project :  Working on ADP's Ventures Development and its related Data Cloud Benchmark projects. Data cloud benchmarking team is responsible to create data pipelines /Machine learning models. Data from the following sources: Employee, HR, Payroll, Large/medium business - Workforce Now(WFN),MAS and small business (SBS) are cleaned, normalized, combined and aggregated and then go through benchmarking cubes/matrices process to build data pipelines and ML models are used by other downstream ADP's projects like: Venture, Next Gen Payroll(Pi) , Innovation lab etc.    Technologies:  ·        Cloudera distribution of Hadoop Framework components/technologies – HIVE, Impala, Spark, HDFS, MR, Oozie workflow, AVRO/PARQUET.  ·        Machine learning(ML) model creation/deployment Technology: H2O distribution, Driverless AI H2O server   ·        Core Java 8 and Python 2.7/3 for feature engineering for ML model  ·        Spring framework (CORE spring, Messaging-JMS template, spring-DAO, JDBC-template, Spring Integration Messaging and Spring Batch), Eh- cache, Oracle 11g, Database – PL/SQL Queries.  ·         Python - Spark Python package, Panda Frame  REST Web service API: java – jax-wrs and python - flask and sqlite3 packages    Roles and Responsibilities:  ·        Working on Python along with Hive/Impala and Spark Python API (PySpark) to prepare Employee level, client level and Payroll level data used for the predictable Analytic Models, used PySpark RDD, Data frame, Hive-SQL and Hive Context.  ·        Creating REST web services to publish Machine learning models - business function and earnings code prediction models via API as predictive analytic service using Java 8 and python flask, codernity DB, SQLite-db. etc.  ·        Creating python scripts to load data from HDFS/CSV to Spark container for analysis and then save the outcome as text/CSV, table data to HDFS.  ·        Using Panda Frame to convert spark data frame to panda frame for various type of quick data analysis and visualization like: scatter plot, area plot, bar plot etc.  ·        Extracting raw Hadoop data files from multiple data sources/products like - ADP Payroll, Small business, Workforce now and cleaning, normalizing, combining and aggregating using Hadoop's - Hive/ Python/Spark and Map Reducers program to create data pipeline for benchmarking ADP data.  ·        Validating and testing benchmark numbers versus external sources like - Federal Reserve data (FRB) using Hive/ python.  ·        Creating feature engineering variables for Machine learning (ML) models using   py-spark /core java 8.  ·        Deploying Machine Learning Models built in H2O on Hadoop cluster (Spark Application).  ·        Using Apache AVRO /Parquet format schema for the HIVE tables to persist data in HDFS  ·        Designed & developed Spring Batch based batch job(s) to invoke and execute the Hadoop Jobs- Hive Jobs, Crunch Jobs and Oozy work flow in Hadoop cluster.  ·        Developing and maintaining use case diagrams, class diagrams, database tables, and mapping between relational database tables and object-oriented java objects.  ·        Mentoring team mates and help them with their technical solutions on daily basis.  Practicing Agile (Scrum) methodologies and scrum process etc. Senior Java Hadoop Developer Citi Bank - New York, NY October 2015 to August 2017 NYC, US  Project  Optima Retail is an enterprise wide initiative to automate the regulatory data submission process for CCAR and BASEL reports, This involves building a system flexible and fast enough to process huge amount of data as the process actually involves consolidating data from each and every retail account of the organization. Optima Retail project is responsible for extracting, processing and generating report data required to be submitted to the regulatory bodies.    Technology  Core Java, Spring framework (CORE spring, Messaging-JMS template, spring-DAO, JDBC-template, Spring Integration Messaging and Spring Batch), Eh- cache, Oracle 11g, Database - PL/SQL Procedures , Hadoop technologies - HDFS, HIVE, Crunch -MR, Oozie workflow , AVRO/PARQUET , Python , Spark Python API , Panda Frame    Roles and Responsibilities  • Designed & developed Spring Batch based batch job(s) to invoke and execute the Hadoop Jobs- Hive Jobs, Crunch Jobs and Oozie work flow in Hadoop cluster.  • Developed common communication and decision making point of the system(Internal Message Handler) to handle feed process using spring integration components like service router, adapters to communicate between various upstream and downstream processes  • Created various type of Hadoop Jobs - Hive jobs, Crunch Jobs using MR jobs and invoked them using Oozie workflow.  • Created Apache AVRO /Parquet format schema for the HDFS tables to persist data in Hadoop cluster.  • Developed use case diagrams, class diagrams, database tables, and mapping between relational database tables and object oriented java objects.  • Worked on Python and Spark Python API (PySpark) to prepare Account level data used for the predictable Analytic Models  • Used Pyspark's RDD, Dataframe, HiveContext, Functions to create various analytic functions like - binning, sort, diff, rank, window, groupby etc. for loss forecast models.  • Created python scripts to load data from HDFS to Spark container for analysis and then save the outcome as text/CSV, table data to HDFS.  • Used PandaFrame to convert spark dataframe to pandaframe for various type of data visualization like: scatter plot, area plot, bar plot etc.  • Mentored team mates /junior members and help them with technical solutions.  • Participated/followed Agile (Scrum) methodologies and scrum process. Senior Java Developer JPMorgan Chase September 2013 to September 2015 NJ , US  Project  Worked in a suit of projects where customer's Auto loan application creation and automatic underwriting /manual under writing functionality has been built using latest JEE technologies.    Overview of each of the application in the project suit has been given below:  a. Scorecard: Its receives credit bureau or FICO score and its related data in JMS messages format from Mainframe system, it calculates the score as per the firm scoring engine    b. Policy Engine: This application process all the loan applications and decides if any application should be given loan. Middle layer components have been developed using JMS point-to-point messaging model and It interacts with ACAP (Old Mainframe system) using text format JMS messages.  c. Credit Assist: This application is used by business people to display/see Dealer Data/ Policy Data which screen scrap main frame data from ACAP system and process them according to business rule and display them on the credit assist UI screen to business user    Technology  Core Java, JEE, Spring framework (CORE spring, Messaging-JMS template, spring-DAO, JDBC-template), Web Service (SOAP and REST), Hibernate, JPA, Eh- cache, Oracle 11g, Database - PL/SQL Procedures    Roles and Responsibilities  • Worked as the key person in terms of Project Architecture, design, development, identifying inputs/outputs discussing with the JPMC's various Legacy Systems.  • Extensively worked on middle tier and persistence tier using the Spring Full Stack and Hibernate/Mybatis,  • Developed C2A Middle layer components using Spring JMS point-to-point messaging model with IBM MQ broker and it interacts with ACAP (Old Mainframe system) using text format JMS messages.  • Used EH-Cache to achieved high performance of the reference data web services.  • Developed reliable transaction system for the web services and JMS applications using spring transaction and AOP and secured the services using SSL/TLS implementation  • Updated and reviewed design documents, created various Test cases, scenarios and Junit test cases to test persistence and service tiers.  • Migrated the Legacy Mainframe system applications to JEE Application  • Worked with BAs and clients to identify the requirements in terms of interaction of the proposed applications with external co-supplier applications, delivery phase, data verification, QA, Production strategy  • Mentored team mates. Senior Java Developer JPMorgan Chase June 2012 to September 2015 Senior Java Developer JPMorgan Chase - New York, NY October 2012 to September 2013 NYC , US  Technology  Core Java, JEE, Spring- (Core, Transaction, JMS and JDBC), Hibernate, EH-CACHE , Web Service, XML, XSLT, Junit, SOAP-UI, Maven build , Jenkins, ABB- client private cloud, Web Service, Vignette Application Portal, SOA, MULE    Project  Worked in Total Fund Repository (TFR) and Data integration.  a. TFR - a central repository for all the funds related data points. TFR provides a web based solution to Fund management and fund related entities like Portfolio managers, Benchmarks, Share classes, Disclosures etc  b. Data integration - Near real-time data synchronization between SalesPage (Retail CRM) and Aprimo (3rd party data provider system) bi-directionally using SOA/ MULE ESB    Roles and Responsibilities  • Developed and maintained the data Integration application eliminating the existing manual data update process in multiple systems using MULE soft ESB framework.  • Achieved near real-time data synchronization between SalesPage (Retail CRM) and Aprimo (3rd party data provider system) bi-directionally.  • Create multiple high performance Restful services using Eh-cache and they were used inside ESB as part of the application.  • Identified future system for data exchange and can be integrated using SOA/MULE ESB.  • Developed a web based solution to Fund management and fund related entities like Portfolio managers, Benchmarks, Share classes, Disclosures etc. Entities involved in JPMC's Total Fund Repository.  • Worked in enhancement, testing and bug fixing.  • Performed code review.  • Mentored and trained junior team members. Java Developer JPMorgan Chase - Kolkata, West Bengal March 2011 to September 2012 Kolkata , India and NYC , US  Project  Worked in IM Core Party Data: A group of applications created to feed Core Party Account data of JPMC Investment Management to downstream applications. Multiple Web services and Messaging applications have been built to cater the downstream applications.    Technology  Core Java, JEE (JSP/Servlet, JMS), Spring- (Core, MVC, Transaction, JMS and JDBC) , Hibernate, JPA , EH-CACHE , Web Service - Apache CXF, Junit, SOAP-UI, Quartz scheduler , Maven build , Jenkins, ABB- client private cloud    Roles and Responsibilities  • Worked with client and BA's of the downstream to gather business requirements for Core Party Account data feeds of JPMC Investment Management Group.  • Designed, developed and maintained several core party SOAP and Restful web services and JMS applications.  • Used EH-Cache to achieved high performance for the reference data web services.  • Developed reliable transaction system for the web services and JMS applications using spring transaction and AOP.  • Secured the services using SSL/TLS implementation and deployed application in JPMC Private Cloud system.  • Performed code review and Mentored and trained junior team members.  • Offshore-onsite coordination. Java Developer JPMorgan Chase August 2010 to May 2012 Java Developer JPMorgan Chase - Kolkata, West Bengal August 2010 to March 2011 Kolkata , India  Project  Worked in IM Common Services Application: Interaction among the various systems of JPMC Investment Management took place at real time using the various applications of IM Common Services.  IM CSA Application was a messaging system that was used to interact with different sub-systems of Investment Management division of JPMorgan Chase Bank.    Technology Core Java, JEE, Spring, Quartz, Restful / SOAP Web service, XML, WebSphere 6.x Application server, Sybase, Unix  Roles and Responsibilities  • Developed multiple RESTful/SOAP Web services and messaging system to build the instrument server which handles Interaction among the various systems of JPMC Investment Management group.  • Migrated JEE components(around 30 small and big applications) build from ant to maven  • Re-Architected and developed all existing EJB components to spring framework components to handle transaction of Indicative Data and Price Data. Java Developer Pro Quest - Noida, Uttar Pradesh October 2008 to July 2010 Noida , India  Project  It provides archives of sources such as newspapers, journals periodicals, dissertations, and aggregated databases of various types and its content is estimated to be approximately 125 billion digital pages    Technology  Core java, JEE, JSP, Servlet, RESTful Web service, Hibernate, JPA (ORM), Oracle DB, Apache Tapestry, Apace Lucene API, JSF, DHTMLX -java script component    Roles and Responsibilities  • Implemented several high performance Restful web services that was used as archives of sources as newspapers, journals periodicals, dissertations, and aggregated databases of various types.  • Handled large number contents estimated to be approximately 125 billion digital pages in UI/web layer using apache tapestry and DHTMLX.  • Followed and used the SCRUM work, Plan view and Rational of IBM for Agile methodology Java Developer Proquest and IHG August 2007 to July 2010 Java Developer Intercontinental Hotels Grp - Noida, Uttar Pradesh August 2007 to October 2008 Noida , India  Project  Worked in Expedia User Management:  This project was built to support IHG Third Party Intermediates (TPI) standards by providing an automated solution for pushing rate and availability status information message notification requested by its external partner.  IHG Web application:  Intercontinental Hotels Group (IHG) was a web based project for hotel reservations handling more than 4000 hotels worldwide and managing around 10 million PCR accounts.    Technology  Core java, Spring MVC, JEE, JSP, Servlet, Resin Pro 3.1.2 as application server, ATG dynamo 7.0 as application server, Genesis Framework    Roles and Responsibilities  • Analyzed the requirements to support IHG TPI standards by providing an automated solution for pushing rate and availability status information message notification  • Developed and maintenance of the admin module and User Management module and handled more than 4000 hotels worldwide and managing around 10 million PCR accounts.  • Used Spring MVC to refactor the legacy application. Education Bachelor of Engineering in Information and Technology Jadavpur University 2003 to 2007 Skills JAVA (10+ years), JDBC (10+ years), JMS (6 years), ORACLE (10+ years), APACHE (10+ years), Apache Spark (3 years), Apache Hive (3 years), Map/Reduce (3 years), Avro (3 years), Parquet (3 years), Python (3 years), Panda Frame (2 years), Apache HDFS (3 years), Flask, C Links https://www.linkedin.com/in/mondalmithun Additional Information Technical Skills  Technology Stack  Language Core Java 7, SQL , Python  Technology Java/J2EE , Hadoop - Hive ,Crunch ,Spark ,Panda Frame, MapReduce ,Oozie ,HDFS,Avro and Parquet  Databases Oracle 11g  Framework Spring 4.x -JMS, Transaction, JDBC, Security, AOP, Batch, Integration and MVC  Middleware Technologies Hibernate 4.x, MyBatis 3.1.1 and IBM MQ 7.x  Front End Technologies JQuery, JSON, Javascript ,DHTMLX  Web servers/ App servers Apache Tomcat 8.x, WAS Server 6.x  Continuous Integration Tools Jenkins, Sonar  Others  Web service (SOAP and RESTful) - Apache CXF and Jersey Client, Distributed transaction, JUnit, Apache POI, J-profiler, Eh-Cache, Maven build, Java 7 Multi-threading