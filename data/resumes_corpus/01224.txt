Database Consultant <span class="hl">Database</span> Consultant Database Consultant Coppell, TX ? Multi skilled professional with track record of managing complex projects in various environments.  ? 10+ years in Information Technology with expertise in Database design, development and maintenance of complex data related applications.  ? Exposure in overall SDLC including requirement gathering, development, testing, debugging, deployment, documentation, production support and Extensive experience in Project Management methodologies.  ? Experience in extracting, transforming and loading (ETL) data from spreadsheets, database tables and other sources using PLSQL, Informatica, ODI/OWB. Work Experience Database Consultant BNP Paribas ( Consulting), Jersey City May 2018 to September 2018 Worked on the REG YY reporting project. This is part of the FED reporting requirement.  ? Designed the entire project from the ground up and developed the data model required for the project.  ? Developed the detailed data flow and lineage documents to understand the data from all the source system.  ? Developed the ETL process to load the data and transform it to the requirements of the REG YY reports.  ? Developed the tableau report to show the trends and also any discrepancies in the data balances, asset/liability and net funding ratios with reference to the corresponding last 1 year data. Informatica Administrator at Charles Schwab Infosys - Dallas, TX February 2018 to May 2018 Administered the DEV and PROD informatics system.  ? Verified all the pre deployment checklists before signing off for the production deployment.  ? Deployed the mappings and workflows into the Production system.  ? Onboarded new applications onto informatics by creating the necessary project related artifacts and defining the user access and creating the necessary connections.  ? Worked on setting up the parallel infrastructure in the Disaster recovery site. Database Consultant BNP Paribas ( Consulting), Jersey City October 2015 to November 2017 Architect the data consolidation from various source systems to feed into Feds Reporting system to generate the 5G, Liquidity and collateral reports.  ? Wrote an exhaustive pre validation process to capture all possible XML validation errors based on the XML schema defined by the Feds.  ? Worked on the data quality, data transformation and data encryption module to send the data to IntelliMatch system which is netting engine and QRM which is a cashflow forecasting engine.  ? Designed and developed the traceability features in key reports to help trace the data back to its origin. This was needed for the Audit purpose. This helped managers to answer the audit related questions very easily and quickly. Before this it would take weeks to handle these questions.  ? Developed Oracle ODI scripts to integrate data from various sources. Earlier the integration was handled thru PLSQL scripts and Informatica workflows. Moving to ODI improved the turnaround time and also the visibility of how the data integration was moving along. Data analyst and Developer Bank Of America ( Consulting), Jersey City March 2015 to October 2015 Worked on designing and developing Actimize model to track solicited parties trades and generating alert reports for the compliance officers.  ? Worked with Informatica ETL team to get the data needed for the solicited party surveillance.  ? Developed complex procedures to support the business logic of the surveillance models  ? Did the data analysis on the source data to verify that the date requirements for the model is fulfilled.  ? Streamlined the existing code to make it more efficient by removing redundancy and consolidated multiple inserts and deletes on the same object.  ? Developed the project specific documents like scoping/assessment, high level definition and low level definition and got the buy in from the stakeholders.  ? Worked with the QA/UAT team and developed mock data to test all the aspects of the model. Data Developer Credit Suisse ( Consulting) - New York, NY October 2013 to January 2015 Managed the database related operations of the reference data hub. Reference data provides the golden source for all the reporting and compliance purpose.  ? Created data governance and data privacy policy and implemented data obfuscation to hide sensitive data.  ? Led an engagement to review poorly performing reports and feed generations there by reducing the daily batch time.  ? Designed and reviewed all the database related changes to make sure they are compliant with the SOX standards and the project coding standards.  ? Performed root cause analysis to fix the service interruptions and provided permanent fix.  ? As part of the organic data growth and retention policy, implemented the data compression on all the historical partitions which resulted in saving space and at the same time making historical data available online.  ? Forecasted the data growth as part of the space management and data availability.  ? Led a project to move the data transfers to all the downstream applications to use the secure FTPS. Lead database developer Hoffmann-La Roche Inc October 2002 to October 2013 Designed and developed data model for the ELN applications.  ? Developed PL/SQL based integration process, created database packages and procedures to handle complex business logic and used the latest constructs like multi table inserts, merge operations, pipelined functions, analytical functions and bulk collect.  ? Developed PL/SQL based ETL to extract data from variety of sources for the Kinase data mart. The sources where protein database (MySQL), chemistry database and flat files provided by the vendor. The ETL also extracts the protein structure and chemical structure and also the associated graph calculations and documents.  ? Served as a lead ETL Informatica developer in the enhancement of an existing ETL frame work that collected and cleansed and integrated the company's research data. The entire project was migrated to Informatica Power Center. Interfaced with the MDM systems for getting the proper biology and chemical terms to validate the data. The enhancements were part of the overall initiative to improve the existing process. This improved the overall cycle time and data quality. The source was Electronic Lab Notebooks (ELN) which was accessible using web service and from different geographic locations.  ? Used analytical functions for aggregating the data and loading into summary table.  ? Created materialized views on the summary table for fast access from the UI applications.  ? Tuned PLSQL and SQL performance using the dbms_profiler, v$ views, execution plan and statistics.  ? Developed an ELN plug in to get the chemical properties of a compound. It makes a web service call to the chemistry database, calculates molecular weight and other formulas and stores in the ELN.  ? Developed a submission module for chemistry ELN. Chemist can request for mass spectrometer and chromatography for their compound. Uses web service to create a new request in the lab database and returns a URL for the chemist to monitor the progress.  ? Automated all the routine jobs using Informatica workflow scheduler, UNIX scripts and Oracle scheduler.  ? Documented all the complex ETL processes explaining the Source/Target data sets, the data flow diagr  ? ams, transformation mappings and quality check/constraints that are performed.    Position Held: Sr. Systems Analyst.  ? Worked as a production support lead to maintain a large data warehousing system.  ? Developed and enhanced the existing ETL process.  ? Analyzed the data issue/ quality issues and availability issues and worked with the stakeholders to resolve the issues.  ? Developed several scripts to automate most of the critical reporting tasks and developed a dashboard to track the progress/status of several processes. This helped the global team to easily track their data and correct the problem and maintain the agreed upon SLA of 24 hours.  ? Took up the responsibility of maintaining and developing the content management website from the vendor thereby saving $150000.00 to the company.  ? Supervised and trained new members in support processes and activities and enhanced the on-boarding process. Education B.E. in Computer Science and Engineering Marathwada University Links http://www.linkedin.com/in/jayvajramani Additional Information Database: Oracle 12c, 11, 10, 9 in RAC clusters.  Modeling tool: ERWin  ETL Tools: Informatica Power Center, PLSQL, Oracle Warehouse Builder, Oracle ODI.  BI Tools: Business Objects  Languages/Scripts: J2EE (JSF), Perl, UNIX Scripts, PLSQL and Pro*C  Source Control: CVS, SVN, SourceSafe.  Web/App Server: Apache, Microsoft IIS  Scheduling: Control-M and Autosys