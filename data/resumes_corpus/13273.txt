Big Data Developer Big Data <span class="hl">Developer</span> Big Data Developer - Wipro LTD Wheeling, IL • 11+ years of experience in software design, development, maintenance, testing, and troubleshooting of enterprise applications  • Over 4 years of experience in development, design, maintenance and support of Big Data Analytics using Hadoop Ecosystem components like HDFS, Hive, Pig, HBase, Sqoop, Flume, Zookeeper, Map Reduce, and Oozie  • Good experience in distributed programming through Spark, specifically in Scala  • Expertise in Java, Spring, Hibernate, JDBC and proficient in using Java API's for application development  • Good knowledge on AWS and Microsoft Azure  • Good working experience with ingestion, storage, querying, processing and analysis of Big Data  • Good knowledge on Streaming Analytics using Spark Streaming, Apache Flume, distributed streaming using Kafka  • Extensive Experience on working with Hadoop Architecture and the components of Hadoop - MapReduce, HDFS, YARN, Name Node and Data Node  • Expertise in writing Hadoop Jobs for analyzing data using Hive, Impala, Hue and Pig  • Good Experience on writing Map Reduce programs using Java  • Good understanding of NoSQL databases like HBase and MongoDB  • Ingesting data to HDFS from Oracle, SQL Server, Teradata using SQOOP  • Experience with SQL, PL/SQL and database concepts  • Good Experience with job workflow scheduling like Oozie, Airflow  • Experience in Tableau in generating reports, dashboards  • Experience with performance tuning on map reduce and Hive jobs  • Load and transform large sets of structured, semi-structured and unstructured data using Hadoop ecosystem components  • Knowledge in installation, configuration, supporting and managing Hadoop clusters  • Experience in working with different data sources like Flat files, XML, Parquet and Databases  • Experience in various phases of Software Development Life Cycle (Analysis, Requirements gathering, Designing) with expertise in documenting various requirement specifications, functional specifications, Test Plans, Source to Target mappings, SQL Joins  • Worked on different operating systems platform UNIX/Linux, Windows  • Quick learner, team player and proficient in handling multiple projects simultaneously Sponsorship required to work in the US Work Experience Big Data Developer Wipro LTD - Chicago, IL May 2018 to Present This project involves ingesting and processing of legacy application data, reports, extracts to Big Data Platform from different sources. Data Cleansing, partitioning, transforming data to create Data Lakes for faster analytics. Create Spark programs to query big data lakes to generate data reports for meaningful data insights, generate dashboards in Tableau.    Responsibilities:  • Distributed programming through Apache Spark using Scala. Created Scala programs to develop the big data queries for business use cases.  • Understanding the existing data warehouse set up and provided design and architecture suggestion converting to Hadoop using MapReduce, Hive, Spark, Sqoop.  • Creating data lakes from Unstructured, Semi Structured data in HDFS.  • Transformation and Analysis in Hive, Parsing the raw data using Spark.  • Create Hadoop workflow jobs using Oozie.  • Use Spark Streaming and Kafka to analyze real time data coming from different sources.  • Worked on capturing transactional changes in the data using Spark and saving the data in HDFS, S3, HBASE.  • Using AWS services like EC2, S3, Route 53.  • Offshore/Onshore coordination of tasks.  • Following Change, and Incidents management for application deployments.    Environment: Spark, Scala, Hive, Kafka, Cloudera, Unix scripting, HBase. Hadoop Developer Wipro LTD January 2017 to April 2018 This Project is developing ETL workflow to support and perform big data analytics and AWS Cloud development for the enterprise by customizing AWS Products and creating Big Data Pipeline using AWS Kinesis and Lambda functions.    Responsibilities:  • Design & Develop ETL workflow using Oozie which includes automating the extraction of data from different database into HDFS using Sqoop scripts, Transformation and Analysis in Hive.  • Developed Map reduce programs for data analytics.  • Create AWS S3 Data Lakes and create a AWS data pipeline to extract data using Kinesis Streams and analyze the logs using Kinesis Analytics and AWS Lambda functions.  • Customizing AWS Products using Cloud Formation Templates, Elastic Load Balancers, Creating roles for access to services of AWS, VPC.  • Transferring data from S3, Redshift, DynamoDB for data analysis.  • Creating Web Application to request AWS EC2 machines, Integration with Ticketing tools.  • Worked with different file formats and compression techniques in Hadoop.  • Performance tuning of Hive queries, Map reduce programs for different applications.    Environment: CDH, Hive, AWS Kinesis, AWS Lambda, Oozie, Flume, Sqoop, Cloudera manager, Tableau Hadoop Developer Wipro LTD January 2016 to November 2016 This project is for processing data from disparate data sources, media base etc., to develop ETL work flow and performing data analytics using the Hadoop ecosystem. Log aggregation and analysis of logs generated from different projects to show the trends and help them to provide better service.    Responsibilities:  • Created Map Reduce Programs to analyze the log and click stream data  • Data pre-processing and querying tables using HIVE  • Worked on importing and exporting data into HDFS and Hive using Sqoop.  • Automated all the jobs from pulling data from databases to loading data into SQL server using shell scripts  • Utilized Flume to import the log data from servers into Hadoop cluster.  • Worked on setting up the Hadoop cluster for the dev, test and prod Environment.    Environment: CDH, Hadoop, HDFS, MapReduce, Hive, Pig, Flume, Sqoop, Tableau. Java Developer Wipro LTD January 2012 to July 2015 EPWF is a work flow application that processes electronic payment transactions and keeps track of various stages of the payment transaction flow. The work flow will track payments from initiation through process to through life cycle process of payment. The work flow will have configurable business rules for each type of customer, payment method, and payment type. Each payment taking system within the customers system will enter an initiated payment process with the work flow. The work flow engine will listen for or the payment status changes. actively trigger events to work a payment through the system and after settlement of payment transactions EPWF posted to Different types of billing Systems.    Responsibilities:  • Used Spring Framework for Dependency injection, Distributed Component and integrated with web services API.  • Involved in code enhancement and defect fixing.  • Actively participated in Object Oriented Analysis & Design sessions of the Project, which is based on MVC Architecture using J2EE and Spring  • Designed and developed presentation layer using HTML, JSP and JavaScript.  • Used XML as data communication format between different modules of the application.  • Created mapping files and POJOS using Hibernate.  • Implemented DAO classes and Service classes for business requirement.  • Conducted code reviews against coding standards and made sure the best practices are maintained in development process.    Environment: Java, JSP, Spring, J2EE, Ajax, JavaScript, JDBC. Java Developer Infosys LTD June 2008 to September 2011 This project involved in maintaining a website to handle the application packaging and deployment workflow, automating the deployment process new web apps to servers. Create scripts to gather the health of the servers and creating dashboards to visualize.    Responsibilities:  • Develop and maintain Web Application for the Process flow, store automation scripts and KB articles.  • Creating scripts to monitor the health of the Web and DB Servers and creating JSP's for visualization.  • Developing XML documents for deploying the web application files to Dev, QA, UAT Environments  • Automating installation of pre-requisites for Web and DB Servers  • Creation of various scripts to assist with tasks for various teams.  • Knowledge on tools like Microsoft SCCM, Microsoft WIX, Microsoft Orca, Wise studio    Environment: Java, JSP and JavaScript, VB Scripting, Power shell scripting, Unix Shell Scripting Education Bachelor's Skills Hdfs, Oozie, Sqoop, Hbase, Kafka, Big Data, ETL, Tableau, Hadoop, Python, Redshift Certifications/Licenses CCA Spark and Hadoop Developer May 2019 to May 2021 Oracle Certified Associate June 2019 to Present AWS Certified Solutions Architect September 2018 to September 2020 Additional Information Technical Skills  • Core Competencies: Big Data / Hadoop, Application Development Core Java SE / J2EE, Spring Framework, Object Oriented Programming (OOP), Full Lifecycle SDLC, Client Server Development, Application Integration  • Programming Languages: C, C++, Java / J2EE, Scala, Python, JavaScript  • Big Data: Apache Spark, Map Reduce, HDFS, PIG, Hive, YARN, Sqoop, Oozie, Kafka, Flume, Zookeeper  • NoSQL Database: HBASE, MongoDB  • Relational Database: Oracle 9i, 10g, 11g, MySQL  • Reporting: Tableau 10  • Operating Systems: Windows, UNIX, Linux, RHEL, CENTOS, Solaris  • Scripting: Shell, PowerShell    Training and Achievements