Data engineer II Data engineer II Data engineer II - Wex Inc • 5+ years of experience in designing and implementing enterprise level data warehouses and data pipeline architectures using Big Data technologies, Apache Hadoop, Apache Spark, Amazon Web Services and Python.  • Experience in building highly reliable, scalable big data solutions on AWS using EMR, Glue, S3 buckets, EC2 instances, Redshift, RDS and others.  • Strong experience in building Enterprise level Data Warehouse applications using Informatica Power Center 9.x/8.x ETL tools.  • Extensively worked on developing Spark applications using spark SQL, Dataframes, and RDD's to improve the transformations on huge datasets.  • Solid knowledge of Data Marts, Operational Data Stores, OLTP/OLAP, Dimensional Data Modeling with Ralph Kimball Methodology (Star Schema Modeling, Snow-Flake Modeling for FACT and Dimensions Tables).  • Hands on experience with HDFS, Map Reduce, Hive and Pig.    • Experience in integration of various data sources with Relational Databases like Oracle, SQL Server and Worked on integrating data from flat files.  • Good analytical, interpersonal, communication, problem solving skills with ability to quickly master new concepts and capable of working in group as well as independently.  • Extensive Experience in Agile Methodology and waterfall methodologies of SDLC. Work Experience Data engineer II Wex Inc July 2018 to Present • Responsible for building data pipelines required for data transformation and movement using AWS cloud services, Apache Spark framework and python.  • Extensive experience in developing data processing scripts using SQL, python to load data into databases.  • Extensively used SQL to build data processing and analytical queries for understanding data behaviour and incoorpating new aggregated features into the data.  • Developed robust data tier to feed the dashboards and customized tiles in TipBoard as per business requirement.  • Responsible to support and monitor data load process, feeding a web-based fleet data analytical application used by thousands of Wex customers.  • Built pipelines to load data into Postgres RDS database, to optimize query execution time and improve Alexa performance.  • Built an event based pipeline to process huge incoming telematics feeds on daily basis using AWS SNS, SQS, Lambda, Cloud watch, S3 and glue services.  • Implemented Spark big data processing using Python and utilizing Data Frames, Spark SQL for faster testing and processing of data.  • Built database architectural strategies to implement and support warehouse for analytical requirements.  • Monitor daily batch processing of transactional data into Postgres and AWS Redshift environment.  • Developed framework to process large sets of data using python & AWS cloud services and scheduled corn jobs to automate the data processing and ETL jobs.  • Used AWS Athena extensively to analysis and process both structured and unstructured data.  • Built data pipelines to perform extract, transform and load using boto3 SDK for python and apache spark framework, sql.  • Streamlined raw data from different unconnected, structured and unstructured data source to help data scientists for faster analytics.  • Maintain documentation for all data pipelines and AWS glue jobs in both dev and prod environments.  • Worked to stream line data pipeline with various. BI Developer (Informatica/AWS/ Python/SQL) Indic Solutions Inc September 2017 to June 2018 • Efficiently implemented a pipeline to load data into Amazon Redshift DWH using Apache Spark, python, spark SQL and AWS EMR concepts.  • Imported data from Amazon S3 into Spark RDD and performed business transformations and actions on RDD.  • Configured AWS Data pipeline using JSON to schedule and handle the data-driven workflows.  • Developed script to unload data from AWS Redshift DB to S3 bucket on incremental basis and generated parquet files using Spark python for reporting purpose.  • Extensively used Apache Spark concepts to process huge volumes of data sets.  • Created external tables using Amazon Spectrum concepts to support faster retrieval of large data sets for reporting.  • Strong experience in designing and developing Business Intelligence solutions in Data Warehousing/ Decision Support Systems using Informatica Power Center.  • Implemented mappings using various Informatica Transformations like Aggregator, Expression, Filter, Sequence Generator, Update Strategy, Source Qualifier, Union, Lookup transformations.  • Extensive experience in developing complex mappings in Informatica to load the data from various sources using different transformations like Source Qualifier, Lookup, Expression, Update Strategy etc.  • Responsible for implementing Re-usable Mapplets, transformations, Mappings, Email, Command tasks and Unix Scripts.  • Building, publishing customized interactive reports and dashboards, report scheduling using Tableau.  • Utilized Jenkins to automate and schedule AWS data processing jobs.  • Developed and maintained ETL (Data Extract, Transformation and Loading) mappings to extract the Data from multiple source systems like Oracle and Flat Files and loaded into AWS S3 bucket & Redshift DB.  • Created pre-session and post session scripts to validate and push the data to Amazon s3 bucket using UNIX shell scripting.  • Extensively worked with Slowly Changing Dimensions Type1, Type2, and Type3 for Data Loads and Data Transformations.  • Created worksheets reports and converted into interactive dashboards by using Tableau Desktop and provided to Business Users, Project Managers and End Users.  • Created different Calculation fields according to requirement, various conditions and applied multiple Filters for various analytical reports and dashboards. Intern University of South Alabama, AL May 2016 to July 2017 • Interacted with Data Modellers and Business Analysts to understand the requirements and the impact of the ETL on the business.  • Designed and developed Informatica ETL mappings to extract master and transactional data from heterogeneous data feeds and load it to target.  • Responsible for developing and maintaining ETL jobs, including ETL implementation and enhancements, testing and quality assurance, troubleshooting issues and ETL/Query performance tuning.  • Designed and developed several SQL Server Stored Procedures, Triggers and Views.  • Extensive experience in Analysis, Design, Data Extraction, Cleansing, Transformation and Loading into Data Marts.  • Monitored sessions using the workflow monitor, which were scheduled, running, completed or failed. Debugged mappings for faile1d sessions.  • Involved in Unit, Integration, System, and Performance testing levels.  • Written documentation to describe program development, logic, coding, testing, changes and corrections.  • Worked as a fully contributing team member, under manager guidance with independent planning & execution responsibilities. Database Developer Atos Pvt Ltd - Pune, Maharashtra June 2014 to December 2015 • Involved in gathering client requirements and converting them into User Requirement Specifications and Functional Requirement Specifications for the designers and developers to understand them as per their perspective.  • Designed and developed end to end mappings, workflows, sessions to transform the source based on the business needs to target system.  • Implemented reusable mapplets and tasks, transformations to reduce redundant mappings and improve efficiency.  • Developed PL/SQL code for migrating financial data managed in SAP in legacy to Amdocs target tables.  • Developed a code for sequence buffer estimation to reduce significant loss in sequence due to ad-hoc buffer guesses  • Reduced migration window from 8 hours to 6 hours using SQL joins, execution plan, query optimization, Indexing.  • C.R (MNP) - Developed a code for PORT-IN and PORT-OUT numbers which fall under different category (Same instance, Inter Instance) called Mobile number portability.  • For migration purpose created SQL tables to streamline Enterprise customers' legacy finance data which used to be managed in Excel. The task required thorough knowledge of BSCS as well as Amdocs finance module  • Good experience developing scripts to maintain the smooth flow of migration using UNIX shell scripting.  • Responsible for the quality assurance of the migrated data.  • Developed mapping documents to explain end to end business logic of how the data is migrated.  • Used agile methodology for the software development.  • Responsible to monitor the process throughout the migration window.  • Resolve any code break issue during the migration window and restart the process trying to minimize the lost time.  • Performed User Acceptance/Business process testing and suggested enhancements to improve the End User functionality. Education Master of Science in Computers and Information Sciences in Computers and Information Sciences University of South Alabama - Mobile, AL July 2017 Bachelor of Technology in Computer Science Engineering in Computer Science Engineering Jawaharlal Nehru Technological University - Hyderabad, Telangana May 2014 Skills MICROSOFT SQL SERVER, SQL SERVER, MYSQL, ORACLE, ORACLE 11 Links https://www.linkedin.com/in/ashritha-ravula-755b2b159