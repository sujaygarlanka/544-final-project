Hadoop Developer Hadoop <span class="hl">Developer</span> Hadoop Developer - Ericsson North Brunswick, NJ • 5+ years of Professional experience in IT Industry in Developing, Implementing, configuring, testing Hadoop ecosystem components and maintenance of various web-based applications using Java, J2EE.  • 3 years of Real-time experience in Hadoop Framework and its ecosystem.  • Worked on Multi Clustered environment and setting up Cloudera Hadoop echo system.  • Good understanding of Classic Hadoop and Yarn architecture along with various Hadoop Demons such as Job Tracker, Task Tracker, Name Node, Data Node, Secondary Name Node, Resource Manager, Node Manager.  • Experience in dealing with Apache Hadoop components like HDFS, Map Reduce, Sqoop, Hive, Oozie.  • Good knowledge on Spark In-memory capabilities and its modules: Spark Streaming, Spark-SQL.  • Skilled in integrating Kafka with Spark streaming for high speed data processing.  • Hands on experience with working on Spark using both Scala and python. Performed various actions and transformations on spark RDD's and DataFrames.  • Expertise in writing Hadoop Jobs for processing and analyzing data using MapReduce, Hive & Pig. Experienced in extending Hive and Pig core functionality by writing custom UDFs using Java.  • Hands-on experience on YARN (MapReduce 2.0) architecture and it components.  • Hands on experience using Core Java, UNIX Shell scripting and RDBMS.  • Excellent Java development skills using J2EE, J2SE, Servlets, JUnit, JSP, JDBC.  • Very good hands-on technical knowledge of ETL Tools, DataStage, SQL and PL/SQL.  • Vast Experience in Teradata and Involved in Converting Projects from Teradata to Hadoop.  • Hands on experience working on virtualization tools like Tableau, Arcadia Data.  • Well versed with Agile working environment using JIRA and code version tools like GIT • Knowledge of job workflow scheduling and monitoring tools like oozie and Zookeeper, of NoSQL databases such as HBase, Cassandra.  • Research-oriented, motivated, proactive, self-starter with strong technical, analytical and interpersonal skills. Authorized to work in the US for any employer Work Experience Hadoop Developer Ericsson - Piscataway, NJ January 2017 to Present Responsibilities: • Responsible for building scalable distributed data solutions using Hadoop.  • Job duties include design and development of various modules in Hadoop Big Data platform and processing data using MapReduce, Hive, SQOOP, Kafka and Oozie.  • Developed job processing scripts using Oozie workflow • Implemented POC to migrate map reduce jobs into Spark RDD transformations using Scala.  • Worked with Apache Hadoop, Spark and Scala.  • Used Data Frame API in Scala for converting the distributed collection of data organized into named columns.  • Used different Hadoop components in Talend to design the framework.  • Involved in Hadoop cluster task like commissioning & decommissioning Nodes without any effect to running jobs and data.  • Spark streaming collects the data from Kafka in near real time and performs necessary transformations and aggregations on the fly to build the common learner data model and persists the data in Cassandra • Wrote Map Reduce jobs to discover trends in data usage by users.  • Worked extensively with Sqoop for importing metadata from Oracle. Used Sqoop to import data from SQL server to Cassandra.  • Real streaming the data using Spark with Kafka.  • Designed, developed and did maintenance of data integration programs in a Hadoop and RDBMS environment with both traditional and non-traditional source systems as we as RDBMS and NoSQL data stores for data access and analysis. Experienced in running Hadoop streaming jobs to process terabytes of xml format data.  • Involved in installing, configuring and managing Hadoop Ecosystem components like Hive, Pig, Sqoop, Kafka and Flume.  • Assisted in exporting analyzed data to relational databases using Sqoop.  • Wrote Hive Queries and UDF's.  • Developed Hive queries to process the data and generate the data cubes for visualizing.    Environment: MapReduce, Spark, HDFS, Pig, HBase, Oozie, Zookeeper, Sqoop, Cassandra, Linux, Kafka, XML, Hadoop, Maven, NoSQL, MySQL, Hive, Java, Eclipse, Python. Hadoop Developer Big Data February 2016 to December 2016 Onblick, Iving, TX    Responsibilities: • Created Hive tables and working on them using Hive QL.  • Involved in installing Hadoop Ecosystem components.  • Validated Name node, Data node status in a HDFS cluster.  • Importing and exporting data from HDFS to RDBMS and vice-versa using SQOOP.  • Experienced in developing HIVE Queries on different data formats like Text file, CSV file.  • Developed multiple MapReduce jobs in java for data cleaning and preprocessing.  • Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis • Installed and configured Hadoop cluster in Test and Production environments • Code review as per the customer coding standards.  • Testing and providing the valid test data to users as per requirement.  • Weekly meetings with technical collaborators and active participation in code review sessions with senior and junior developers.  • Responsible to manage data coming from different sources.  • Supporting Hbase Architecture Design with the Hadoop Architect team to develop a Database Design in HDFS. Involved in HDFS maintenance and loading of structured and unstructured data.  • Wrote Hive queries for data analysis to meet the business requirements.  • Installed and configured Pig and also written Pig Latin scripts.  • Developed UDFs for Pig Data Analysis.  • Involved in managing and reviewing Hadoop log files.  • Developed Scripts and Batch Job to schedule various Hadoop Program.    Environment: Java, Hadoop, MapReduce, HDFS, Hive, Pig, Sqoop, Zookeeper, , XML, Eclipse, Cloudera. Java Developer RIG - Hyderabad, Telangana August 2013 to December 2014 Responsibilities: • Individually worked on all the stages of a Software Development Life Cycle (SDLC).  • Used JavaScript code, HTML and CSS style declarations to enrich websites.  • Implemented the application using Spring MVC Framework which is based on MVC design pattern.  • Developed application service components and configured beans using (applicationContext.xml) Spring IOC • Designed User Interface and the business logic for customer registration and maintenance.  • Integrating Web services and working with data in different servers.  • Involved in designing and Development of SOA services using Web Services.  • Understanding the requirements from business users and end users.  • Working with XML/XSLT files.  • Experience creating UML class and sequence diagram.  • Experience in Creating Tables, Views, Triggers, Indexes, Constraints and functions in SQL • Server2005.  • Worked in content management for versioning and notifications.    Environment: Java, J2EE, JSP, spring, Struts, Hibernate, Eclipse, SOA, WebLogic, Oracle, HTML, CSS, Web Services, JUnit, SVN, Windows, UNIX. Education Masters in Computer in Computer Texas A&M University 2016